{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import gym\n",
    "import tianshou as ts\n",
    "import Environment\n",
    "from tianshou.env import (\n",
    "    ContinuousToDiscrete,\n",
    "    DummyVectorEnv,\n",
    "    PettingZooEnv,\n",
    "    ShmemVectorEnv,\n",
    "    SubprocVectorEnv,\n",
    ")\n",
    "from pettingzoo.utils.wrappers import BaseWrapper\n",
    "# define a multi-agent environment\n",
    "\n",
    "# import the predefined networks from tianshou\n",
    "from tianshou.utils.net.common import ActorCritic, Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_env():\n",
    "    env = Environment.VehicleJobSchedulingEnvACE()\n",
    "    env = BaseWrapper(env)\n",
    "    env = PettingZooEnv(env)\n",
    "    return env\n",
    "env = get_env()\n",
    "num_agents = env.num_agents\n",
    "obs_shape,*_ = env.observation_space.shape\n",
    "act_shape = env.action_space.n \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a policy network and a critic network for each agent\n",
    "policy_nets = [Net(obs_shape, act_shape, hidden_sizes=[64], ) for _ in range(num_agents)] # use output_size=act_shape.sum() for multi-discrete action space\n",
    "critic_nets = [Net(obs_shape * num_agents + act_shape * num_agents, hidden_sizes=[64], action_shape=1) for _ in range(num_agents)] # use act_shape.sum() instead of act_shape for multi-discrete action space\n",
    "\n",
    "# define a policy and a critic for each agent using tianshou and the predefined networks\n",
    "policies = []\n",
    "critics = []\n",
    "for i in range(num_agents):\n",
    "    # use different optimizers and parameters for policy and critic networks\n",
    "    optim_p = torch.optim.Adam(policy_nets[i].parameters(), lr=1e-4)\n",
    "    optim_c = torch.optim.Adam(critic_nets[i].parameters(), lr=1e-3)\n",
    "    # use DDPGPolicy\n",
    "    policy = ts.policy.DDPGPolicy(actor=policy_nets[i], actor_optim=optim_p,critic=critic_nets[i], critic_optim=optim_c,gamma=0.95,tau=0.01)\n",
    "    policies.append(policy)\n",
    "maddpg = ts.policy.MultiAgentPolicyManager(policies,env)\n",
    "# define a replay buffer to store transitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SubprocVectorEnv([lambda: env for _ in range(10)])\n",
    "buffer = ts.data.VectorReplayBuffer(total_size=100000,buffer_num=10)\n",
    "# define a collector to interact with the environment and collect data\n",
    "collector = ts.data.Collector(maddpg, env, buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# define a trainer to train the policies and critics\n",
    "trainer = ts.trainer.offpolicy_trainer(\n",
    "    policy= maddpg,\n",
    "    buffer=buffer,\n",
    "    train_collector = collector,\n",
    "    test_collector = collector,\n",
    "    step_per_collect = 10,\n",
    "    episode_per_test = 10,\n",
    "    max_epoch=100,\n",
    "    step_per_epoch=10000,\n",
    "    batch_size=128,\n",
    "    update_per_step=10,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
