{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'PettingZooEnv' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m buffer \u001b[39m=\u001b[39m ts\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mReplayBuffer(size\u001b[39m=\u001b[39m\u001b[39m100000\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39m# define a collector to interact with the environment and collect data\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m collector \u001b[39m=\u001b[39m ts\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mCollector(maddpg, env, buffer)\n\u001b[1;32m     51\u001b[0m \u001b[39m# define a trainer to train the policies and critics\u001b[39;00m\n\u001b[1;32m     52\u001b[0m trainer \u001b[39m=\u001b[39m ts\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moffpolicy_trainer(\n\u001b[1;32m     53\u001b[0m     policies,\n\u001b[1;32m     54\u001b[0m     critics,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     update_per_step\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     61\u001b[0m )\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/tianshou/data/collector.py:72\u001b[0m, in \u001b[0;36mCollector.__init__\u001b[0;34m(self, policy, env, buffer, preprocess_fn, exploration_noise)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_num \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv)\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexploration_noise \u001b[39m=\u001b[39m exploration_noise\n\u001b[1;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_buffer(buffer)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'PettingZooEnv' has no len()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import gym\n",
    "import tianshou as ts\n",
    "import Environment\n",
    "from tianshou.env import (\n",
    "    ContinuousToDiscrete,\n",
    "    DummyVectorEnv,\n",
    "    PettingZooEnv,\n",
    "    ShmemVectorEnv,\n",
    "    SubprocVectorEnv,\n",
    ")\n",
    "from pettingzoo.utils.wrappers import BaseWrapper\n",
    "# define a multi-agent environment\n",
    "\n",
    "def get_env():\n",
    "    env = Environment.VehicleJobSchedulingEnvACE()\n",
    "    env = BaseWrapper(env)\n",
    "    env = PettingZooEnv(env)\n",
    "    return env\n",
    "env = get_env()\n",
    "num_agents = env.num_agents\n",
    "obs_shape,*_ = env.observation_space.shape\n",
    "act_shape = env.action_space.n \n",
    "\n",
    "# import the predefined networks from tianshou\n",
    "from tianshou.utils.net.common import ActorCritic, Net\n",
    "\n",
    "# define a policy network and a critic network for each agent\n",
    "policy_nets = [Net(obs_shape, act_shape, hidden_sizes=[64], ) for _ in range(num_agents)] # use output_size=act_shape.sum() for multi-discrete action space\n",
    "critic_nets = [Net(obs_shape * num_agents + act_shape * num_agents, hidden_sizes=[64], action_shape=1) for _ in range(num_agents)] # use act_shape.sum() instead of act_shape for multi-discrete action space\n",
    "\n",
    "# define a policy and a critic for each agent using tianshou and the predefined networks\n",
    "policies = []\n",
    "critics = []\n",
    "for i in range(num_agents):\n",
    "    # use different optimizers and parameters for policy and critic networks\n",
    "    optim_p = torch.optim.Adam(policy_nets[i].parameters(), lr=1e-4)\n",
    "    optim_c = torch.optim.Adam(critic_nets[i].parameters(), lr=1e-3)\n",
    "    # use DDPGPolicy\n",
    "    policy = ts.policy.DDPGPolicy(actor=policy_nets[i], actor_optim=optim_p,critic=critic_nets[i], critic_optim=optim_c,gamma=0.95,tau=0.01)\n",
    "    policies.append(policy)\n",
    "maddpg = ts.policy.MultiAgentPolicyManager(policies,env)\n",
    "# define a replay buffer to store transitions\n",
    "buffer = ts.data.ReplayBuffer(size=100000)\n",
    "\n",
    "# define a collector to interact with the environment and collect data\n",
    "collector = ts.data.Collector(maddpg, env, buffer)\n",
    "\n",
    "# define a trainer to train the policies and critics\n",
    "trainer = ts.trainer.offpolicy_trainer(\n",
    "    policies,\n",
    "    critics,\n",
    "    collectors=[collector],\n",
    "    max_epoch=100,\n",
    "    step_per_epoch=1000,\n",
    "    collect_per_step=10,\n",
    "    batch_size=128,\n",
    "    update_per_step=10,\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
