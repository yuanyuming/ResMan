{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义超参数\n",
    "- 创建环境或模拟器\n",
    "- 设计策略网络和价值模型\n",
    "- 创建回放缓冲区和数据加载器\n",
    "- 运行训练循环并分析结果\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict  # 返回缺失键的默认值\n",
    "import matplotlib.pyplot as plt  # 提供类似 MATLAB 的绘图框架\n",
    "import torch  # PyTorch 的顶级包，一个深度学习框架\n",
    "# PyTorch 模块的基类，使用 tensordict.TensorDict 作为输入和输出\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor  # 从输入张量中提取正态分布的参数\n",
    "from torch import nn  # PyTorch 的子包，提供用于构建神经网络的模块和类\n",
    "from torchrl.collectors import SyncDataCollector  # 使用 actor 模块从多个环境中同步收集数据的类\n",
    "from torchrl.data.replay_buffers import ReplayBuffer  # 存储从环境中收集的数据并允许进行训练采样的类\n",
    "# 从重放缓冲存储器中无放回地采样数据的类\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "# 将数据存储为内存中的张量并允许延迟访问它们的类\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import (\n",
    "    Compose,  # 将多个环境变换组合成一个变换的类\n",
    "    DoubleToFloat,  # 将环境观察或动作空间中的双精度张量转换为单精度张量的类\n",
    "    ObservationNorm,  # 使用平均值和标准差的运行统计信息对观察进行归一化的类\n",
    "    StepCounter,  # 计算在环境中执行了多少步并将其添加到观察空间作为额外维度的类\n",
    "    TransformedEnv,  # 对观察、动作、奖励或完成标志应用变换的环境抽象基类\n",
    ")\n",
    "from torchrl.envs.libs.gym import GymEnv  # 实现了 torchrl.envs.Env 接口的 gym 环境包装器类\n",
    "# 检查环境是否符合预期规范并设置支持探索模式（如随机 vs 确定性动作）的函数\n",
    "from torchrl.envs.utils import check_env_specs, set_exploration_mode\n",
    "# 根据观察输出动作概率分布的 actor（策略网络）类\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss  # 实现策略梯度方法中近端策略优化（PPO）损失函数剪裁版本的类\n",
    "from torchrl.objectives.value import GAE  # 计算策略梯度方法中广义优势估计（GAE）信号的类\n",
    "from tqdm import tqdm  # 提供循环和迭代器进度条\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" if not torch.has_cuda else \"cuda:0\"\n",
    "num_cells = 256  # number of cells in each layer\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_skip = 1\n",
    "frames_per_batch = 1000 // frame_skip\n",
    "# For a complete training, bring the number of frames up to 1M\n",
    "total_frames = 50_000 // frame_skip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "sub_batch_size = 64\n",
    "num_epochs = 10  # optimisation steps per batch of data collected\n",
    "clip_epsilon = (\n",
    "    # clip value for PPO loss: see the equation in the intro for more context.\n",
    "    0.2\n",
    ")\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = GymEnv(\"InvertedDoublePendulum-v4\",\n",
    "                  device=device, frame_skip=frame_skip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    Compose(\n",
    "        # normalize observations\n",
    "        ObservationNorm(in_keys=[\"observation\"]),\n",
    "        DoubleToFloat(in_keys=[\"observation\"]),\n",
    "        StepCounter(),\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"normalization constant shape:\", env.transform[0].loc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"input_spec:\", env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", env.action_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env_specs(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = env.rollout(6)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(2 * env.action_spec.shape[-1], device=device),\n",
    "    NormalParamExtractor(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"loc\", \"scale\"],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"min\": env.action_spec.space.minimum,\n",
    "        \"max\": env.action_spec.space.maximum,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    # we'll need the log-prob for the numerator of the importance weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net = nn.Sequential(\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(num_cells, device=device),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1, device=device),\n",
    ")\n",
    "\n",
    "value_module = ValueOperator(\n",
    "    module=value_net,\n",
    "    in_keys=[\"observation\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running policy:\", policy_module(env.reset()))\n",
    "print(\"Running value:\", value_module(env.reset()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    "    split_trajs=False,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(frames_per_batch),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value_module, average_gae=True\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor=policy_module,\n",
    "    critic=value_module,\n",
    "    advantage_key=\"advantage\",\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    value_target_key=advantage_module.value_target_key,\n",
    "    critic_coef=1.0,\n",
    "    gamma=0.99,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = defaultdict(list)\n",
    "pbar = tqdm(total=total_frames * frame_skip)\n",
    "eval_str = \"\"\n",
    "\n",
    "# We iterate over the collector until it reaches the total number of frames it was\n",
    "# designed to collect:\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "    # we now have a batch of data to work with. Let's learn something from it.\n",
    "    for _ in range(num_epochs):\n",
    "        # We'll need an \"advantage\" signal to make PPO work.\n",
    "        # We re-compute it at each epoch as its value depends on the value\n",
    "        # network which is updated in the inner loop.\n",
    "        advantage_module(tensordict_data)\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata, *_ = replay_buffer.sample(sub_batch_size)\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            # Optimization: backward, grad clipping and optim step\n",
    "            loss_value.backward()\n",
    "            # this is not strictly mandatory but it's good practice to keep\n",
    "            # your gradient norm bounded\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n",
    "    pbar.update(tensordict_data.numel() * frame_skip)\n",
    "    cum_reward_str = (\n",
    "        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n",
    "    )\n",
    "    logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n",
    "    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n",
    "    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n",
    "    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n",
    "    if i % 10 == 0:\n",
    "        # We evaluate the policy once every 10 batches of data.\n",
    "        # Evaluation is rather simple: execute the policy without exploration\n",
    "        # (take the expected value of the action distribution) for a given\n",
    "        # number of steps (1000, which is our env horizon).\n",
    "        # The ``rollout`` method of the env can take a policy as argument:\n",
    "        # it will then execute this policy at each step.\n",
    "        with set_exploration_mode(\"mean\"), torch.no_grad():\n",
    "            # execute a rollout with the trained policy\n",
    "            eval_rollout = env.rollout(1000, policy_module)\n",
    "            logs[\"eval reward\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].mean().item())\n",
    "            logs[\"eval reward (sum)\"].append(\n",
    "                eval_rollout[\"next\", \"reward\"].sum().item()\n",
    "            )\n",
    "            logs[\"eval step_count\"].append(\n",
    "                eval_rollout[\"step_count\"].max().item())\n",
    "            eval_str = (\n",
    "                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n",
    "                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n",
    "                f\"eval step-count: {logs['eval step_count'][-1]}\"\n",
    "            )\n",
    "            del eval_rollout\n",
    "    pbar.set_description(\n",
    "        \", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n",
    "\n",
    "    # We're also using a learning rate scheduler. Like the gradient clipping,\n",
    "    # this is a nice-to-have but nothing necessary for PPO to work.\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(logs[\"reward\"])\n",
    "plt.title(\"training rewards (average)\")\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(logs[\"step_count\"])\n",
    "plt.title(\"Max step count (training)\")\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(logs[\"eval reward (sum)\"])\n",
    "plt.title(\"Return (test)\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(logs[\"eval step_count\"])\n",
    "plt.title(\"Max step count (test)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stable baseline\n",
    "1. import dependency\n",
    "2. load environment\n",
    "3. train an RL\n",
    "4. save and reload model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install stable-baselines3[extra]\n",
    "%pip install pyglet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "import os\n",
    "# Environment\n",
    "import gym\n",
    "# PPO\n",
    "from stable_baselines3 import PPO\n",
    "# Vectorize Environment\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "# test the performance\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load environment\n",
    "\n",
    "### OpenAI Gym Spaces\n",
    "- Box\n",
    "- Discrete\n",
    "- Tuple\n",
    "- Dict\n",
    "- MultiBinary\n",
    "- MultiDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v0'\n",
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} ,Score:{}'.format(episode,score))\n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train an RL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('train','logs')\n",
    "\n",
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy',env=env, verbose=1,tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('train','model','PPO_Model_CartPole')\n",
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_Path,env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model=model,env=env,n_eval_episodes=10,render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range(episodes):\n",
    "   obs = env.reset()\n",
    "   done = False\n",
    "   score = 0\n",
    "   \n",
    "   while not done:\n",
    "    # env.render()\n",
    "    action,state = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "    \n",
    "   print('Episode:{} ,Score:{}'.format(episode, score))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join('train','logs','PPO_1')\n",
    "\n",
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add callback, alt algorithms and Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "eval_callback = EvalCallback(env,callback_on_new_best=stop_callback,eval_freq=10000,best_model_save_path= os.path.join('train','model'),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('train', 'logs')\n",
    "model = PPO('MlpPolicy',env,verbose=1,tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10000, callback=eval_callback)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change polices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arch = dict(pi=[128,128,128,128],vf=[128,128,128,128])\n",
    "model = PPO('MlpPolicy',env,verbose=1,tensorboard_log=log_path,policy_kwargs={'net_arch':new_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10000, callback=eval_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use alternate algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "model = DQN('MlpPolicy',env,verbose=1,tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10000,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m atari_py.import_roms .\\ROMS\\ROMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'Breakout-v0'\n",
    "env = gym.make(environment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0 ,Score:2.0\n",
      "Episode:1 ,Score:2.0\n",
      "Episode:2 ,Score:2.0\n",
      "Episode:3 ,Score:1.0\n",
      "Episode:4 ,Score:0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} ,Score:{}'.format(episode, score))\n",
    "env.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Environment and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_atari_env('Breakout-v0',n_envs=4,seed=0)\n",
    "env = VecFrameStack(env,n_stack=4)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join('train','logs')\n",
    "model = A2C('CnnPolicy',env,verbose=1,tensorboard_log=log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to train\\logs\\A2C_1\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 281      |\n",
      "|    ep_rew_mean        | 1.51     |\n",
      "| time/                 |          |\n",
      "|    fps                | 215      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | -0.0607  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.815   |\n",
      "|    value_loss         | 0.952    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 291      |\n",
      "|    ep_rew_mean        | 1.69     |\n",
      "| time/                 |          |\n",
      "|    fps                | 214      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -13.4    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.0365  |\n",
      "|    value_loss         | 0.00274  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 302      |\n",
      "|    ep_rew_mean        | 1.91     |\n",
      "| time/                 |          |\n",
      "|    fps                | 218      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.0256   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.285   |\n",
      "|    value_loss         | 0.127    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 313      |\n",
      "|    ep_rew_mean        | 2.13     |\n",
      "| time/                 |          |\n",
      "|    fps                | 221      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.27    |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.0646  |\n",
      "|    value_loss         | 0.0287   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 315      |\n",
      "|    ep_rew_mean        | 2.18     |\n",
      "| time/                 |          |\n",
      "|    fps                | 222      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.182    |\n",
      "|    value_loss         | 0.0448   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 2.34     |\n",
      "| time/                 |          |\n",
      "|    fps                | 222      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 53       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | 0.777    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.0927   |\n",
      "|    value_loss         | 0.126    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 331      |\n",
      "|    ep_rew_mean        | 2.46     |\n",
      "| time/                 |          |\n",
      "|    fps                | 220      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.158    |\n",
      "|    value_loss         | 0.0546   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 334      |\n",
      "|    ep_rew_mean        | 2.55     |\n",
      "| time/                 |          |\n",
      "|    fps                | 220      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 72       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.796   |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.0773  |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 335      |\n",
      "|    ep_rew_mean        | 2.57     |\n",
      "| time/                 |          |\n",
      "|    fps                | 219      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 81       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.733    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.0775   |\n",
      "|    value_loss         | 0.1      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 336      |\n",
      "|    ep_rew_mean        | 2.59     |\n",
      "| time/                 |          |\n",
      "|    fps                | 220      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 90       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.23    |\n",
      "|    explained_variance | 0.861    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.0921   |\n",
      "|    value_loss         | 0.0344   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 327      |\n",
      "|    ep_rew_mean        | 2.49     |\n",
      "| time/                 |          |\n",
      "|    fps                | 219      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 100      |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0.547    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.0292   |\n",
      "|    value_loss         | 0.154    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 318      |\n",
      "|    ep_rew_mean        | 2.36     |\n",
      "| time/                 |          |\n",
      "|    fps                | 219      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.26    |\n",
      "|    explained_variance | 0.252    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.275   |\n",
      "|    value_loss         | 0.168    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 321       |\n",
      "|    ep_rew_mean        | 2.51      |\n",
      "| time/                 |           |\n",
      "|    fps                | 218       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 118       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0128   |\n",
      "|    explained_variance | 0.945     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -3.18e-05 |\n",
      "|    value_loss         | 0.00458   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 307      |\n",
      "|    ep_rew_mean        | 2.4      |\n",
      "| time/                 |          |\n",
      "|    fps                | 218      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 128      |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.693    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.0734   |\n",
      "|    value_loss         | 0.0557   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 340      |\n",
      "|    ep_rew_mean        | 3.15     |\n",
      "| time/                 |          |\n",
      "|    fps                | 218      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 137      |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.617    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.203   |\n",
      "|    value_loss         | 0.296    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 354      |\n",
      "|    ep_rew_mean        | 3.38     |\n",
      "| time/                 |          |\n",
      "|    fps                | 219      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 146      |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.804   |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.0113  |\n",
      "|    value_loss         | 0.106    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 379      |\n",
      "|    ep_rew_mean        | 3.86     |\n",
      "| time/                 |          |\n",
      "|    fps                | 219      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 154      |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.0623   |\n",
      "|    value_loss         | 0.0812   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 424      |\n",
      "|    ep_rew_mean        | 4.74     |\n",
      "| time/                 |          |\n",
      "|    fps                | 220      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 163      |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.84    |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.0379   |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 440      |\n",
      "|    ep_rew_mean        | 5.08     |\n",
      "| time/                 |          |\n",
      "|    fps                | 220      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 172      |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.582   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.0306   |\n",
      "|    value_loss         | 0.0301   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 448      |\n",
      "|    ep_rew_mean        | 5.17     |\n",
      "| time/                 |          |\n",
      "|    fps                | 221      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 180      |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.667   |\n",
      "|    explained_variance | 0.13     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.12     |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 444      |\n",
      "|    ep_rew_mean        | 5.09     |\n",
      "| time/                 |          |\n",
      "|    fps                | 222      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 188      |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.882   |\n",
      "|    explained_variance | 0.286    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 0.225    |\n",
      "|    value_loss         | 0.37     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 447      |\n",
      "|    ep_rew_mean        | 5.18     |\n",
      "| time/                 |          |\n",
      "|    fps                | 222      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 197      |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.435   |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -0.0743  |\n",
      "|    value_loss         | 0.0895   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 433      |\n",
      "|    ep_rew_mean        | 4.92     |\n",
      "| time/                 |          |\n",
      "|    fps                | 223      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 205      |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 0.0927   |\n",
      "|    value_loss         | 0.0605   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 432      |\n",
      "|    ep_rew_mean        | 4.79     |\n",
      "| time/                 |          |\n",
      "|    fps                | 223      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 214      |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.631   |\n",
      "|    explained_variance | 0.752    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 0.0836   |\n",
      "|    value_loss         | 0.281    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 444      |\n",
      "|    ep_rew_mean        | 5.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 224      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 222      |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.754   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | -0.0182  |\n",
      "|    value_loss         | 0.00392  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 449      |\n",
      "|    ep_rew_mean        | 5.16     |\n",
      "| time/                 |          |\n",
      "|    fps                | 224      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 231      |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.508   |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 0.0726   |\n",
      "|    value_loss         | 0.0406   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 449      |\n",
      "|    ep_rew_mean        | 5.15     |\n",
      "| time/                 |          |\n",
      "|    fps                | 225      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 239      |\n",
      "|    total_timesteps    | 54000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.666   |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.0332  |\n",
      "|    value_loss         | 0.089    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 446      |\n",
      "|    ep_rew_mean        | 5.15     |\n",
      "| time/                 |          |\n",
      "|    fps                | 225      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 248      |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.545   |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | -0.0751  |\n",
      "|    value_loss         | 0.0524   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 443      |\n",
      "|    ep_rew_mean        | 5.09     |\n",
      "| time/                 |          |\n",
      "|    fps                | 225      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 257      |\n",
      "|    total_timesteps    | 58000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.441   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 0.00301  |\n",
      "|    value_loss         | 0.0259   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 442      |\n",
      "|    ep_rew_mean        | 5.09     |\n",
      "| time/                 |          |\n",
      "|    fps                | 224      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 266      |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.676   |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | -0.0359  |\n",
      "|    value_loss         | 0.0398   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 445      |\n",
      "|    ep_rew_mean        | 5.06     |\n",
      "| time/                 |          |\n",
      "|    fps                | 224      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 275      |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.365   |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -0.00325 |\n",
      "|    value_loss         | 0.0782   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 450      |\n",
      "|    ep_rew_mean        | 5.14     |\n",
      "| time/                 |          |\n",
      "|    fps                | 224      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 284      |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.194   |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 0.00418  |\n",
      "|    value_loss         | 0.128    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 445      |\n",
      "|    ep_rew_mean        | 5.02     |\n",
      "| time/                 |          |\n",
      "|    fps                | 224      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 293      |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.158   |\n",
      "|    explained_variance | 0.804    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -0.0422  |\n",
      "|    value_loss         | 0.0688   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 448      |\n",
      "|    ep_rew_mean        | 5.06     |\n",
      "| time/                 |          |\n",
      "|    fps                | 224      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 302      |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.259   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -0.0521  |\n",
      "|    value_loss         | 0.0364   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 454      |\n",
      "|    ep_rew_mean        | 5.14     |\n",
      "| time/                 |          |\n",
      "|    fps                | 225      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 311      |\n",
      "|    total_timesteps    | 70000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.347   |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 0.0264   |\n",
      "|    value_loss         | 0.0202   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 458      |\n",
      "|    ep_rew_mean        | 5.21     |\n",
      "| time/                 |          |\n",
      "|    fps                | 224      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 320      |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.471   |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | -0.0172  |\n",
      "|    value_loss         | 0.0463   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 468      |\n",
      "|    ep_rew_mean        | 5.42     |\n",
      "| time/                 |          |\n",
      "|    fps                | 225      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 328      |\n",
      "|    total_timesteps    | 74000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.241   |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 0.0073   |\n",
      "|    value_loss         | 0.051    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 479      |\n",
      "|    ep_rew_mean        | 5.69     |\n",
      "| time/                 |          |\n",
      "|    fps                | 225      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 337      |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.514   |\n",
      "|    explained_variance | 0.791    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 0.0204   |\n",
      "|    value_loss         | 0.0429   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 478      |\n",
      "|    ep_rew_mean        | 5.71     |\n",
      "| time/                 |          |\n",
      "|    fps                | 225      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 345      |\n",
      "|    total_timesteps    | 78000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.758   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.0247   |\n",
      "|    value_loss         | 0.0255   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 479      |\n",
      "|    ep_rew_mean        | 5.72     |\n",
      "| time/                 |          |\n",
      "|    fps                | 225      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 354      |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.716   |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -0.0162  |\n",
      "|    value_loss         | 0.0294   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 474      |\n",
      "|    ep_rew_mean        | 5.7      |\n",
      "| time/                 |          |\n",
      "|    fps                | 225      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 363      |\n",
      "|    total_timesteps    | 82000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.531   |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -0.0253  |\n",
      "|    value_loss         | 0.0409   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 469      |\n",
      "|    ep_rew_mean        | 5.63     |\n",
      "| time/                 |          |\n",
      "|    fps                | 225      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 372      |\n",
      "|    total_timesteps    | 84000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.514   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | -0.0805  |\n",
      "|    value_loss         | 0.00964  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 448      |\n",
      "|    ep_rew_mean        | 5.19     |\n",
      "| time/                 |          |\n",
      "|    fps                | 225      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 381      |\n",
      "|    total_timesteps    | 86000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.516   |\n",
      "|    explained_variance | 0.776    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | 0.0316   |\n",
      "|    value_loss         | 0.0525   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 460      |\n",
      "|    ep_rew_mean        | 5.34     |\n",
      "| time/                 |          |\n",
      "|    fps                | 225      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 389      |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.594   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | -0.157   |\n",
      "|    value_loss         | 0.133    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 451      |\n",
      "|    ep_rew_mean        | 5.3      |\n",
      "| time/                 |          |\n",
      "|    fps                | 226      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 398      |\n",
      "|    total_timesteps    | 90000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.521   |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 0.098    |\n",
      "|    value_loss         | 0.0672   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 443      |\n",
      "|    ep_rew_mean        | 5.06     |\n",
      "| time/                 |          |\n",
      "|    fps                | 226      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 406      |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.564   |\n",
      "|    explained_variance | 0.749    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | 0.0595   |\n",
      "|    value_loss         | 0.131    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 453      |\n",
      "|    ep_rew_mean        | 5.23     |\n",
      "| time/                 |          |\n",
      "|    fps                | 226      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 414      |\n",
      "|    total_timesteps    | 94000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.538   |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -0.0498  |\n",
      "|    value_loss         | 0.0506   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 469      |\n",
      "|    ep_rew_mean        | 5.49     |\n",
      "| time/                 |          |\n",
      "|    fps                | 226      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 423      |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.816   |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 0.084    |\n",
      "|    value_loss         | 0.155    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 467      |\n",
      "|    ep_rew_mean        | 5.46     |\n",
      "| time/                 |          |\n",
      "|    fps                | 227      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 431      |\n",
      "|    total_timesteps    | 98000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.574   |\n",
      "|    explained_variance | 0.947    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | -0.325   |\n",
      "|    value_loss         | 0.115    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 477      |\n",
      "|    ep_rew_mean        | 5.58     |\n",
      "| time/                 |          |\n",
      "|    fps                | 227      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 439      |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.511   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | 0.0491   |\n",
      "|    value_loss         | 0.0287   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x20512590520>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c_path = os.path.join('train','model','A2C')\n",
    "model.save(a2c_path)\n",
    "model.load(a2c_path,env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.175, 2.8362607425975486)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_atari_env('Breakout-v0',n_envs=1,seed=1)\n",
    "env  = VecFrameStack(env,n_stack=4)\n",
    "evaluate_policy(model,env,n_eval_episodes=40,render=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
