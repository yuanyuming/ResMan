{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.exploration import GaussianNoise\n",
    "from tianshou.policy import DDPGPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.continuous import Actor, Critic\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--task', type=str, default='Pendulum-v1')\n",
    "    parser.add_argument('--reward-threshold', type=float, default=None)\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "    parser.add_argument('--buffer-size', type=int, default=20000)\n",
    "    parser.add_argument('--actor-lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--critic-lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--tau', type=float, default=0.005)\n",
    "    parser.add_argument('--exploration-noise', type=float, default=0.1)\n",
    "    parser.add_argument('--epoch', type=int, default=5)\n",
    "    parser.add_argument('--step-per-epoch', type=int, default=20000)\n",
    "    parser.add_argument('--step-per-collect', type=int, default=8)\n",
    "    parser.add_argument('--update-per-step', type=float, default=0.125)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--hidden-sizes', type=int, nargs='*', default=[128, 128])\n",
    "    parser.add_argument('--training-num', type=int, default=8)\n",
    "    parser.add_argument('--test-num', type=int, default=100)\n",
    "    parser.add_argument('--logdir', type=str, default='log')\n",
    "    parser.add_argument('--render', type=float, default=0.)\n",
    "    parser.add_argument('--rew-norm', action=\"store_true\", default=False)\n",
    "    parser.add_argument('--n-step', type=int, default=3)\n",
    "    parser.add_argument(\n",
    "        '--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    args = parser.parse_known_args()[0]\n",
    "    return args\n",
    "\n",
    "\n",
    "def test_ddpg(args=get_args()):\n",
    "    env = gym.make(args.task)\n",
    "    args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "    args.action_shape = env.action_space.shape or env.action_space.n\n",
    "    args.max_action = env.action_space.high[0]\n",
    "    if args.reward_threshold is None:\n",
    "        default_reward_threshold = {\"Pendulum-v0\": -250, \"Pendulum-v1\": -250}\n",
    "        args.reward_threshold = default_reward_threshold.get(\n",
    "            args.task, env.spec.reward_threshold\n",
    "        )\n",
    "    # you can also use tianshou.env.SubprocVectorEnv\n",
    "    # train_envs = gym.make(args.task)\n",
    "    train_envs = DummyVectorEnv(\n",
    "        [lambda: gym.make(args.task) for _ in range(args.training_num)]\n",
    "    )\n",
    "    # test_envs = gym.make(args.task)\n",
    "    test_envs = DummyVectorEnv(\n",
    "        [lambda: gym.make(args.task) for _ in range(args.test_num)]\n",
    "    )\n",
    "    # seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    train_envs.seed(args.seed)\n",
    "    test_envs.seed(args.seed)\n",
    "    # model\n",
    "    net = Net(args.state_shape, hidden_sizes=args.hidden_sizes, device=args.device)\n",
    "    actor = Actor(\n",
    "        net, args.action_shape, max_action=args.max_action, device=args.device\n",
    "    ).to(args.device)\n",
    "    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n",
    "    net = Net(\n",
    "        args.state_shape,\n",
    "        args.action_shape,\n",
    "        hidden_sizes=args.hidden_sizes,\n",
    "        concat=True,\n",
    "        device=args.device\n",
    "    )\n",
    "    critic = Critic(net, device=args.device).to(args.device)\n",
    "    critic_optim = torch.optim.Adam(critic.parameters(), lr=args.critic_lr)\n",
    "    policy = DDPGPolicy(\n",
    "        actor,\n",
    "        actor_optim,\n",
    "        critic,\n",
    "        critic_optim,\n",
    "        tau=args.tau,\n",
    "        gamma=args.gamma,\n",
    "        exploration_noise=GaussianNoise(sigma=args.exploration_noise),\n",
    "        reward_normalization=args.rew_norm,\n",
    "        estimation_step=args.n_step,\n",
    "        action_space=env.action_space\n",
    "    )\n",
    "    # collector\n",
    "    train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(args.buffer_size, len(train_envs)),\n",
    "        exploration_noise=True\n",
    "    )\n",
    "    test_collector = Collector(policy, test_envs)\n",
    "    # log\n",
    "    log_path = os.path.join(args.logdir, args.task, 'ddpg')\n",
    "    writer = SummaryWriter(log_path)\n",
    "    logger = TensorboardLogger(writer)\n",
    "\n",
    "    def save_best_fn(policy):\n",
    "        torch.save(policy.state_dict(), os.path.join(log_path, 'policy.pth'))\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= args.reward_threshold\n",
    "\n",
    "    # trainer\n",
    "    result = offpolicy_trainer(\n",
    "        policy,\n",
    "        train_collector,\n",
    "        test_collector,\n",
    "        args.epoch,\n",
    "        args.step_per_epoch,\n",
    "        args.step_per_collect,\n",
    "        args.test_num,\n",
    "        args.batch_size,\n",
    "        update_per_step=args.update_per_step,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        logger=logger\n",
    "    )\n",
    "    assert stop_fn(result['best_reward'])\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        pprint.pprint(result)\n",
    "        # Let's watch its performance!\n",
    "        env = gym.make(args.task)\n",
    "        policy.eval()\n",
    "        collector = Collector(policy, env)\n",
    "        result = collector.collect(n_episode=1, render=args.render)\n",
    "        rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "        print(f\"Final reward: {rews.mean()}, length: {lens.mean()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 20001it [00:34, 571.69it/s, env_step=20000, len=200, loss/actor=129.021, loss/critic=11.332, n/ep=0, n/st=8, rew=-557.07]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -697.020395 ± 55.156102, best_reward: -697.020395 ± 55.156102 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2:  52%|#####2    | 10400/20000 [00:16<00:15, 612.36it/s, env_step=30400, len=200, n/ep=8, n/st=8, rew=-180.24]                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_result': '-149.28 ± 80.51',\n",
      " 'best_reward': -149.27721713256952,\n",
      " 'duration': '56.71s',\n",
      " 'test_episode': 300,\n",
      " 'test_speed': '9511.39 step/s',\n",
      " 'test_step': 60000,\n",
      " 'test_time': '6.31s',\n",
      " 'train_episode': 152,\n",
      " 'train_speed': '603.14 step/s',\n",
      " 'train_step': 30400,\n",
      " 'train_time/collector': '8.64s',\n",
      " 'train_time/model': '41.77s'}\n",
      "Final reward: -128.97530423652887, length: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/tianshou/data/collector.py:68: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    }
   ],
   "source": [
    "test_ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import gym\n",
    "import tianshou as ts\n",
    "import Environment\n",
    "from tianshou.env import (\n",
    "    ContinuousToDiscrete,\n",
    "    DummyVectorEnv,\n",
    "    PettingZooEnv,\n",
    "    ShmemVectorEnv,\n",
    "    SubprocVectorEnv,\n",
    ")\n",
    "from pettingzoo.utils.wrappers import BaseWrapper\n",
    "# define a multi-agent environment\n",
    "\n",
    "# import the predefined networks from tianshou\n",
    "from tianshou.utils.net.common import ActorCritic, Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_env():\n",
    "    env = Environment.VehicleJobSchedulingEnvACE()\n",
    "    env = BaseWrapper(env)\n",
    "    env = PettingZooEnv(env)\n",
    "    return env\n",
    "env = get_env()\n",
    "num_agents = env.num_agents\n",
    "obs_shape,*_ = env.observation_space.shape\n",
    "act_shape = env.action_space.n \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a policy network and a critic network for each agent\n",
    "policy_nets = [Net(obs_shape, act_shape, hidden_sizes=[64], ) for _ in range(num_agents)] # use output_size=act_shape.sum() for multi-discrete action space\n",
    "critic_nets = [Net(obs_shape * num_agents + act_shape * num_agents, hidden_sizes=[64], action_shape=1) for _ in range(num_agents)] # use act_shape.sum() instead of act_shape for multi-discrete action space\n",
    "\n",
    "# define a policy and a critic for each agent using tianshou and the predefined networks\n",
    "policies = []\n",
    "for i in range(num_agents):\n",
    "    # use different optimizers and parameters for policy and critic networks\n",
    "    optim_p = torch.optim.Adam(policy_nets[i].parameters(), lr=1e-4)\n",
    "    optim_c = torch.optim.Adam(critic_nets[i].parameters(), lr=1e-3)\n",
    "    # use DDPGPolicy\n",
    "    policy = ts.policy.DDPGPolicy(actor=policy_nets[i], actor_optim=optim_p,critic=critic_nets[i], critic_optim=optim_c,gamma=0.95,tau=0.01)\n",
    "    policies.append(policy)\n",
    "maddpg = ts.policy.MultiAgentPolicyManager(policies,env)\n",
    "# define a replay buffer to store transitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = SubprocVectorEnv([lambda: env for _ in range(10)])\n",
    "test_env = SubprocVectorEnv([lambda: env for _ in range(10)])\n",
    "buffer = ts.data.VectorReplayBuffer(total_size=100000,buffer_num=10)\n",
    "# define a collector to interact with the environment and collect data\n",
    "train_collector = ts.data.Collector(maddpg, train_env, buffer)\n",
    "test_collector = ts.data.Collector(maddpg, test_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 10,\n",
       " 'n/st': 4412,\n",
       " 'rews': array([[  50.13333333, 1038.93333333,  711.26666667,  611.15555556,\n",
       "          284.53333333,  449.84444444,  449.97777778,  197.31111111,\n",
       "          132.02222222,   69.13333333],\n",
       "        [  10.66666667,   53.66666667,  123.06666667,  383.53333333,\n",
       "          295.26666667,   84.        ,  861.73333333,  256.28888889,\n",
       "          401.28888889,  273.2       ],\n",
       "        [ 122.22222222,  498.        ,  670.68888889,  261.68888889,\n",
       "           88.84444444,  352.73333333,  292.48888889,  141.82222222,\n",
       "           48.13333333, 1352.13333333],\n",
       "        [ 310.88888889,  391.15555556,  509.6       ,  338.26666667,\n",
       "          189.8       ,  988.        ,  259.84444444,   43.24444444,\n",
       "          512.48888889,    0.        ],\n",
       "        [  14.31111111, 2008.22222222,  408.95555556,  499.42222222,\n",
       "          282.82222222,  853.08888889,  136.48888889,    0.        ,\n",
       "          159.75555556,  224.33333333],\n",
       "        [  43.55555556,  628.24444444,  666.84444444,  234.4       ,\n",
       "          437.02222222,  228.44444444,  258.95555556,  354.06666667,\n",
       "          179.77777778,   46.86666667],\n",
       "        [   0.        ,  557.66666667,  203.93333333,  142.66666667,\n",
       "          219.95555556,  625.22222222,  482.51111111,   47.6       ,\n",
       "          431.28888889,   12.53333333],\n",
       "        [  67.8       ,  269.37777778,   65.33333333,  691.97777778,\n",
       "          214.82222222,  377.73333333,  398.93333333,   77.2       ,\n",
       "          262.88888889,   98.        ],\n",
       "        [ 401.11111111,  221.06666667,  340.33333333,  693.15555556,\n",
       "          840.68888889,  428.2       ,  460.68888889,  194.11111111,\n",
       "          104.        ,  149.6       ],\n",
       "        [ 484.08888889,  878.2       ,  309.37777778,  654.82222222,\n",
       "          276.84444444,  600.31111111,  332.66666667,  266.        ,\n",
       "          425.28888889,   15.66666667]]),\n",
       " 'lens': array([396, 401, 404, 431, 434, 439, 451, 465, 493, 498]),\n",
       " 'idxs': array([20000, 10000, 60000, 40000, 70000,     0, 30000, 80000, 90000,\n",
       "        50000]),\n",
       " 'rew': 350.98311111111104,\n",
       " 'len': 441.2,\n",
       " 'rew_std': 309.2604896051463,\n",
       " 'len_std': 34.31559412278913}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_collector.collect(n_episode=10, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# define a trainer to train the policies and critics\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[39m=\u001b[39m ts\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moffpolicy_trainer(\n\u001b[1;32m      3\u001b[0m     policy\u001b[39m=\u001b[39m maddpg,\n\u001b[1;32m      4\u001b[0m     buffer\u001b[39m=\u001b[39mbuffer,\n\u001b[1;32m      5\u001b[0m     train_collector \u001b[39m=\u001b[39m train_collector,\n\u001b[1;32m      6\u001b[0m     test_collector \u001b[39m=\u001b[39m test_collector,\n\u001b[1;32m      7\u001b[0m     max_epoch\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m,\n\u001b[1;32m      8\u001b[0m     step_per_epoch\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m,\n\u001b[1;32m      9\u001b[0m     step_per_collect\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m,\n\u001b[1;32m     10\u001b[0m     episode_per_test\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     11\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m,\n\u001b[1;32m     12\u001b[0m     update_per_step\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[1;32m     13\u001b[0m     test_in_train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     \n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ts' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# define a trainer to train the policies and critics\n",
    "trainer = ts.trainer.offpolicy_trainer(\n",
    "    policy= maddpg,\n",
    "    buffer=buffer,\n",
    "    train_collector = train_collector,\n",
    "    test_collector = test_collector,\n",
    "    max_epoch=5000,\n",
    "    step_per_epoch=10000,\n",
    "    step_per_collect=500,\n",
    "    episode_per_test=10,\n",
    "    batch_size=64,\n",
    "    update_per_step=0.1,\n",
    "    test_in_train=False,\n",
    "    \n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
