{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "from pettingzoo.utils.wrappers import BaseWrapper\n",
    "import Environment\n",
    "\n",
    "def _get_agents(\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    env = _get_env()\n",
    "    observation_space = (\n",
    "        env.observation_space[\"observation\"]\n",
    "        if isinstance(env.observation_space, gym.spaces.Dict)\n",
    "        else env.observation_space\n",
    "    )\n",
    "    agents = []\n",
    "    for i in range(env.num_agents):\n",
    "        # model\n",
    "        net = Net(\n",
    "            state_shape=observation_space.shape or observation_space.n,\n",
    "            action_shape=env.action_space.shape or env.action_space.n,\n",
    "            hidden_sizes=[128, 128, 128, 128],\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "        agent_learn = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor=0.9,\n",
    "            estimation_step=3,\n",
    "            target_update_freq=320,\n",
    "        )\n",
    "        agents.append(agent_learn)\n",
    "        \n",
    "\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, optim, env.agents\n",
    "\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"\n",
    "    # return PettingZooEnv(tictactoe_v3.env())\n",
    "    return get_env()\n",
    "\n",
    "def get_env():\n",
    "    env = Environment.VehicleJobSchedulingEnvACE()\n",
    "    env = BaseWrapper(env)\n",
    "    env = PettingZooEnv(env)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Step 1: Environment setup =========\n",
    "train_envs = DummyVectorEnv([_get_env for _ in range(10)])\n",
    "test_envs = DummyVectorEnv([_get_env for _ in range(10)])\n",
    "# seed\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "train_envs.seed(seed)\n",
    "test_envs.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Step 2: Agent setup =========\n",
    "policy, optim, agents = _get_agents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Step 3: Collector setup =========\n",
    "train_collector = Collector(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    VectorReplayBuffer(20_000, len(train_envs)),\n",
    "    exploration_noise=True,\n",
    ")\n",
    "test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "# policy.set_eps(1)\n",
    "train_collector.collect(n_step=64 * 1000)  # batch size * training_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Step 4: Callback functions setup =========\n",
    "def save_best_fn(policy):\n",
    "    model_save_path = os.path.join(\"log\", \"rps\", \"dqn\", \"policy.pth\")\n",
    "    os.makedirs(os.path.join(\"log\", \"rps\", \"dqn\"), exist_ok=True)\n",
    "    torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "def reward_metric(rews):\n",
    "    return rews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Step 5: Run the trainer =========\n",
    "result = offpolicy_trainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=5000,\n",
    "    step_per_epoch=10000,\n",
    "    step_per_collect=500,\n",
    "    episode_per_test=10,\n",
    "    batch_size=64,\n",
    "\n",
    "    save_best_fn=save_best_fn,\n",
    "    update_per_step=0.1,\n",
    "    test_in_train=true,\n",
    "    reward_metric=lambda rews:rews,\n",
    ")\n",
    "# return result, policy.policies[agents[1]]\n",
    "print(f\"\\n==========Result==========\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.pretty import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(\"DQNPolicy:\")\n",
    "rslt = test_collector.collect(n_episode=10)\n",
    "pprint(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load environment\n",
    "env = Environment.VehicleJobSchedulingEnvACE()\n",
    "# 2. Wrap the environment with vectorized wrapper\n",
    "env = BaseWrapper(env)\n",
    "agents = env.num_agents\n",
    "env = PettingZooEnv(env)\n",
    "policies = MultiAgentPolicyManager([RandomPolicy() for _ in range(10)], env)\n",
    "# env = SubprocVectorEnv([lambda: env for _ in range(5)])\n",
    "env = DummyVectorEnv([lambda: env for _ in range(1)])\n",
    "# action_space = env.action_spaces[env.agents[0]]\n",
    "# 3. Create policy\n",
    "# Runtime Environment\n",
    "# 4. Create collector\n",
    "collector = Collector(policies, env)\n",
    "# 5. Execute one episode\n",
    "result = collector.collect(n_episode=10)\n",
    "pprint(\"RandomPolicy:\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
