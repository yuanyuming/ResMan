{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllib_setup import get_env\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "alg_name = \"DQN\"\n",
    "env_name = \"VJS\"\n",
    "register_env(env_name,lambda config: get_env())\n",
    "\n",
    "test_env = get_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "def policies(agent_ids):\n",
    "    return {\n",
    "        str(i): (\n",
    "            None,\n",
    "            obs_space,\n",
    "            act_space,\n",
    "            {}\n",
    "            # config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    DQNConfig()\n",
    "    .environment(env=env_name, disable_env_checking=True)\n",
    "    .rollouts(num_rollout_workers=10, rollout_fragment_length=300,create_env_on_local_worker=True,num_envs_per_worker=1,)\n",
    "    .training(\n",
    "        train_batch_size=200,\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies=policies(test_env._agent_ids),\n",
    "        policy_mapping_fn=(lambda agent_id, *args, **kwargs: agent_id),\n",
    "    )\n",
    "    .framework(framework=\"torch\")\n",
    "    .exploration(\n",
    "        exploration_config={\n",
    "            # The Exploration class to use.\n",
    "            \"type\": \"EpsilonGreedy\",\n",
    "            # Config for the Exploration class' constructor:\n",
    "            \"initial_epsilon\": 0.1,\n",
    "            \"final_epsilon\": 0.0,\n",
    "            \"epsilon_timesteps\": 100000,  # Timesteps over which to anneal epsilon.\n",
    "        }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:442: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1923.1690673828125\n"
     ]
    }
   ],
   "source": [
    "for j in range(1):\n",
    "    for i in range(10):\n",
    "        info = algo.train()\n",
    "    print(info['info']['learner']['Machine_0']['mean_td_error'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'sampler_results': {'episode_reward_max': 21025115.43749996,\n",
       "   'episode_reward_min': 21025115.43749996,\n",
       "   'episode_reward_mean': 21025115.43749996,\n",
       "   'episode_len_mean': 4743.0,\n",
       "   'episode_media': {},\n",
       "   'episodes_this_iter': 1,\n",
       "   'policy_reward_min': {'Machine_5': 3852487.125000001,\n",
       "    'Machine_2': 2610096.0750000016,\n",
       "    'Machine_3': -1014.75,\n",
       "    'Machine_6': 1017248.2874999996,\n",
       "    'Machine_7': 5439752.1,\n",
       "    'Machine_4': 1507001.0624999993,\n",
       "    'Machine_8': 242538.41250000006,\n",
       "    'Machine_1': 2613772.6875000005,\n",
       "    'Machine_0': 530206.7625,\n",
       "    'Machine_9': 1435930.5375000003,\n",
       "    'Machine_10': 529230.3750000007,\n",
       "    'Machine_11': 1247866.7625},\n",
       "   'policy_reward_max': {'Machine_5': 3852487.125000001,\n",
       "    'Machine_2': 2610096.0750000016,\n",
       "    'Machine_3': -1014.75,\n",
       "    'Machine_6': 1017248.2874999996,\n",
       "    'Machine_7': 5439752.1,\n",
       "    'Machine_4': 1507001.0624999993,\n",
       "    'Machine_8': 242538.41250000006,\n",
       "    'Machine_1': 2613772.6875000005,\n",
       "    'Machine_0': 530206.7625,\n",
       "    'Machine_9': 1435930.5375000003,\n",
       "    'Machine_10': 529230.3750000007,\n",
       "    'Machine_11': 1247866.7625},\n",
       "   'policy_reward_mean': {'Machine_5': 3852487.125000001,\n",
       "    'Machine_2': 2610096.0750000016,\n",
       "    'Machine_3': -1014.75,\n",
       "    'Machine_6': 1017248.2874999996,\n",
       "    'Machine_7': 5439752.1,\n",
       "    'Machine_4': 1507001.0624999993,\n",
       "    'Machine_8': 242538.41250000006,\n",
       "    'Machine_1': 2613772.6875000005,\n",
       "    'Machine_0': 530206.7625,\n",
       "    'Machine_9': 1435930.5375000003,\n",
       "    'Machine_10': 529230.3750000007,\n",
       "    'Machine_11': 1247866.7625},\n",
       "   'custom_metrics': {},\n",
       "   'hist_stats': {'episode_reward': [21025115.43749996],\n",
       "    'episode_lengths': [4743],\n",
       "    'policy_Machine_5_reward': [3852487.125000001],\n",
       "    'policy_Machine_2_reward': [2610096.0750000016],\n",
       "    'policy_Machine_3_reward': [-1014.75],\n",
       "    'policy_Machine_6_reward': [1017248.2874999996],\n",
       "    'policy_Machine_7_reward': [5439752.1],\n",
       "    'policy_Machine_4_reward': [1507001.0624999993],\n",
       "    'policy_Machine_8_reward': [242538.41250000006],\n",
       "    'policy_Machine_1_reward': [2613772.6875000005],\n",
       "    'policy_Machine_0_reward': [530206.7625],\n",
       "    'policy_Machine_9_reward': [1435930.5375000003],\n",
       "    'policy_Machine_10_reward': [529230.3750000007],\n",
       "    'policy_Machine_11_reward': [1247866.7625]},\n",
       "   'sampler_perf': {'mean_raw_obs_processing_ms': 0.491214376271381,\n",
       "    'mean_inference_ms': 3.699381301166304,\n",
       "    'mean_action_processing_ms': 0.19032488441130332,\n",
       "    'mean_env_wait_ms': 0.2262337295767713,\n",
       "    'mean_env_render_ms': 0.0},\n",
       "   'num_faulty_episodes': 0,\n",
       "   'connector_metrics': {'ObsPreprocessorConnector_ms': 0.008988380432128906,\n",
       "    'StateBufferConnector_ms': 0.006389617919921875,\n",
       "    'ViewRequirementAgentConnector_ms': 0.18624067306518555}},\n",
       "  'episode_reward_max': 21025115.43749996,\n",
       "  'episode_reward_min': 21025115.43749996,\n",
       "  'episode_reward_mean': 21025115.43749996,\n",
       "  'episode_len_mean': 4743.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 1,\n",
       "  'policy_reward_min': {'Machine_5': 3852487.125000001,\n",
       "   'Machine_2': 2610096.0750000016,\n",
       "   'Machine_3': -1014.75,\n",
       "   'Machine_6': 1017248.2874999996,\n",
       "   'Machine_7': 5439752.1,\n",
       "   'Machine_4': 1507001.0624999993,\n",
       "   'Machine_8': 242538.41250000006,\n",
       "   'Machine_1': 2613772.6875000005,\n",
       "   'Machine_0': 530206.7625,\n",
       "   'Machine_9': 1435930.5375000003,\n",
       "   'Machine_10': 529230.3750000007,\n",
       "   'Machine_11': 1247866.7625},\n",
       "  'policy_reward_max': {'Machine_5': 3852487.125000001,\n",
       "   'Machine_2': 2610096.0750000016,\n",
       "   'Machine_3': -1014.75,\n",
       "   'Machine_6': 1017248.2874999996,\n",
       "   'Machine_7': 5439752.1,\n",
       "   'Machine_4': 1507001.0624999993,\n",
       "   'Machine_8': 242538.41250000006,\n",
       "   'Machine_1': 2613772.6875000005,\n",
       "   'Machine_0': 530206.7625,\n",
       "   'Machine_9': 1435930.5375000003,\n",
       "   'Machine_10': 529230.3750000007,\n",
       "   'Machine_11': 1247866.7625},\n",
       "  'policy_reward_mean': {'Machine_5': 3852487.125000001,\n",
       "   'Machine_2': 2610096.0750000016,\n",
       "   'Machine_3': -1014.75,\n",
       "   'Machine_6': 1017248.2874999996,\n",
       "   'Machine_7': 5439752.1,\n",
       "   'Machine_4': 1507001.0624999993,\n",
       "   'Machine_8': 242538.41250000006,\n",
       "   'Machine_1': 2613772.6875000005,\n",
       "   'Machine_0': 530206.7625,\n",
       "   'Machine_9': 1435930.5375000003,\n",
       "   'Machine_10': 529230.3750000007,\n",
       "   'Machine_11': 1247866.7625},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [21025115.43749996],\n",
       "   'episode_lengths': [4743],\n",
       "   'policy_Machine_5_reward': [3852487.125000001],\n",
       "   'policy_Machine_2_reward': [2610096.0750000016],\n",
       "   'policy_Machine_3_reward': [-1014.75],\n",
       "   'policy_Machine_6_reward': [1017248.2874999996],\n",
       "   'policy_Machine_7_reward': [5439752.1],\n",
       "   'policy_Machine_4_reward': [1507001.0624999993],\n",
       "   'policy_Machine_8_reward': [242538.41250000006],\n",
       "   'policy_Machine_1_reward': [2613772.6875000005],\n",
       "   'policy_Machine_0_reward': [530206.7625],\n",
       "   'policy_Machine_9_reward': [1435930.5375000003],\n",
       "   'policy_Machine_10_reward': [529230.3750000007],\n",
       "   'policy_Machine_11_reward': [1247866.7625]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.491214376271381,\n",
       "   'mean_inference_ms': 3.699381301166304,\n",
       "   'mean_action_processing_ms': 0.19032488441130332,\n",
       "   'mean_env_wait_ms': 0.2262337295767713,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': 0.008988380432128906,\n",
       "   'StateBufferConnector_ms': 0.006389617919921875,\n",
       "   'ViewRequirementAgentConnector_ms': 0.18624067306518555},\n",
       "  'num_agent_steps_sampled_this_iter': 3000,\n",
       "  'num_env_steps_sampled_this_iter': 3000,\n",
       "  'timesteps_this_iter': 3000}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:15:17,588 E 881477 881477] (raylet) node_manager.cc:3069: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: e8c50b41e1219f6981aee0ea755f4b41bc398917a8b23b26abb70544, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:16:17,589 E 881477 881477] (raylet) node_manager.cc:3069: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: e8c50b41e1219f6981aee0ea755f4b41bc398917a8b23b26abb70544, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:18:18,886 E 881477 881477] (raylet) node_manager.cc:3069: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: e8c50b41e1219f6981aee0ea755f4b41bc398917a8b23b26abb70544, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:19:18,889 E 881477 881477] (raylet) node_manager.cc:3069: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: e8c50b41e1219f6981aee0ea755f4b41bc398917a8b23b26abb70544, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:22:18,893 E 881477 881477] (raylet) node_manager.cc:3069: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: e8c50b41e1219f6981aee0ea755f4b41bc398917a8b23b26abb70544, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:23:18,894 E 881477 881477] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: e8c50b41e1219f6981aee0ea755f4b41bc398917a8b23b26abb70544, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:25:19,041 E 881477 881477] (raylet) node_manager.cc:3069: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: e8c50b41e1219f6981aee0ea755f4b41bc398917a8b23b26abb70544, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    info = algo.evaluate()\n",
    "    \n",
    "info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
