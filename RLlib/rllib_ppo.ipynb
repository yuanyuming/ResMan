{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune import register_env\n",
    "\n",
    "import rllib_setup\n",
    "\n",
    "ray.init()\n",
    "env_name = \"VJS\"\n",
    "register_env(\n",
    "    env_name,\n",
    "    lambda config: rllib_setup.get_env_continuous(),\n",
    ")\n",
    "test_env = rllib_setup.get_env_continuous()\n",
    "\n",
    "\n",
    "def policies(agent_ids):\n",
    "    obs_space = test_env.observation_space\n",
    "    act_space = test_env.action_space\n",
    "    return {\n",
    "        str(i): (\n",
    "            None,\n",
    "            obs_space,\n",
    "            act_space,\n",
    "            {}\n",
    "            # config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig().rollouts(num_rollout_workers=10)\n",
    "    .training(gamma=0.9, lr=0.01) \n",
    "    .resources(num_gpus=1)\n",
    "    .multi_agent(\n",
    "        policies=policies(test_env._agent_ids),\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: str(agent_id),\n",
    "    )\n",
    "    .environment(disable_env_checking=True)\n",
    ")\n",
    "print(config.to_dict())\n",
    "# Build a Algorithm object from the config and run one training iteration.\n",
    "algo = config.build(env=env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:19:28,025 E 3473473 3473473] (raylet) node_manager.cc:3069: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 9648b9014e4abaede66aee64b1344246a1b2b8bbe34248617d11f84d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (46, 1)) of distribution Normal(loc: torch.Size([46, 1]), scale: torch.Size([46, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)\n tracebackTraceback (most recent call last):\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1235, in _worker\n    self.loss(model, self.dist_class, sample_batch)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py\", line 85, in loss\n    curr_action_dist = dist_class(logits, model)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/models/torch/torch_action_dist.py\", line 250, in __init__\n    self.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/normal.py\", line 56, in __init__\n    super().__init__(batch_shape, validate_args=validate_args)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 62, in __init__\n    raise ValueError(\nValueError: Expected parameter loc (Tensor of shape (46, 1)) of distribution Normal(loc: torch.Size([46, 1]), scale: torch.Size([46, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)\n\nIn tower 0 on device cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py:1235\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc.<locals>._worker\u001b[0;34m(shard_idx, model, sample_batch, device)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[39mwith\u001b[39;00m NullContextManager() \u001b[39mif\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     device\n\u001b[1;32m   1233\u001b[0m ):\n\u001b[1;32m   1234\u001b[0m     loss_out \u001b[39m=\u001b[39m force_list(\n\u001b[0;32m-> 1235\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdist_class, sample_batch)\n\u001b[1;32m   1236\u001b[0m     )\n\u001b[1;32m   1238\u001b[0m     \u001b[39m# Call Model's custom-loss with Policy loss outputs and\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m     \u001b[39m# train_batch.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py:85\u001b[0m, in \u001b[0;36mPPOTorchPolicy.loss\u001b[0;34m(self, model, dist_class, train_batch)\u001b[0m\n\u001b[1;32m     84\u001b[0m logits, state \u001b[39m=\u001b[39m model(train_batch)\n\u001b[0;32m---> 85\u001b[0m curr_action_dist \u001b[39m=\u001b[39m dist_class(logits, model)\n\u001b[1;32m     87\u001b[0m \u001b[39m# RNN case: Mask away 0-padded chunks at end of time axis.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/models/torch/torch_action_dist.py:250\u001b[0m, in \u001b[0;36mTorchDiagGaussian.__init__\u001b[0;34m(self, inputs, model, action_space)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_std \u001b[39m=\u001b[39m log_std\n\u001b[0;32m--> 250\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdistributions\u001b[39m.\u001b[39;49mnormal\u001b[39m.\u001b[39;49mNormal(mean, torch\u001b[39m.\u001b[39;49mexp(log_std))\n\u001b[1;32m    251\u001b[0m \u001b[39m# Remember to squeeze action samples in case action space is Box(shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/torch/distributions/normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     55\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 56\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/torch/distributions/distribution.py:62\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 62\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m             )\n\u001b[1;32m     69\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (46, 1)) of distribution Normal(loc: torch.Size([46, 1]), scale: torch.Size([46, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/yuan/ResMan/RLlib/rllib_mappo.ipynb 单元格 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_mappo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_mappo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_mappo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         info \u001b[39m=\u001b[39m algo\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_mappo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         info[\u001b[39m'\u001b[39m\u001b[39minfo\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlearner\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mMachine_6\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlearner_stats\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtotal_loss\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:389\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mraise\u001b[39;00m skipped \u001b[39mfrom\u001b[39;00m \u001b[39mexception_cause\u001b[39;00m(skipped)\n\u001b[1;32m    391\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mstep() needs to return a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m \u001b[39m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:386\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:803\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m     (\n\u001b[1;32m    796\u001b[0m         results,\n\u001b[1;32m    797\u001b[0m         train_iter_ctx,\n\u001b[1;32m    798\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    799\u001b[0m \u001b[39m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[39m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     results, train_iter_ctx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_training_iteration()\n\u001b[1;32m    805\u001b[0m \u001b[39m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[39mif\u001b[39;00m evaluate_this_iter \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:2853\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2852\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[0;32m-> 2853\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step()\n\u001b[1;32m   2854\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2855\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo.py:430\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m     train_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearner_group\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    424\u001b[0m         train_batch,\n\u001b[1;32m    425\u001b[0m         minibatch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39msgd_minibatch_size,\n\u001b[1;32m    426\u001b[0m         num_iters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_sgd_iter,\n\u001b[1;32m    427\u001b[0m     )\n\u001b[1;32m    429\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39msimple_optimizer:\n\u001b[0;32m--> 430\u001b[0m     train_results \u001b[39m=\u001b[39m train_one_step(\u001b[39mself\u001b[39;49m, train_batch)\n\u001b[1;32m    431\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     train_results \u001b[39m=\u001b[39m multi_gpu_train_one_step(\u001b[39mself\u001b[39m, train_batch)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/execution/train_ops.py:52\u001b[0m, in \u001b[0;36mtrain_one_step\u001b[0;34m(algorithm, train_batch, policies_to_train)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mwith\u001b[39;00m learn_timer:\n\u001b[1;32m     49\u001b[0m     \u001b[39m# Subsample minibatches (size=`sgd_minibatch_size`) from the\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[39m# train batch and loop through train batch `num_sgd_iter` times.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[39mif\u001b[39;00m num_sgd_iter \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m sgd_minibatch_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m         info \u001b[39m=\u001b[39m do_minibatch_sgd(\n\u001b[1;32m     53\u001b[0m             train_batch,\n\u001b[1;32m     54\u001b[0m             {\n\u001b[1;32m     55\u001b[0m                 pid: local_worker\u001b[39m.\u001b[39;49mget_policy(pid)\n\u001b[1;32m     56\u001b[0m                 \u001b[39mfor\u001b[39;49;00m pid \u001b[39min\u001b[39;49;00m policies_to_train\n\u001b[1;32m     57\u001b[0m                 \u001b[39mor\u001b[39;49;00m local_worker\u001b[39m.\u001b[39;49mget_policies_to_train(train_batch)\n\u001b[1;32m     58\u001b[0m             },\n\u001b[1;32m     59\u001b[0m             local_worker,\n\u001b[1;32m     60\u001b[0m             num_sgd_iter,\n\u001b[1;32m     61\u001b[0m             sgd_minibatch_size,\n\u001b[1;32m     62\u001b[0m             [],\n\u001b[1;32m     63\u001b[0m         )\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Single update step using train batch.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m         info \u001b[39m=\u001b[39m local_worker\u001b[39m.\u001b[39mlearn_on_batch(train_batch)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/sgd.py:129\u001b[0m, in \u001b[0;36mdo_minibatch_sgd\u001b[0;34m(samples, policies, local_worker, num_sgd_iter, sgd_minibatch_size, standardize_fields)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_sgd_iter):\n\u001b[1;32m    127\u001b[0m         \u001b[39mfor\u001b[39;00m minibatch \u001b[39min\u001b[39;00m minibatches(batch, sgd_minibatch_size):\n\u001b[1;32m    128\u001b[0m             results \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 129\u001b[0m                 local_worker\u001b[39m.\u001b[39;49mlearn_on_batch(\n\u001b[1;32m    130\u001b[0m                     MultiAgentBatch({policy_id: minibatch}, minibatch\u001b[39m.\u001b[39;49mcount)\n\u001b[1;32m    131\u001b[0m                 )\n\u001b[1;32m    132\u001b[0m             )[policy_id]\n\u001b[1;32m    133\u001b[0m             learner_info_builder\u001b[39m.\u001b[39madd_learn_on_batch_results(results, policy_id)\n\u001b[1;32m    135\u001b[0m learner_info \u001b[39m=\u001b[39m learner_info_builder\u001b[39m.\u001b[39mfinalize()\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:1029\u001b[0m, in \u001b[0;36mRolloutWorker.learn_on_batch\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             to_fetch[pid] \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39m_build_learn_on_batch(builders[pid], batch)\n\u001b[1;32m   1028\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1029\u001b[0m             info_out[pid] \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mlearn_on_batch(batch)\n\u001b[1;32m   1031\u001b[0m     info_out\u001b[39m.\u001b[39mupdate({pid: builders[pid]\u001b[39m.\u001b[39mget(v) \u001b[39mfor\u001b[39;00m pid, v \u001b[39min\u001b[39;00m to_fetch\u001b[39m.\u001b[39mitems()})\n\u001b[1;32m   1032\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n\u001b[1;32m     25\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_lock\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py:669\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_batch\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mon_learn_on_batch(\n\u001b[1;32m    664\u001b[0m     policy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, train_batch\u001b[39m=\u001b[39mpostprocessed_batch, result\u001b[39m=\u001b[39mlearn_stats\n\u001b[1;32m    665\u001b[0m )\n\u001b[1;32m    667\u001b[0m \u001b[39m# Compute gradients (will calculate all losses and `backward()`\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[39m# them to get the grads).\u001b[39;00m\n\u001b[0;32m--> 669\u001b[0m grads, fetches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_gradients(postprocessed_batch)\n\u001b[1;32m    671\u001b[0m \u001b[39m# Step the optimizers.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(_directStepOptimizerSingleton)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n\u001b[1;32m     25\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_lock\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py:873\u001b[0m, in \u001b[0;36mTorchPolicyV2.compute_gradients\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lazy_tensor_dict(postprocessed_batch, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevices[\u001b[39m0\u001b[39m])\n\u001b[1;32m    872\u001b[0m \u001b[39m# Do the (maybe parallelized) gradient calculation step.\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m tower_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_gpu_parallel_grad_calc([postprocessed_batch])\n\u001b[1;32m    875\u001b[0m all_grads, grad_info \u001b[39m=\u001b[39m tower_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    877\u001b[0m grad_info[\u001b[39m\"\u001b[39m\u001b[39mallreduce_latency\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizers)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py:1320\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc\u001b[0;34m(self, sample_batches)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         last_result \u001b[39m=\u001b[39m results[\u001b[39mlen\u001b[39m(results) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m   1319\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(last_result[\u001b[39m0\u001b[39m], \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m-> 1320\u001b[0m             \u001b[39mraise\u001b[39;00m last_result[\u001b[39m0\u001b[39m] \u001b[39mfrom\u001b[39;00m \u001b[39mlast_result\u001b[39;00m[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1321\u001b[0m \u001b[39m# Multi device (GPU) case: Parallelize via threads.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     threads \u001b[39m=\u001b[39m [\n\u001b[1;32m   1324\u001b[0m         threading\u001b[39m.\u001b[39mThread(\n\u001b[1;32m   1325\u001b[0m             target\u001b[39m=\u001b[39m_worker, args\u001b[39m=\u001b[39m(shard_idx, model, sample_batch, device)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         )\n\u001b[1;32m   1330\u001b[0m     ]\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (46, 1)) of distribution Normal(loc: torch.Size([46, 1]), scale: torch.Size([46, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)\n tracebackTraceback (most recent call last):\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1235, in _worker\n    self.loss(model, self.dist_class, sample_batch)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py\", line 85, in loss\n    curr_action_dist = dist_class(logits, model)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/models/torch/torch_action_dist.py\", line 250, in __init__\n    self.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/normal.py\", line 56, in __init__\n    super().__init__(batch_shape, validate_args=validate_args)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 62, in __init__\n    raise ValueError(\nValueError: Expected parameter loc (Tensor of shape (46, 1)) of distribution Normal(loc: torch.Size([46, 1]), scale: torch.Size([46, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)\n\nIn tower 0 on device cuda:0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:21:28,027 E 3473473 3473473] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 9648b9014e4abaede66aee64b1344246a1b2b8bbe34248617d11f84d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:22:28,028 E 3473473 3473473] (raylet) node_manager.cc:3069: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 9648b9014e4abaede66aee64b1344246a1b2b8bbe34248617d11f84d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:25:28,565 E 3473473 3473473] (raylet) node_manager.cc:3069: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 9648b9014e4abaede66aee64b1344246a1b2b8bbe34248617d11f84d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:26:28,567 E 3473473 3473473] (raylet) node_manager.cc:3069: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 9648b9014e4abaede66aee64b1344246a1b2b8bbe34248617d11f84d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    for i in range(10):\n",
    "        info = algo.train()\n",
    "        info['info']['learner']['Machine_6']['learner_stats']['total_loss']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
