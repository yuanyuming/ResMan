{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 13:18:08,144\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:442: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-09-29 13:18:09,578\tINFO algorithm.py:536 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': None, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'is_atari': None, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.9, 'lr': 0.01, 'lr_schedule': None, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'use_critic': True, 'use_gae': True, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'kl_target': 0.01, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'Machine_0': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_1': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_2': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_3': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_4': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_5': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_6': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_7': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_8': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_9': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_10': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_11': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {})}, 'policy_mapping_fn': <function <lambda> at 0x7f321f4c7b80>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 10}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune import register_env\n",
    "\n",
    "import rllib_setup\n",
    "\n",
    "ray.init()\n",
    "env_name = \"VJS\"\n",
    "register_env(\n",
    "    env_name,\n",
    "    lambda config: rllib_setup.get_env_continuous(),\n",
    ")\n",
    "test_env = rllib_setup.get_env_continuous()\n",
    "\n",
    "\n",
    "def policies(agent_ids):\n",
    "    obs_space = test_env.observation_space\n",
    "    act_space = test_env.action_space\n",
    "    return {\n",
    "        str(i): (\n",
    "            None,\n",
    "            obs_space,\n",
    "            act_space,\n",
    "            {}\n",
    "            # config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }\n",
    "\n",
    "\n",
    "config = (\n",
    "    PPOConfig().rollouts(num_rollout_workers=10)\n",
    "    .training(gamma=0.9, lr=0.01) \n",
    "    .resources(num_gpus=1)\n",
    "    .multi_agent(\n",
    "        policies=policies(test_env._agent_ids),\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: str(agent_id),\n",
    "    )\n",
    "    .environment(disable_env_checking=True)\n",
    ")\n",
    "print(config.to_dict())\n",
    "# Build a Algorithm object from the config and run one training iteration.\n",
    "algo = config.build(env=env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 13:18:20,339\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "2023-09-29 13:18:35,081\tWARNING ppo.py:520 -- The mean reward returned from the environment is 380.3554992675781 but the vf_clip_param is set to 10.0. Consider increasing it for policy: Machine_0 to improve value function convergence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'info': {'learner': {'Machine_0': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 6.36249870856603,\n",
       "     'cur_kl_coeff': 0.20000000000000007,\n",
       "     'cur_lr': 0.010000000000000002,\n",
       "     'total_loss': 7.416676092147827,\n",
       "     'policy_loss': 0.20174345221221873,\n",
       "     'vf_loss': 6.943181117375691,\n",
       "     'vf_explained_var': -1.5894571940104166e-08,\n",
       "     'kl': 1.3587572819863756,\n",
       "     'entropy': 2.8487657845020293,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 117.0,\n",
       "    'num_grad_updates_lifetime': 15.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 14.5},\n",
       "   'Machine_7': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 4.486259746799866,\n",
       "     'cur_kl_coeff': 0.20000000000000004,\n",
       "     'cur_lr': 0.01,\n",
       "     'total_loss': 9.149518422285716,\n",
       "     'policy_loss': 0.11943101973518727,\n",
       "     'vf_loss': 8.788565146923066,\n",
       "     'vf_explained_var': -1.2914339701334636e-08,\n",
       "     'kl': 1.207611031892399,\n",
       "     'entropy': 2.6414481381575268,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 109.0,\n",
       "    'num_grad_updates_lifetime': 60.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 59.5},\n",
       "   'Machine_2': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 5.494136558638679,\n",
       "     'cur_kl_coeff': 0.19999999999999996,\n",
       "     'cur_lr': 0.009999999999999998,\n",
       "     'total_loss': 8.424233717388576,\n",
       "     'policy_loss': 0.11740711061283945,\n",
       "     'vf_loss': 8.19208263291253,\n",
       "     'vf_explained_var': -1.0596381293402778e-08,\n",
       "     'kl': 0.5737200116492911,\n",
       "     'entropy': 1.962694878048367,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 102.0,\n",
       "    'num_grad_updates_lifetime': 45.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 44.5},\n",
       "   'Machine_8': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 5.088744387361738,\n",
       "     'cur_kl_coeff': 0.19999999999999996,\n",
       "     'cur_lr': 0.009999999999999998,\n",
       "     'total_loss': 8.761563396453857,\n",
       "     'policy_loss': 0.05032011287597318,\n",
       "     'vf_loss': 8.672107251485189,\n",
       "     'vf_explained_var': -6.622738308376736e-09,\n",
       "     'kl': 0.19568014937621128,\n",
       "     'entropy': 1.605299519168006,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 118.33333333333333,\n",
       "    'num_grad_updates_lifetime': 45.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 44.5},\n",
       "   'Machine_4': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 6.620272280772527,\n",
       "     'cur_kl_coeff': 0.20000000000000004,\n",
       "     'cur_lr': 0.01,\n",
       "     'total_loss': 8.584814548492432,\n",
       "     'policy_loss': 0.20594973783008755,\n",
       "     'vf_loss': 7.963207646210988,\n",
       "     'vf_explained_var': 1.4901161193847657e-09,\n",
       "     'kl': 2.078286118863616,\n",
       "     'entropy': 3.3481075167655945,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 115.75,\n",
       "    'num_grad_updates_lifetime': 60.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 59.5},\n",
       "   'Machine_10': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 7.208124987284342,\n",
       "     'cur_kl_coeff': 0.19999999999999993,\n",
       "     'cur_lr': 0.010000000000000002,\n",
       "     'total_loss': 9.108747625350953,\n",
       "     'policy_loss': 0.16493796687573195,\n",
       "     'vf_loss': 8.623988493283589,\n",
       "     'vf_explained_var': -1.1920928955078126e-08,\n",
       "     'kl': 1.5991060643747914,\n",
       "     'entropy': 2.6998339941104255,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 96.0,\n",
       "    'num_grad_updates_lifetime': 30.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 29.5},\n",
       "   'Machine_9': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 7.186611580848694,\n",
       "     'cur_kl_coeff': 0.19999999999999996,\n",
       "     'cur_lr': 0.009999999999999998,\n",
       "     'total_loss': 9.647531175613404,\n",
       "     'policy_loss': 0.2670835277893477,\n",
       "     'vf_loss': 9.126843235227797,\n",
       "     'vf_explained_var': -2.0530488755967883e-07,\n",
       "     'kl': 1.268022342564331,\n",
       "     'entropy': 2.703180181317859,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 87.0,\n",
       "    'num_grad_updates_lifetime': 45.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 44.5},\n",
       "   'Machine_6': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 4.878361686865489,\n",
       "     'cur_kl_coeff': 0.19999999999999996,\n",
       "     'cur_lr': 0.01,\n",
       "     'total_loss': 9.17884685198466,\n",
       "     'policy_loss': 0.10347584832770129,\n",
       "     'vf_loss': 8.988107992808024,\n",
       "     'vf_explained_var': -5.960464477539063e-09,\n",
       "     'kl': 0.43631538609430814,\n",
       "     'entropy': 1.9027669970194498,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 104.8,\n",
       "    'num_grad_updates_lifetime': 75.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 74.5},\n",
       "   'Machine_5': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 5.539787402550379,\n",
       "     'cur_kl_coeff': 0.19999999999999996,\n",
       "     'cur_lr': 0.01,\n",
       "     'total_loss': 8.732486826578777,\n",
       "     'policy_loss': 0.09620252483213941,\n",
       "     'vf_loss': 8.524745219548544,\n",
       "     'vf_explained_var': 8.742014567057292e-09,\n",
       "     'kl': 0.5576950841854947,\n",
       "     'entropy': 1.9439994884530702,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 103.8,\n",
       "    'num_grad_updates_lifetime': 75.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 74.5},\n",
       "   'Machine_3': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 6.187353541453679,\n",
       "     'cur_kl_coeff': 0.20000000000000004,\n",
       "     'cur_lr': 0.01,\n",
       "     'total_loss': 8.708607665697734,\n",
       "     'policy_loss': 0.19592126226052642,\n",
       "     'vf_loss': 8.260318104426066,\n",
       "     'vf_explained_var': -1.564621925354004e-07,\n",
       "     'kl': 1.2618413254308203,\n",
       "     'entropy': 2.669589367012183,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 97.75,\n",
       "    'num_grad_updates_lifetime': 60.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 59.5},\n",
       "   'Machine_1': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 8.836936975518862,\n",
       "     'cur_kl_coeff': 0.19999999999999993,\n",
       "     'cur_lr': 0.010000000000000002,\n",
       "     'total_loss': 7.795729780197144,\n",
       "     'policy_loss': 0.17261636353408297,\n",
       "     'vf_loss': 7.179729270935058,\n",
       "     'vf_explained_var': -2.0861625671386717e-08,\n",
       "     'kl': 2.21692085613807,\n",
       "     'entropy': 3.1559802134831747,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 107.5,\n",
       "    'num_grad_updates_lifetime': 30.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 29.5},\n",
       "   'Machine_11': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 12.721373462677002,\n",
       "     'cur_kl_coeff': 0.20000000000000007,\n",
       "     'cur_lr': 0.010000000000000002,\n",
       "     'total_loss': 8.672459046045939,\n",
       "     'policy_loss': 0.17115207983498332,\n",
       "     'vf_loss': 7.987310679753621,\n",
       "     'vf_explained_var': -1.1920928955078126e-08,\n",
       "     'kl': 2.569981396943331,\n",
       "     'entropy': 2.9747085213661193,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 111.0,\n",
       "    'num_grad_updates_lifetime': 15.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 14.5}},\n",
       "  'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 4000,\n",
       "  'num_agent_steps_sampled': 3890,\n",
       "  'num_agent_steps_trained': 3890},\n",
       " 'sampler_results': {'episode_reward_max': nan,\n",
       "  'episode_reward_min': nan,\n",
       "  'episode_reward_mean': nan,\n",
       "  'episode_len_mean': nan,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 0,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [], 'episode_lengths': []},\n",
       "  'sampler_perf': {},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {}},\n",
       " 'episode_reward_max': nan,\n",
       " 'episode_reward_min': nan,\n",
       " 'episode_reward_mean': nan,\n",
       " 'episode_len_mean': nan,\n",
       " 'episodes_this_iter': 0,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'hist_stats': {'episode_reward': [], 'episode_lengths': []},\n",
       " 'sampler_perf': {},\n",
       " 'num_faulty_episodes': 0,\n",
       " 'connector_metrics': {},\n",
       " 'num_healthy_workers': 10,\n",
       " 'num_in_flight_async_reqs': 0,\n",
       " 'num_remote_worker_restarts': 0,\n",
       " 'num_agent_steps_sampled': 3890,\n",
       " 'num_agent_steps_trained': 3890,\n",
       " 'num_env_steps_sampled': 4000,\n",
       " 'num_env_steps_trained': 4000,\n",
       " 'num_env_steps_sampled_this_iter': 4000,\n",
       " 'num_env_steps_trained_this_iter': 4000,\n",
       " 'num_env_steps_sampled_throughput_per_sec': 254.29721057422003,\n",
       " 'num_env_steps_trained_throughput_per_sec': 254.29721057422003,\n",
       " 'timesteps_total': 4000,\n",
       " 'num_steps_trained_this_iter': 4000,\n",
       " 'agent_timesteps_total': 3890,\n",
       " 'timers': {'training_iteration_time_ms': 15729.48,\n",
       "  'sample_time_ms': 983.236,\n",
       "  'learn_time_ms': 14685.39,\n",
       "  'learn_throughput': 272.38,\n",
       "  'synch_weights_time_ms': 56.242},\n",
       " 'counters': {'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 4000,\n",
       "  'num_agent_steps_sampled': 3890,\n",
       "  'num_agent_steps_trained': 3890},\n",
       " 'done': False,\n",
       " 'episodes_total': 0,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2023-09-29_13-18-35',\n",
       " 'timestamp': 1695964715,\n",
       " 'time_this_iter_s': 15.739232540130615,\n",
       " 'time_total_s': 15.739232540130615,\n",
       " 'pid': 30935,\n",
       " 'hostname': 'lwh-Super-Server',\n",
       " 'node_ip': '192.168.3.6',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 1,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_learner_workers': 0,\n",
       "  'num_gpus_per_learner_worker': 0,\n",
       "  'num_cpus_per_learner_worker': 1,\n",
       "  'local_gpu_idx': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'env': 'VJS',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'disable_env_checking': True,\n",
       "  'is_atari': False,\n",
       "  'auto_wrap_old_gym_envs': True,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'sample_async': False,\n",
       "  'enable_connectors': True,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'validate_workers_after_construction': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'compress_observations': False,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'gamma': 0.9,\n",
       "  'lr': 0.01,\n",
       "  'lr_schedule': None,\n",
       "  'grad_clip': None,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  'train_batch_size': 4000,\n",
       "  'model': {'_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1},\n",
       "  'optimizer': {},\n",
       "  'max_requests_in_flight_per_sampler_worker': 2,\n",
       "  '_learner_class': None,\n",
       "  '_enable_learner_api': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'policy_states_are_swappable': False,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 180.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_config': None,\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_workers': 0,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'enable_async_evaluation': False,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  'worker_cls': None,\n",
       "  'ignore_worker_failures': False,\n",
       "  'recreate_failed_workers': False,\n",
       "  'max_num_worker_restarts': 1000,\n",
       "  'delay_between_worker_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_worker_failures_tolerance': 100,\n",
       "  'worker_health_probe_timeout_s': 60,\n",
       "  'worker_restore_timeout_s': 1800,\n",
       "  'rl_module_spec': None,\n",
       "  '_enable_rl_module_api': False,\n",
       "  '_AlgorithmConfig__prior_exploration_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': True,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  'simple_optimizer': True,\n",
       "  'replay_sequence_length': None,\n",
       "  'horizon': -1,\n",
       "  'soft_horizon': -1,\n",
       "  'no_done_at_end': -1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'kl_coeff': 0.2,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'num_sgd_iter': 30,\n",
       "  'shuffle_sequences': True,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'kl_target': 0.01,\n",
       "  'vf_share_layers': -1,\n",
       "  'lambda': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'multiagent': {'policies': {'Machine_0': (None,\n",
       "     Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00], (73,), float64),\n",
       "     Box(0.5, 2.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_1': (None,\n",
       "     Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00], (73,), float64),\n",
       "     Box(0.5, 2.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_2': (None,\n",
       "     Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00], (73,), float64),\n",
       "     Box(0.5, 2.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_3': (None,\n",
       "     Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00], (73,), float64),\n",
       "     Box(0.5, 2.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_4': (None,\n",
       "     Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00], (73,), float64),\n",
       "     Box(0.5, 2.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_5': (None,\n",
       "     Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00], (73,), float64),\n",
       "     Box(0.5, 2.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_6': (None,\n",
       "     Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00], (73,), float64),\n",
       "     Box(0.5, 2.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_7': (None,\n",
       "     Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00], (73,), float64),\n",
       "     Box(0.5, 2.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_8': (None,\n",
       "     Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00], (73,), float64),\n",
       "     Box(0.5, 2.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_9': (None,\n",
       "     Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00], (73,), float64),\n",
       "     Box(0.5, 2.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_10': (None,\n",
       "     Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00], (73,), float64),\n",
       "     Box(0.5, 2.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_11': (None,\n",
       "     Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
       "      4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
       "      1.000e+00 1.000e+00 1.000e+00], (73,), float64),\n",
       "     Box(0.5, 2.0, (1,), float32),\n",
       "     {})},\n",
       "   'policy_mapping_fn': <function __main__.<lambda>(agent_id, episode, **kwargs)>,\n",
       "   'policies_to_train': None,\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': -1,\n",
       "   'count_steps_by': 'env_steps',\n",
       "   'observation_fn': None},\n",
       "  'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch',\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'num_workers': 10},\n",
       " 'time_since_restore': 15.739232540130615,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': 15.416666666666664,\n",
       "  'ram_util_percent': 20.600000000000005,\n",
       "  'gpu_util_percent0': 0.03333333333333334,\n",
       "  'vram_util_percent0': 0.039984809027777776,\n",
       "  'gpu_util_percent1': 0.6811111111111111,\n",
       "  'vram_util_percent1': 0.9081217447916666}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 13:22:25,157\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/a3c.py` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "2023-09-29 13:22:25,160\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/a3c.py` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "2023-09-29 13:22:25,363\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/maml/maml.py` has been deprecated. Use `rllib_contrib/maml/` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-09-29 13:31:24</td></tr>\n",
       "<tr><td>Running for: </td><td>00:08:58.82        </td></tr>\n",
       "<tr><td>Memory:      </td><td>120.2/251.8 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 11.0/56 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                    </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_VJS_2cb97_00000</td><td style=\"text-align: right;\">           1</td><td>/home/yuan/ray_results/PPO/PPO_VJS_2cb97_00000_0_2023-09-29_13-22-25/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_VJS_2cb97_00000</td><td>ERROR   </td><td>192.168.3.6:104298</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         502.159</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=104298)\u001b[0m 2023-09-29 13:22:32,438\tWARNING algorithm_config.py:643 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(PPO pid=104298)\u001b[0m 2023-09-29 13:22:32,457\tINFO algorithm.py:536 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(PPO pid=104298)\u001b[0m 2023-09-29 13:22:42,542\tINFO trainable.py:173 -- Trainable.setup took 10.087 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=104298)\u001b[0m 2023-09-29 13:22:43,722\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(PPO pid=104298)\u001b[0m 2023-09-29 13:22:56,131\tWARNING ppo.py:520 -- The mean reward returned from the environment is 840.9097900390625 but the vf_clip_param is set to 10.0. Consider increasing it for policy: Machine_8 to improve value function convergence.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics  </th><th>counters                                                                                                                                </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname        </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip    </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                                                                                                                                                                        </th><th style=\"text-align: right;\">   pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf  </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                         </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_VJS_2cb97_00000</td><td style=\"text-align: right;\">                 111890</td><td>{}                 </td><td>{&#x27;num_env_steps_sampled&#x27;: 112000, &#x27;num_env_steps_trained&#x27;: 112000, &#x27;num_agent_steps_sampled&#x27;: 111890, &#x27;num_agent_steps_trained&#x27;: 111890}</td><td>{}              </td><td>2023-09-29_13-31-09</td><td>False </td><td style=\"text-align: right;\">               nan</td><td>{}             </td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                  nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               0</td><td style=\"text-align: right;\">               0</td><td>lwh-Super-Server</td><td>{&#x27;learner&#x27;: {&#x27;Machine_8&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 1.437719802930951, &#x27;cur_kl_coeff&#x27;: 0.022525405883789068, &#x27;cur_lr&#x27;: 0.009999999999999998, &#x27;total_loss&#x27;: 7.127647076712714, &#x27;policy_loss&#x27;: -0.0009228790576647346, &#x27;vf_loss&#x27;: 7.128446435928344, &#x27;vf_explained_var&#x27;: -1.0596381293402778e-08, &#x27;kl&#x27;: 0.005484780956046759, &#x27;entropy&#x27;: 5.0277416706085205, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 127.66666666666667, &#x27;num_grad_updates_lifetime&#x27;: 2835.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 44.5}, &#x27;Machine_5&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 2.200789535989364, &#x27;cur_kl_coeff&#x27;: 0.1601806640625, &#x27;cur_lr&#x27;: 0.01, &#x27;total_loss&#x27;: 6.840622762044271, &#x27;policy_loss&#x27;: 0.002759126254046957, &#x27;vf_loss&#x27;: 6.835780528386434, &#x27;vf_explained_var&#x27;: 1.5894571940104168e-09, &#x27;kl&#x27;: 0.01300372443763384, &#x27;entropy&#x27;: 4.71923137029012, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 115.6, &#x27;num_grad_updates_lifetime&#x27;: 4125.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 74.5}, &#x27;Machine_0&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 1.9640027046203614, &#x27;cur_kl_coeff&#x27;: 0.24027099609375008, &#x27;cur_lr&#x27;: 0.010000000000000002, &#x27;total_loss&#x27;: 6.858553171157837, &#x27;policy_loss&#x27;: -0.033854015668233234, &#x27;vf_loss&#x27;: 6.880733998616536, &#x27;vf_explained_var&#x27;: 7.947285970052083e-09, &#x27;kl&#x27;: 0.04858252743142657, &#x27;entropy&#x27;: 2.003364050388336, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 109.0, &#x27;num_grad_updates_lifetime&#x27;: 855.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 14.5}, &#x27;Machine_2&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 3.106200289250248, &#x27;cur_kl_coeff&#x27;: 0.7208129882812502, &#x27;cur_lr&#x27;: 0.009999999999999998, &#x27;total_loss&#x27;: 7.626851389143202, &#x27;policy_loss&#x27;: 0.020247099031176833, &#x27;vf_loss&#x27;: 7.598937442567613, &#x27;vf_explained_var&#x27;: -7.947285970052083e-09, &#x27;kl&#x27;: 0.010636384691881377, &#x27;entropy&#x27;: 7.882420931922065, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 98.33333333333333, &#x27;num_grad_updates_lifetime&#x27;: 2475.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 44.5}, &#x27;Machine_7&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 43.27962287068367, &#x27;cur_kl_coeff&#x27;: 1.9221679687499995, &#x27;cur_lr&#x27;: 0.01, &#x27;total_loss&#x27;: 20.20456072092056, &#x27;policy_loss&#x27;: 0.1924962409219006, &#x27;vf_loss&#x27;: 7.631917742888133, &#x27;vf_explained_var&#x27;: 4.470348358154297e-09, &#x27;kl&#x27;: 6.440720418191146, &#x27;entropy&#x27;: 10.13101287484169, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 115.75, &#x27;num_grad_updates_lifetime&#x27;: 3300.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 59.5}, &#x27;Machine_10&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 1.1547515739376346, &#x27;cur_kl_coeff&#x27;: 0.006335270404815674, &#x27;cur_lr&#x27;: 0.010000000000000002, &#x27;total_loss&#x27;: 7.321459015210469, &#x27;policy_loss&#x27;: -0.004434324289225818, &#x27;vf_loss&#x27;: 7.325842865308126, &#x27;vf_explained_var&#x27;: -1.5894571940104166e-08, &#x27;kl&#x27;: 0.007962874746954184, &#x27;entropy&#x27;: 4.300526905059814, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 108.5, &#x27;num_grad_updates_lifetime&#x27;: 1650.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 29.5}, &#x27;Machine_9&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 2.9045507642957897, &#x27;cur_kl_coeff&#x27;: 0.8109146118164059, &#x27;cur_lr&#x27;: 0.009999999999999998, &#x27;total_loss&#x27;: 6.816878032684326, &#x27;policy_loss&#x27;: 0.00841148319757647, &#x27;vf_loss&#x27;: 6.800533543692695, &#x27;vf_explained_var&#x27;: -2.6490953233506945e-09, &#x27;kl&#x27;: 0.009782879024795774, &#x27;entropy&#x27;: 8.201890680525038, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 95.33333333333333, &#x27;num_grad_updates_lifetime&#x27;: 2475.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 44.5}, &#x27;Machine_1&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 6.931161832809448, &#x27;cur_kl_coeff&#x27;: 1.92216796875, &#x27;cur_lr&#x27;: 0.010000000000000002, &#x27;total_loss&#x27;: 9.134987330436706, &#x27;policy_loss&#x27;: 0.02812302972112472, &#x27;vf_loss&#x27;: 7.5827396790186565, &#x27;vf_explained_var&#x27;: 1.4901161193847656e-08, &#x27;kl&#x27;: 0.7929196012516816, &#x27;entropy&#x27;: 18.996732393900555, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 100.5, &#x27;num_grad_updates_lifetime&#x27;: 1650.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 29.5}, &#x27;Machine_3&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 14.987502020266321, &#x27;cur_kl_coeff&#x27;: 2.4327438354492195, &#x27;cur_lr&#x27;: 0.009999999999999998, &#x27;total_loss&#x27;: 7.348897716734144, &#x27;policy_loss&#x27;: 0.013817086805486017, &#x27;vf_loss&#x27;: 7.242380163404676, &#x27;vf_explained_var&#x27;: 1.2583202785915799e-08, &#x27;kl&#x27;: 0.03810527516632444, &#x27;entropy&#x27;: 12.072184255388049, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 117.0, &#x27;num_grad_updates_lifetime&#x27;: 2745.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 44.5}, &#x27;Machine_6&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 1.8429552510008216, &#x27;cur_kl_coeff&#x27;: 0.03003387451171874, &#x27;cur_lr&#x27;: 0.01, &#x27;total_loss&#x27;: 7.675135329564412, &#x27;policy_loss&#x27;: -0.0013099847547709941, &#x27;vf_loss&#x27;: 7.6763523515065515, &#x27;vf_explained_var&#x27;: -9.139378865559895e-09, &#x27;kl&#x27;: 0.0030958803647505796, &#x27;entropy&#x27;: 22.21707633972168, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 111.4, &#x27;num_grad_updates_lifetime&#x27;: 4125.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 74.5}, &#x27;Machine_4&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 2.4288134084393582, &#x27;cur_kl_coeff&#x27;: 4.827976226806639e-07, &#x27;cur_lr&#x27;: 0.01, &#x27;total_loss&#x27;: 7.1188591837883, &#x27;policy_loss&#x27;: -0.0015593464447495838, &#x27;vf_loss&#x27;: 7.120418508847554, &#x27;vf_explained_var&#x27;: -4.470348358154297e-09, &#x27;kl&#x27;: 0.0019163018463586924, &#x27;entropy&#x27;: 38.889642016092935, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 113.5, &#x27;num_grad_updates_lifetime&#x27;: 3300.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 59.5}, &#x27;Machine_11&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 1.978119137386481, &#x27;cur_kl_coeff&#x27;: 0.6407226562499998, &#x27;cur_lr&#x27;: 0.010000000000000002, &#x27;total_loss&#x27;: 7.542456865310669, &#x27;policy_loss&#x27;: -0.013865548647784939, &#x27;vf_loss&#x27;: 7.5471696853637695, &#x27;vf_explained_var&#x27;: 0.0, &#x27;kl&#x27;: 0.014284609446379666, &#x27;entropy&#x27;: 1.8834317723910015, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 106.0, &#x27;num_grad_updates_lifetime&#x27;: 855.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 14.5}}, &#x27;num_env_steps_sampled&#x27;: 112000, &#x27;num_env_steps_trained&#x27;: 112000, &#x27;num_agent_steps_sampled&#x27;: 111890, &#x27;num_agent_steps_trained&#x27;: 111890}</td><td style=\"text-align: right;\">                        28</td><td>192.168.3.6</td><td style=\"text-align: right;\">                   111890</td><td style=\"text-align: right;\">                   111890</td><td style=\"text-align: right;\">                 112000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                                    186.99</td><td style=\"text-align: right;\">                 112000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                                    186.99</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                   10</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 46.55200000000001, &#x27;ram_util_percent&#x27;: 47.92400000000001, &#x27;gpu_util_percent0&#x27;: 0.7848, &#x27;vram_util_percent0&#x27;: 0.8077799479166667, &#x27;gpu_util_percent1&#x27;: 0.6128, &#x27;vram_util_percent1&#x27;: 0.8033854166666667}</td><td style=\"text-align: right;\">104298</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{}            </td><td>{&#x27;episode_reward_max&#x27;: nan, &#x27;episode_reward_min&#x27;: nan, &#x27;episode_reward_mean&#x27;: nan, &#x27;episode_len_mean&#x27;: nan, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 0, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [], &#x27;episode_lengths&#x27;: []}, &#x27;sampler_perf&#x27;: {}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {}}</td><td style=\"text-align: right;\">             502.159</td><td style=\"text-align: right;\">           21.4095</td><td style=\"text-align: right;\">       502.159</td><td>{&#x27;training_iteration_time_ms&#x27;: 20071.489, &#x27;sample_time_ms&#x27;: 1223.229, &#x27;learn_time_ms&#x27;: 18765.439, &#x27;learn_throughput&#x27;: 213.158, &#x27;synch_weights_time_ms&#x27;: 79.647}</td><td style=\"text-align: right;\"> 1695965469</td><td style=\"text-align: right;\">           112000</td><td style=\"text-align: right;\">                  28</td><td>2cb97_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 13:22:56,851\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 0.622 s, which may be a performance bottleneck.\n",
      "2023-09-29 13:22:56,854\tWARNING util.py:315 -- The `process_trial_result` operation took 0.628 s, which may be a performance bottleneck.\n",
      "2023-09-29 13:22:56,855\tWARNING util.py:315 -- Processing trial results took 0.629 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-09-29 13:22:56,856\tWARNING util.py:315 -- The `process_trial_result` operation took 0.630 s, which may be a performance bottleneck.\n",
      "2023-09-29 13:24:14,010\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.065 s, which may be a performance bottleneck.\n",
      "2023-09-29 13:24:14,018\tWARNING util.py:315 -- The `process_trial_result` operation took 1.075 s, which may be a performance bottleneck.\n",
      "2023-09-29 13:24:14,019\tWARNING util.py:315 -- Processing trial results took 1.076 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-09-29 13:24:14,020\tWARNING util.py:315 -- The `process_trial_result` operation took 1.078 s, which may be a performance bottleneck.\n",
      "2023-09-29 13:31:24,141\tERROR tune_controller.py:873 -- Trial task failed for trial PPO_VJS_2cb97_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/_private/worker.py\", line 2540, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::PPO.train()\u001b[39m (pid=104298, ip=192.168.3.6, actor_id=65ee2a0d74f37f8f65b7b5c601000000, repr=PPO)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py\", line 85, in loss\n",
      "    curr_action_dist = dist_class(logits, model)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/models/torch/torch_action_dist.py\", line 250, in __init__\n",
      "    self.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/normal.py\", line 56, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 62, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter loc (Tensor of shape (128, 1)) of distribution Normal(loc: torch.Size([128, 1]), scale: torch.Size([128, 1])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::PPO.train()\u001b[39m (pid=104298, ip=192.168.3.6, actor_id=65ee2a0d74f37f8f65b7b5c601000000, repr=PPO)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 386, in train\n",
      "    result = self.step()\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 803, in step\n",
      "    results, train_iter_ctx = self._run_one_training_iteration()\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py\", line 2853, in _run_one_training_iteration\n",
      "    results = self.training_step()\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo.py\", line 430, in training_step\n",
      "    train_results = train_one_step(self, train_batch)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/execution/train_ops.py\", line 52, in train_one_step\n",
      "    info = do_minibatch_sgd(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/sgd.py\", line 129, in do_minibatch_sgd\n",
      "    local_worker.learn_on_batch(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1029, in learn_on_batch\n",
      "    info_out[pid] = policy.learn_on_batch(batch)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n",
      "    return func(self, *a, **k)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 669, in learn_on_batch\n",
      "    grads, fetches = self.compute_gradients(postprocessed_batch)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n",
      "    return func(self, *a, **k)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 873, in compute_gradients\n",
      "    tower_outputs = self._multi_gpu_parallel_grad_calc([postprocessed_batch])\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1320, in _multi_gpu_parallel_grad_calc\n",
      "    raise last_result[0] from last_result[1]\n",
      "ValueError: Expected parameter loc (Tensor of shape (128, 1)) of distribution Normal(loc: torch.Size([128, 1]), scale: torch.Size([128, 1])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)\n",
      " tracebackTraceback (most recent call last):\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1235, in _worker\n",
      "    self.loss(model, self.dist_class, sample_batch)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py\", line 85, in loss\n",
      "    curr_action_dist = dist_class(logits, model)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/models/torch/torch_action_dist.py\", line 250, in __init__\n",
      "    self.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/normal.py\", line 56, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 62, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter loc (Tensor of shape (128, 1)) of distribution Normal(loc: torch.Size([128, 1]), scale: torch.Size([128, 1])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)\n",
      "\n",
      "In tower 0 on device cuda:0\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_VJS_2cb97_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yuan/ResMan/RLlib/rllib_ppo.ipynb 单元格 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_ppo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m alg_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_ppo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m tune\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_ppo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     alg_name,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_ppo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPPO\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_ppo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     stop\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mepisodes_total\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m10000\u001b[39;49m},\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_ppo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     checkpoint_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_ppo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     config\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mto_dict(),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_ppo.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/tune/tune.py:1105\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, checkpoint_keep_all_ranks, checkpoint_upload_from_workers, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, trial_executor, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[39mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m   1104\u001b[0m     \u001b[39mif\u001b[39;00m raise_on_failed_trial \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m experiment_interrupted_event\u001b[39m.\u001b[39mis_set():\n\u001b[0;32m-> 1105\u001b[0m         \u001b[39mraise\u001b[39;00m TuneError(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete\u001b[39m\u001b[39m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m   1106\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m         logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_VJS_2cb97_00000])"
     ]
    }
   ],
   "source": [
    "alg_name = \"PPO\"\n",
    "tune.run(\n",
    "    alg_name,\n",
    "    name=\"PPO\",\n",
    "    stop={\"episodes_total\": 10000},\n",
    "    checkpoint_freq=10,\n",
    "    config=config.to_dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:19:28,025 E 3473473 3473473] (raylet) node_manager.cc:3069: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 9648b9014e4abaede66aee64b1344246a1b2b8bbe34248617d11f84d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (46, 1)) of distribution Normal(loc: torch.Size([46, 1]), scale: torch.Size([46, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)\n tracebackTraceback (most recent call last):\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1235, in _worker\n    self.loss(model, self.dist_class, sample_batch)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py\", line 85, in loss\n    curr_action_dist = dist_class(logits, model)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/models/torch/torch_action_dist.py\", line 250, in __init__\n    self.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/normal.py\", line 56, in __init__\n    super().__init__(batch_shape, validate_args=validate_args)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 62, in __init__\n    raise ValueError(\nValueError: Expected parameter loc (Tensor of shape (46, 1)) of distribution Normal(loc: torch.Size([46, 1]), scale: torch.Size([46, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)\n\nIn tower 0 on device cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py:1235\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc.<locals>._worker\u001b[0;34m(shard_idx, model, sample_batch, device)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[39mwith\u001b[39;00m NullContextManager() \u001b[39mif\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     device\n\u001b[1;32m   1233\u001b[0m ):\n\u001b[1;32m   1234\u001b[0m     loss_out \u001b[39m=\u001b[39m force_list(\n\u001b[0;32m-> 1235\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdist_class, sample_batch)\n\u001b[1;32m   1236\u001b[0m     )\n\u001b[1;32m   1238\u001b[0m     \u001b[39m# Call Model's custom-loss with Policy loss outputs and\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m     \u001b[39m# train_batch.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py:85\u001b[0m, in \u001b[0;36mPPOTorchPolicy.loss\u001b[0;34m(self, model, dist_class, train_batch)\u001b[0m\n\u001b[1;32m     84\u001b[0m logits, state \u001b[39m=\u001b[39m model(train_batch)\n\u001b[0;32m---> 85\u001b[0m curr_action_dist \u001b[39m=\u001b[39m dist_class(logits, model)\n\u001b[1;32m     87\u001b[0m \u001b[39m# RNN case: Mask away 0-padded chunks at end of time axis.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/models/torch/torch_action_dist.py:250\u001b[0m, in \u001b[0;36mTorchDiagGaussian.__init__\u001b[0;34m(self, inputs, model, action_space)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_std \u001b[39m=\u001b[39m log_std\n\u001b[0;32m--> 250\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdistributions\u001b[39m.\u001b[39;49mnormal\u001b[39m.\u001b[39;49mNormal(mean, torch\u001b[39m.\u001b[39;49mexp(log_std))\n\u001b[1;32m    251\u001b[0m \u001b[39m# Remember to squeeze action samples in case action space is Box(shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/torch/distributions/normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     55\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 56\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/torch/distributions/distribution.py:62\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 62\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m             )\n\u001b[1;32m     69\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (46, 1)) of distribution Normal(loc: torch.Size([46, 1]), scale: torch.Size([46, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/yuan/ResMan/RLlib/rllib_mappo.ipynb 单元格 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_mappo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_mappo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_mappo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         info \u001b[39m=\u001b[39m algo\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_mappo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         info[\u001b[39m'\u001b[39m\u001b[39minfo\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlearner\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mMachine_6\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlearner_stats\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtotal_loss\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:389\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mraise\u001b[39;00m skipped \u001b[39mfrom\u001b[39;00m \u001b[39mexception_cause\u001b[39;00m(skipped)\n\u001b[1;32m    391\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mstep() needs to return a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m \u001b[39m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:386\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:803\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m     (\n\u001b[1;32m    796\u001b[0m         results,\n\u001b[1;32m    797\u001b[0m         train_iter_ctx,\n\u001b[1;32m    798\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    799\u001b[0m \u001b[39m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[39m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     results, train_iter_ctx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_training_iteration()\n\u001b[1;32m    805\u001b[0m \u001b[39m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[39mif\u001b[39;00m evaluate_this_iter \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:2853\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2852\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[0;32m-> 2853\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step()\n\u001b[1;32m   2854\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2855\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo.py:430\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m     train_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearner_group\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    424\u001b[0m         train_batch,\n\u001b[1;32m    425\u001b[0m         minibatch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39msgd_minibatch_size,\n\u001b[1;32m    426\u001b[0m         num_iters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_sgd_iter,\n\u001b[1;32m    427\u001b[0m     )\n\u001b[1;32m    429\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39msimple_optimizer:\n\u001b[0;32m--> 430\u001b[0m     train_results \u001b[39m=\u001b[39m train_one_step(\u001b[39mself\u001b[39;49m, train_batch)\n\u001b[1;32m    431\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     train_results \u001b[39m=\u001b[39m multi_gpu_train_one_step(\u001b[39mself\u001b[39m, train_batch)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/execution/train_ops.py:52\u001b[0m, in \u001b[0;36mtrain_one_step\u001b[0;34m(algorithm, train_batch, policies_to_train)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mwith\u001b[39;00m learn_timer:\n\u001b[1;32m     49\u001b[0m     \u001b[39m# Subsample minibatches (size=`sgd_minibatch_size`) from the\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[39m# train batch and loop through train batch `num_sgd_iter` times.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[39mif\u001b[39;00m num_sgd_iter \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m sgd_minibatch_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m         info \u001b[39m=\u001b[39m do_minibatch_sgd(\n\u001b[1;32m     53\u001b[0m             train_batch,\n\u001b[1;32m     54\u001b[0m             {\n\u001b[1;32m     55\u001b[0m                 pid: local_worker\u001b[39m.\u001b[39;49mget_policy(pid)\n\u001b[1;32m     56\u001b[0m                 \u001b[39mfor\u001b[39;49;00m pid \u001b[39min\u001b[39;49;00m policies_to_train\n\u001b[1;32m     57\u001b[0m                 \u001b[39mor\u001b[39;49;00m local_worker\u001b[39m.\u001b[39;49mget_policies_to_train(train_batch)\n\u001b[1;32m     58\u001b[0m             },\n\u001b[1;32m     59\u001b[0m             local_worker,\n\u001b[1;32m     60\u001b[0m             num_sgd_iter,\n\u001b[1;32m     61\u001b[0m             sgd_minibatch_size,\n\u001b[1;32m     62\u001b[0m             [],\n\u001b[1;32m     63\u001b[0m         )\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Single update step using train batch.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m         info \u001b[39m=\u001b[39m local_worker\u001b[39m.\u001b[39mlearn_on_batch(train_batch)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/sgd.py:129\u001b[0m, in \u001b[0;36mdo_minibatch_sgd\u001b[0;34m(samples, policies, local_worker, num_sgd_iter, sgd_minibatch_size, standardize_fields)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_sgd_iter):\n\u001b[1;32m    127\u001b[0m         \u001b[39mfor\u001b[39;00m minibatch \u001b[39min\u001b[39;00m minibatches(batch, sgd_minibatch_size):\n\u001b[1;32m    128\u001b[0m             results \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 129\u001b[0m                 local_worker\u001b[39m.\u001b[39;49mlearn_on_batch(\n\u001b[1;32m    130\u001b[0m                     MultiAgentBatch({policy_id: minibatch}, minibatch\u001b[39m.\u001b[39;49mcount)\n\u001b[1;32m    131\u001b[0m                 )\n\u001b[1;32m    132\u001b[0m             )[policy_id]\n\u001b[1;32m    133\u001b[0m             learner_info_builder\u001b[39m.\u001b[39madd_learn_on_batch_results(results, policy_id)\n\u001b[1;32m    135\u001b[0m learner_info \u001b[39m=\u001b[39m learner_info_builder\u001b[39m.\u001b[39mfinalize()\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:1029\u001b[0m, in \u001b[0;36mRolloutWorker.learn_on_batch\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             to_fetch[pid] \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39m_build_learn_on_batch(builders[pid], batch)\n\u001b[1;32m   1028\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1029\u001b[0m             info_out[pid] \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mlearn_on_batch(batch)\n\u001b[1;32m   1031\u001b[0m     info_out\u001b[39m.\u001b[39mupdate({pid: builders[pid]\u001b[39m.\u001b[39mget(v) \u001b[39mfor\u001b[39;00m pid, v \u001b[39min\u001b[39;00m to_fetch\u001b[39m.\u001b[39mitems()})\n\u001b[1;32m   1032\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n\u001b[1;32m     25\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_lock\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py:669\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_batch\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mon_learn_on_batch(\n\u001b[1;32m    664\u001b[0m     policy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, train_batch\u001b[39m=\u001b[39mpostprocessed_batch, result\u001b[39m=\u001b[39mlearn_stats\n\u001b[1;32m    665\u001b[0m )\n\u001b[1;32m    667\u001b[0m \u001b[39m# Compute gradients (will calculate all losses and `backward()`\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[39m# them to get the grads).\u001b[39;00m\n\u001b[0;32m--> 669\u001b[0m grads, fetches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_gradients(postprocessed_batch)\n\u001b[1;32m    671\u001b[0m \u001b[39m# Step the optimizers.\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(_directStepOptimizerSingleton)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mk)\n\u001b[1;32m     25\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_lock\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py:873\u001b[0m, in \u001b[0;36mTorchPolicyV2.compute_gradients\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lazy_tensor_dict(postprocessed_batch, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevices[\u001b[39m0\u001b[39m])\n\u001b[1;32m    872\u001b[0m \u001b[39m# Do the (maybe parallelized) gradient calculation step.\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m tower_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_multi_gpu_parallel_grad_calc([postprocessed_batch])\n\u001b[1;32m    875\u001b[0m all_grads, grad_info \u001b[39m=\u001b[39m tower_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    877\u001b[0m grad_info[\u001b[39m\"\u001b[39m\u001b[39mallreduce_latency\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizers)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py:1320\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc\u001b[0;34m(self, sample_batches)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         last_result \u001b[39m=\u001b[39m results[\u001b[39mlen\u001b[39m(results) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m   1319\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(last_result[\u001b[39m0\u001b[39m], \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m-> 1320\u001b[0m             \u001b[39mraise\u001b[39;00m last_result[\u001b[39m0\u001b[39m] \u001b[39mfrom\u001b[39;00m \u001b[39mlast_result\u001b[39;00m[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1321\u001b[0m \u001b[39m# Multi device (GPU) case: Parallelize via threads.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     threads \u001b[39m=\u001b[39m [\n\u001b[1;32m   1324\u001b[0m         threading\u001b[39m.\u001b[39mThread(\n\u001b[1;32m   1325\u001b[0m             target\u001b[39m=\u001b[39m_worker, args\u001b[39m=\u001b[39m(shard_idx, model, sample_batch, device)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         )\n\u001b[1;32m   1330\u001b[0m     ]\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (46, 1)) of distribution Normal(loc: torch.Size([46, 1]), scale: torch.Size([46, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)\n tracebackTraceback (most recent call last):\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1235, in _worker\n    self.loss(model, self.dist_class, sample_batch)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py\", line 85, in loss\n    curr_action_dist = dist_class(logits, model)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/models/torch/torch_action_dist.py\", line 250, in __init__\n    self.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/normal.py\", line 56, in __init__\n    super().__init__(batch_shape, validate_args=validate_args)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 62, in __init__\n    raise ValueError(\nValueError: Expected parameter loc (Tensor of shape (46, 1)) of distribution Normal(loc: torch.Size([46, 1]), scale: torch.Size([46, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], device='cuda:0', grad_fn=<SplitBackward0>)\n\nIn tower 0 on device cuda:0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:21:28,027 E 3473473 3473473] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 9648b9014e4abaede66aee64b1344246a1b2b8bbe34248617d11f84d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:22:28,028 E 3473473 3473473] (raylet) node_manager.cc:3069: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 9648b9014e4abaede66aee64b1344246a1b2b8bbe34248617d11f84d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:25:28,565 E 3473473 3473473] (raylet) node_manager.cc:3069: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 9648b9014e4abaede66aee64b1344246a1b2b8bbe34248617d11f84d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:26:28,567 E 3473473 3473473] (raylet) node_manager.cc:3069: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 9648b9014e4abaede66aee64b1344246a1b2b8bbe34248617d11f84d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    for i in range(10):\n",
    "        info = algo.train()\n",
    "        info['info']['learner']['Machine_6']['learner_stats']['total_loss']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
