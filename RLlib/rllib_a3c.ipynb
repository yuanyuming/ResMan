{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 11:21:11,945\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-09-29 11:21:13,229\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/a3c.py` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "2023-09-29 11:21:13,231\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/a3c/a3c.py` has been deprecated. Use `rllib_contrib/a3c/` instead. This will raise an error in the future!\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:442: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-09-29 11:21:13,256\tINFO algorithm.py:536 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.a3c import A3CConfig\n",
    "\n",
    "import ray\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune import register_env\n",
    "\n",
    "import rllib_setup\n",
    "\n",
    "ray.init()\n",
    "env_name = \"VJS\"\n",
    "register_env(\n",
    "    env_name,\n",
    "    lambda config: rllib_setup.get_env_continuous(),\n",
    ")\n",
    "test_env = rllib_setup.get_env_continuous()\n",
    "\n",
    "\n",
    "def policies(agent_ids):\n",
    "    obs_space = test_env.observation_space\n",
    "    act_space = test_env.action_space\n",
    "    return {\n",
    "        str(i): (\n",
    "            None,\n",
    "            obs_space,\n",
    "            act_space,\n",
    "            {}\n",
    "            # config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }\n",
    "\n",
    "\n",
    "config = (\n",
    "    A3CConfig().rollouts(num_rollout_workers=10, rollout_fragment_length=30,).training( \n",
    "    lr=0.001)\n",
    "    .multi_agent(\n",
    "        policies=policies(test_env._agent_ids),\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: str(agent_id),\n",
    "    )\n",
    "    .environment(disable_env_checking=True)\n",
    ")\n",
    "# Build a Algorithm object from the config and run one training iteration.\n",
    "config.sample_async\n",
    "algo = config.build(env=env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 11:23:57,867\tERROR actor_manager.py:507 -- Ray error, taking actor 1 out of service. \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3747882, ip=192.168.3.6, actor_id=3d7e3a65a8862a1bbf4c046e01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f057a395910>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py\", line 222, in sample_and_compute_grads\n",
      "    samples = worker.sample()\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 915, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 495, in get_data\n",
      "    raise RuntimeError(\"Sampling thread has died\")\n",
      "RuntimeError: Sampling thread has died\n"
     ]
    },
    {
     "ename": "RayTaskError(RuntimeError)",
     "evalue": "\u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3747882, ip=192.168.3.6, actor_id=3d7e3a65a8862a1bbf4c046e01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f057a395910>)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n    raise e\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n    return func(self, *args, **kwargs)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py\", line 222, in sample_and_compute_grads\n    samples = worker.sample()\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 915, in sample\n    batches = [self.input_reader.next()]\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n    batches = [self.get_data()]\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 495, in get_data\n    raise RuntimeError(\"Sampling thread has died\")\nRuntimeError: Sampling thread has died",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(RuntimeError)\u001b[0m                Traceback (most recent call last)",
      "\u001b[1;32m/home/yuan/ResMan/RLlib/rllib_a3c.ipynb 单元格 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_a3c.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m info \u001b[39m=\u001b[39m algo\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:389\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mraise\u001b[39;00m skipped \u001b[39mfrom\u001b[39;00m \u001b[39mexception_cause\u001b[39;00m(skipped)\n\u001b[1;32m    391\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mstep() needs to return a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m \u001b[39m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:386\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:803\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m     (\n\u001b[1;32m    796\u001b[0m         results,\n\u001b[1;32m    797\u001b[0m         train_iter_ctx,\n\u001b[1;32m    798\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    799\u001b[0m \u001b[39m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[39m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     results, train_iter_ctx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_training_iteration()\n\u001b[1;32m    805\u001b[0m \u001b[39m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[39mif\u001b[39;00m evaluate_this_iter \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:2853\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2852\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[0;32m-> 2853\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step()\n\u001b[1;32m   2854\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2855\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py:239\u001b[0m, in \u001b[0;36mA3C.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[GRAD_WAIT_TIMER]:\n\u001b[1;32m    233\u001b[0m     \u001b[39m# Results are a mapping from ActorHandle (RolloutWorker) to their\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[39m# returned gradient calculation results.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mforeach_worker_async(\n\u001b[1;32m    236\u001b[0m         func\u001b[39m=\u001b[39msample_and_compute_grads,\n\u001b[1;32m    237\u001b[0m         healthy_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    238\u001b[0m     )\n\u001b[0;32m--> 239\u001b[0m     async_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworkers\u001b[39m.\u001b[39;49mfetch_ready_async_reqs()\n\u001b[1;32m    241\u001b[0m \u001b[39m# Loop through all fetched worker-computed gradients (if any)\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39m# and apply them - one by one - to the local worker's model.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m# After each apply step (one step per worker that returned some gradients),\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m# update that particular worker's weights.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m global_vars \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:830\u001b[0m, in \u001b[0;36mWorkerSet.fetch_ready_async_reqs\u001b[0;34m(self, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get esults from outstanding asynchronous requests that are ready.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \n\u001b[1;32m    815\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[39m    paired with the indices of the callee workers.\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    824\u001b[0m remote_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__worker_manager\u001b[39m.\u001b[39mfetch_ready_async_reqs(\n\u001b[1;32m    825\u001b[0m     timeout_seconds\u001b[39m=\u001b[39mtimeout_seconds,\n\u001b[1;32m    826\u001b[0m     return_obj_refs\u001b[39m=\u001b[39mreturn_obj_refs,\n\u001b[1;32m    827\u001b[0m     mark_healthy\u001b[39m=\u001b[39mmark_healthy,\n\u001b[1;32m    828\u001b[0m )\n\u001b[0;32m--> 830\u001b[0m handle_remote_call_result_errors(remote_results, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ignore_worker_failures)\n\u001b[1;32m    832\u001b[0m \u001b[39mreturn\u001b[39;00m [(r\u001b[39m.\u001b[39mactor_id, r\u001b[39m.\u001b[39mget()) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m remote_results\u001b[39m.\u001b[39mignore_errors()]\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:75\u001b[0m, in \u001b[0;36mhandle_remote_call_result_errors\u001b[0;34m(results, ignore_worker_failures)\u001b[0m\n\u001b[1;32m     73\u001b[0m     logger\u001b[39m.\u001b[39mexception(r\u001b[39m.\u001b[39mget())\n\u001b[1;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mraise\u001b[39;00m r\u001b[39m.\u001b[39mget()\n",
      "\u001b[0;31mRayTaskError(RuntimeError)\u001b[0m: \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3747882, ip=192.168.3.6, actor_id=3d7e3a65a8862a1bbf4c046e01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f057a395910>)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n    raise e\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n    return func(self, *args, **kwargs)\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py\", line 222, in sample_and_compute_grads\n    samples = worker.sample()\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 915, in sample\n    batches = [self.input_reader.next()]\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n    batches = [self.get_data()]\n  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 495, in get_data\n    raise RuntimeError(\"Sampling thread has died\")\nRuntimeError: Sampling thread has died"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 11:24:03,334\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3747888, ip=192.168.3.6, actor_id=8c85a7a7b19828066e426d3001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0f621b9940>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py\", line 222, in sample_and_compute_grads\n",
      "    samples = worker.sample()\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 915, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 495, in get_data\n",
      "    raise RuntimeError(\"Sampling thread has died\")\n",
      "RuntimeError: Sampling thread has died\n",
      "2023-09-29 11:24:03,338\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3747887, ip=192.168.3.6, actor_id=4810bb2ab56e50b09ca4b66701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8839936970>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py\", line 222, in sample_and_compute_grads\n",
      "    samples = worker.sample()\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 915, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 495, in get_data\n",
      "    raise RuntimeError(\"Sampling thread has died\")\n",
      "RuntimeError: Sampling thread has died\n",
      "2023-09-29 11:24:03,340\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3747889, ip=192.168.3.6, actor_id=915ce727a3639d8d7376985101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8160bd29a0>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py\", line 222, in sample_and_compute_grads\n",
      "    samples = worker.sample()\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 915, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 495, in get_data\n",
      "    raise RuntimeError(\"Sampling thread has died\")\n",
      "RuntimeError: Sampling thread has died\n",
      "2023-09-29 11:24:03,343\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3784112, ip=192.168.3.6, actor_id=7388ea09ec744495dd1e0d7101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0d03cf78e0>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c_torch_policy.py\", line 76, in loss\n",
      "    dist = dist_class(logits, model)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/models/torch/torch_action_dist.py\", line 250, in __init__\n",
      "    self.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/normal.py\", line 56, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 62, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter loc (Tensor of shape (2, 1)) of distribution Normal(loc: torch.Size([2, 1]), scale: torch.Size([2, 1])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan],\n",
      "        [nan]], grad_fn=<SplitBackward0>)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3784112, ip=192.168.3.6, actor_id=7388ea09ec744495dd1e0d7101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0d03cf78e0>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py\", line 223, in sample_and_compute_grads\n",
      "    grads, infos = worker.compute_gradients(samples)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1162, in compute_gradients\n",
      "    grad_out[pid], info_out[pid] = self.policy_map[pid].compute_gradients(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n",
      "    return func(self, *a, **k)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 873, in compute_gradients\n",
      "    tower_outputs = self._multi_gpu_parallel_grad_calc([postprocessed_batch])\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1320, in _multi_gpu_parallel_grad_calc\n",
      "    raise last_result[0] from last_result[1]\n",
      "ValueError: Expected parameter loc (Tensor of shape (2, 1)) of distribution Normal(loc: torch.Size([2, 1]), scale: torch.Size([2, 1])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan],\n",
      "        [nan]], grad_fn=<SplitBackward0>)\n",
      " tracebackTraceback (most recent call last):\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1235, in _worker\n",
      "    self.loss(model, self.dist_class, sample_batch)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c_torch_policy.py\", line 76, in loss\n",
      "    dist = dist_class(logits, model)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/models/torch/torch_action_dist.py\", line 250, in __init__\n",
      "    self.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/normal.py\", line 56, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 62, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter loc (Tensor of shape (2, 1)) of distribution Normal(loc: torch.Size([2, 1]), scale: torch.Size([2, 1])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan],\n",
      "        [nan]], grad_fn=<SplitBackward0>)\n",
      "\n",
      "In tower 0 on device cpu\n",
      "2023-09-29 11:24:03,345\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3783073, ip=192.168.3.6, actor_id=c2fadccadecb304a2f45cda601000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fdae38be910>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c_torch_policy.py\", line 76, in loss\n",
      "    dist = dist_class(logits, model)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/models/torch/torch_action_dist.py\", line 250, in __init__\n",
      "    self.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/normal.py\", line 56, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 62, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter loc (Tensor of shape (3, 1)) of distribution Normal(loc: torch.Size([3, 1]), scale: torch.Size([3, 1])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], grad_fn=<SplitBackward0>)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3783073, ip=192.168.3.6, actor_id=c2fadccadecb304a2f45cda601000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fdae38be910>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py\", line 223, in sample_and_compute_grads\n",
      "    grads, infos = worker.compute_gradients(samples)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1162, in compute_gradients\n",
      "    grad_out[pid], info_out[pid] = self.policy_map[pid].compute_gradients(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n",
      "    return func(self, *a, **k)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 873, in compute_gradients\n",
      "    tower_outputs = self._multi_gpu_parallel_grad_calc([postprocessed_batch])\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1320, in _multi_gpu_parallel_grad_calc\n",
      "    raise last_result[0] from last_result[1]\n",
      "ValueError: Expected parameter loc (Tensor of shape (3, 1)) of distribution Normal(loc: torch.Size([3, 1]), scale: torch.Size([3, 1])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], grad_fn=<SplitBackward0>)\n",
      " tracebackTraceback (most recent call last):\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/policy/torch_policy_v2.py\", line 1235, in _worker\n",
      "    self.loss(model, self.dist_class, sample_batch)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c_torch_policy.py\", line 76, in loss\n",
      "    dist = dist_class(logits, model)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/models/torch/torch_action_dist.py\", line 250, in __init__\n",
      "    self.dist = torch.distributions.normal.Normal(mean, torch.exp(log_std))\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/normal.py\", line 56, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/torch/distributions/distribution.py\", line 62, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter loc (Tensor of shape (3, 1)) of distribution Normal(loc: torch.Size([3, 1]), scale: torch.Size([3, 1])) to satisfy the constraint Real(), but found invalid values:\n",
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan]], grad_fn=<SplitBackward0>)\n",
      "\n",
      "In tower 0 on device cpu\n",
      "2023-09-29 11:24:03,347\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3747891, ip=192.168.3.6, actor_id=aa801bc8040ff787f66e8fef01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fa0065dc970>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py\", line 222, in sample_and_compute_grads\n",
      "    samples = worker.sample()\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 915, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 495, in get_data\n",
      "    raise RuntimeError(\"Sampling thread has died\")\n",
      "RuntimeError: Sampling thread has died\n",
      "2023-09-29 11:24:03,349\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3747890, ip=192.168.3.6, actor_id=37c898138c5237e8fead621401000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe3b8133970>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py\", line 222, in sample_and_compute_grads\n",
      "    samples = worker.sample()\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 915, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 495, in get_data\n",
      "    raise RuntimeError(\"Sampling thread has died\")\n",
      "RuntimeError: Sampling thread has died\n",
      "2023-09-29 11:24:03,350\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3747886, ip=192.168.3.6, actor_id=9081e3d9beb877a1fcadc69701000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd7524de940>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py\", line 222, in sample_and_compute_grads\n",
      "    samples = worker.sample()\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 915, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 495, in get_data\n",
      "    raise RuntimeError(\"Sampling thread has died\")\n",
      "RuntimeError: Sampling thread has died\n",
      "2023-09-29 11:24:03,352\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RolloutWorker.apply()\u001b[39m (pid=3747884, ip=192.168.3.6, actor_id=ed284885da238231c5a4f55c01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f888eb5a970>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 185, in apply\n",
      "    raise e\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py\", line 176, in apply\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/a3c/a3c.py\", line 222, in sample_and_compute_grads\n",
      "    samples = worker.sample()\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 915, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 92, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/sampler.py\", line 495, in get_data\n",
      "    raise RuntimeError(\"Sampling thread has died\")\n",
      "RuntimeError: Sampling thread has died\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:25:13,219 E 3744013 3744013] (raylet) node_manager.cc:3069: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 5d869e95c4d257142ece2188885319db93e99282be62d26ddb97a7d2, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 11:26:13,221 E 3744013 3744013] (raylet) node_manager.cc:3069: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 5d869e95c4d257142ece2188885319db93e99282be62d26ddb97a7d2, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "info = algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learner': {},\n",
       " 'num_env_steps_sampled': 7110,\n",
       " 'num_env_steps_trained': 7110,\n",
       " 'num_agent_steps_sampled': 7000,\n",
       " 'num_agent_steps_trained': 7000}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info['info']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
