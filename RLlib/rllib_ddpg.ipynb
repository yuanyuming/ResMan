{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 09:13:20,919\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:442: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-09-29 09:13:22,093\tINFO algorithm.py:536 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': None, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'is_atari': None, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 30, 'batch_mode': 'complete_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.01, 'lr_schedule': None, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 256, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'OrnsteinUhlenbeckNoise', 'random_timesteps': 1000, 'ou_base_scale': 0.1, 'ou_theta': 0.15, 'ou_sigma': 0.2, 'initial_scale': 1.0, 'final_scale': 0.02, 'scale_timesteps': 10000}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'target_network_update_freq': 0, 'replay_buffer_config': {'type': 'MultiAgentPrioritizedReplayBuffer', 'capacity': 50000, 'prioritized_replay': -1, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'worker_side_prioritization': False}, 'num_steps_sampled_before_learning_starts': 1500, 'store_buffer_in_checkpoints': False, 'adam_epsilon': 1e-08, 'tau': 0.002, 'twin_q': False, 'policy_delay': 1, 'smooth_target_policy': False, 'target_noise': 0.2, 'target_noise_clip': 0.5, 'use_state_preprocessor': False, 'actor_hiddens': [400, 300], 'actor_hidden_activation': 'relu', 'critic_hiddens': [400, 300], 'critic_hidden_activation': 'relu', 'n_step': 1, 'training_intensity': None, 'critic_lr': 0.001, 'actor_lr': 0.001, 'use_huber': False, 'huber_threshold': 1.0, 'l2_reg': 1e-06, 'worker_side_prioritization': -1, 'input': 'sampler', 'multiagent': {'policies': {'Machine_0': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_1': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_2': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_3': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_4': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_5': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_6': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_7': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_8': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_9': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_10': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_11': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {})}, 'policy_mapping_fn': <function <lambda> at 0x7f493f453940>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 10}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ddpg.ddpg import DDPGConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune import register_env\n",
    "\n",
    "import rllib_setup\n",
    "\n",
    "ray.init()\n",
    "env_name = \"VJS\"\n",
    "register_env(\n",
    "    env_name,\n",
    "    lambda config: rllib_setup.get_env_continuous(),\n",
    ")\n",
    "test_env = rllib_setup.get_env_continuous()\n",
    "\n",
    "\n",
    "def policies(agent_ids):\n",
    "    obs_space = test_env.observation_space\n",
    "    act_space = test_env.action_space\n",
    "    return {\n",
    "        str(i): (\n",
    "            None,\n",
    "            obs_space,\n",
    "            act_space,\n",
    "            {}\n",
    "            # config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }\n",
    "\n",
    "\n",
    "config = (\n",
    "    DDPGConfig().rollouts(num_rollout_workers=10, rollout_fragment_length=30)\n",
    "    .training(lr=0.01)\n",
    "    .resources(num_gpus=1)\n",
    "    .multi_agent(\n",
    "        policies=policies(test_env._agent_ids),\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: str(agent_id),\n",
    "    )\n",
    "    .environment(disable_env_checking=True)\n",
    ")\n",
    "config.batch_mode = \"complete_episodes\"\n",
    "print(config.to_dict())\n",
    "# Build a Algorithm object from the config and run one training iteration.\n",
    "algo = config.build(env=env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-928.0687255859375\n",
      "-1975.14599609375\n",
      "-1231.05126953125\n",
      "-784.518310546875\n",
      "-904.2579345703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:23:22,409 E 3702006 3702006] (raylet) node_manager.cc:3069: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-09-29 10:23:25,779\tERROR actor_manager.py:507 -- Ray error, taking actor 7 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-09-29 10:23:25,784\tERROR actor_manager.py:507 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:24:24,353 E 3702006 3702006] (raylet) node_manager.cc:3069: 17 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-09-29 10:24:41,814\tERROR actor_manager.py:507 -- Ray error, taking actor 8 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:25:25,767 E 3702006 3702006] (raylet) node_manager.cc:3069: 17 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-09-29 10:26:16,395\tERROR actor_manager.py:507 -- Ray error, taking actor 5 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-09-29 10:26:16,445\tERROR actor_manager.py:507 -- Ray error, taking actor 10 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:26:26,723 E 3702006 3702006] (raylet) node_manager.cc:3069: 18 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-09-29 10:27:03,478\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff1f437b7684bf436521df912c01000000 Worker ID: 7cb2d28e3961c17b1a6b213f6d271f25dea6ddf9caaae7b310c05e57 Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 39657 Worker PID: 3705548 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:27:08,981\tERROR actor_manager.py:507 -- Ray error, taking actor 9 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-09-29 10:27:19,676\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff76524f59dce85ea9e85a566101000000 Worker ID: bfe8bfe02252c2b452d6258d2ecb381f8679ec0e218abbdd295e6e21 Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 43643 Worker PID: 3705545 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:27:19,678\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff35eebe346481f17b9a8fc73101000000 Worker ID: a4985a6de7f352d60217a680ac3d521dd100b5cdee4e3eddaebe0ac5 Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 42723 Worker PID: 3705543 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:27:19,679\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff13e1cd16731d896c08659e2801000000 Worker ID: 0fdfcf2d875391071182f0bc66e5a473dff1be3b30e3914d4f1e0b10 Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 42783 Worker PID: 3705542 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:27:19,679\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffa280a6c5bc576edfa16c02da01000000 Worker ID: 8f3bc0b6428b477c60abdd905b04acf36fc3cc5d9b49ad42c00bfbe0 Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 44055 Worker PID: 3705541 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:27:27,460 E 3702006 3702006] (raylet) node_manager.cc:3069: 15 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-09-29 10:27:39,049\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff1fe591bcc520f4d4c316a5eb01000000 Worker ID: 534590256d18025174e203f465cc310843316a86c738d39426bc88fd Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 39295 Worker PID: 3705540 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:28:20,304\tERROR actor_manager.py:507 -- Ray error, taking actor 1 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-09-29 10:28:20,353\tERROR actor_manager.py:507 -- Ray error, taking actor 2 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-09-29 10:28:20,354\tERROR actor_manager.py:507 -- Ray error, taking actor 3 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-09-29 10:28:20,356\tERROR actor_manager.py:507 -- Ray error, taking actor 4 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-09-29 10:28:20,357\tERROR actor_manager.py:507 -- Ray error, taking actor 5 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-09-29 10:28:20,358\tERROR actor_manager.py:507 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:28:30,364 E 3702006 3702006] (raylet) node_manager.cc:3069: 16 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-09-29 10:29:06,010\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff6e66129fedea67c4d9377d7f01000000 Worker ID: 54ee8fd1e05a9ca0c2a40b050f9625dd8fba6d34492009f290e5ed70 Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 33323 Worker PID: 1237366 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:29:06,062\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffa280a6c5bc576edfa16c02da01000000 Worker ID: b7295ce0fe3b4d43a54e780f9b869aa72e016632eca15162af4a4f08 Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 44581 Worker PID: 1282995 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:29:06,063\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff1f437b7684bf436521df912c01000000 Worker ID: 3d41504c087217a5df7dc2b71dd9ec18e9e64600ed3dcc7abc41b6cc Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 44551 Worker PID: 1279840 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:29:06,063\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff43d038f3d176d4cf94252bd701000000 Worker ID: f3c60554aab6349253520c2a01a3a905a4c9ad3e632f96f4090ff34f Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 41657 Worker PID: 1282988 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:29:17,465\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff76524f59dce85ea9e85a566101000000 Worker ID: 181e183977c0b45fcf9c9ca830d983c343cce15274a2f7114e18efcb Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 34195 Worker PID: 1282992 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:29:17,991\tERROR actor_manager.py:507 -- Ray error, taking actor 2 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-09-29 10:29:18,252\tERROR actor_manager.py:507 -- Ray error, taking actor 6 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-09-29 10:29:18,254\tERROR actor_manager.py:507 -- Ray error, taking actor 7 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-09-29 10:29:18,255\tERROR actor_manager.py:507 -- Ray error, taking actor 8 out of service. The actor died unexpectedly before finishing this task.\n",
      "2023-09-29 10:29:18,255\tERROR actor_manager.py:507 -- Ray error, taking actor 9 out of service. The actor died unexpectedly before finishing this task.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:29:36,625 E 3702006 3702006] (raylet) node_manager.cc:3069: 12 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:30:37,902 E 3702006 3702006] (raylet) node_manager.cc:3069: 15 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:31:44,012 E 3702006 3702006] (raylet) node_manager.cc:3069: 10 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-09-29 10:32:17,226\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff13e1cd16731d896c08659e2801000000 Worker ID: 3af0b12f3609bbdee773bd86018ef71b3e57e7c9c3a6df2c58fa74c6 Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 41829 Worker PID: 1340399 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:32:17,335\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff1f437b7684bf436521df912c01000000 Worker ID: 23bf8361fcd0b5ebd29e81c2c4de17ed227c9eb502d2dbeed1d11cc1 Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 44403 Worker PID: 1343075 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:32:17,336\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff1fe591bcc520f4d4c316a5eb01000000 Worker ID: ec2e612a3fd26a782c2ea536e3ac79c784a0e53e2eace17ff245c773 Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 45031 Worker PID: 1307645 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:32:17,336\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff35eebe346481f17b9a8fc73101000000 Worker ID: f4831c0e53e6a12970fa2c93d84c54b670004de5ce9c9ab9f7296233 Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 34789 Worker PID: 1293683 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:32:17,337\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffa280a6c5bc576edfa16c02da01000000 Worker ID: bf5aa3c27d3e98420e126cc432215dbc494a9302a7f5c0c775b3d23f Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 36769 Worker PID: 1343073 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-29 10:32:17,337\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff6e66129fedea67c4d9377d7f01000000 Worker ID: 4147cf764649cddcf0804013935e8a69720a5b1ae42553b3f2482e1f Node ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579 Worker IP address: 192.168.3.6 Worker port: 33121 Worker PID: 1351619 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:32:44,031 E 3702006 3702006] (raylet) node_manager.cc:3069: 12 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:33:44,819 E 3702006 3702006] (raylet) node_manager.cc:3069: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:34:45,301 E 3702006 3702006] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:35:46,953 E 3702006 3702006] (raylet) node_manager.cc:3069: 13 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:36:46,954 E 3702006 3702006] (raylet) node_manager.cc:3069: 20 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:37:46,955 E 3702006 3702006] (raylet) node_manager.cc:3069: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:38:46,956 E 3702006 3702006] (raylet) node_manager.cc:3069: 16 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:39:46,957 E 3702006 3702006] (raylet) node_manager.cc:3069: 28 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:41:46,962 E 3702006 3702006] (raylet) node_manager.cc:3069: 6 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:42:47,099 E 3702006 3702006] (raylet) node_manager.cc:3069: 22 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:43:47,101 E 3702006 3702006] (raylet) node_manager.cc:3069: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:44:47,859 E 3702006 3702006] (raylet) node_manager.cc:3069: 6 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:45:48,145 E 3702006 3702006] (raylet) node_manager.cc:3069: 17 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:46:48,146 E 3702006 3702006] (raylet) node_manager.cc:3069: 15 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:47:48,148 E 3702006 3702006] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:48:48,268 E 3702006 3702006] (raylet) node_manager.cc:3069: 12 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:49:48,323 E 3702006 3702006] (raylet) node_manager.cc:3069: 22 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:50:48,326 E 3702006 3702006] (raylet) node_manager.cc:3069: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:51:48,758 E 3702006 3702006] (raylet) node_manager.cc:3069: 15 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:52:48,759 E 3702006 3702006] (raylet) node_manager.cc:3069: 29 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:53:48,760 E 3702006 3702006] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:54:48,761 E 3702006 3702006] (raylet) node_manager.cc:3069: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:55:49,606 E 3702006 3702006] (raylet) node_manager.cc:3069: 24 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-29 10:56:49,608 E 3702006 3702006] (raylet) node_manager.cc:3069: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 658040670f34eae9d7e31a0e38396297cac64a01d85cfb2d5eab3579, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yuan/ResMan/RLlib/rllib_ddpg.ipynb  2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_ddpg.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_ddpg.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_ddpg.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         info \u001b[39m=\u001b[39m algo\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib/rllib_ddpg.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(info[\u001b[39m'\u001b[39m\u001b[39minfo\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlearner\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mMachine_0\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlearner_stats\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mactor_loss\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:386\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     skipped \u001b[39m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:803\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m     (\n\u001b[1;32m    796\u001b[0m         results,\n\u001b[1;32m    797\u001b[0m         train_iter_ctx,\n\u001b[1;32m    798\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    799\u001b[0m \u001b[39m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[39m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     results, train_iter_ctx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_training_iteration()\n\u001b[1;32m    805\u001b[0m \u001b[39m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[39mif\u001b[39;00m evaluate_this_iter \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:2853\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2852\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[0;32m-> 2853\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step()\n\u001b[1;32m   2854\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2855\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/simple_q/simple_q.py:359\u001b[0m, in \u001b[0;36mSimpleQ.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m     train_results \u001b[39m=\u001b[39m multi_gpu_train_one_step(\u001b[39mself\u001b[39m, train_batch)\n\u001b[1;32m    358\u001b[0m \u001b[39m# Update replay buffer priorities.\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m update_priorities_in_replay_buffer(\n\u001b[1;32m    360\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_replay_buffer,\n\u001b[1;32m    361\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    362\u001b[0m     train_batch,\n\u001b[1;32m    363\u001b[0m     train_results,\n\u001b[1;32m    364\u001b[0m )\n\u001b[1;32m    366\u001b[0m last_update \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_counters[LAST_TARGET_UPDATE_TS]\n\u001b[1;32m    367\u001b[0m \u001b[39mif\u001b[39;00m cur_ts \u001b[39m-\u001b[39m last_update \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtarget_network_update_freq:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/replay_buffers/utils.py:121\u001b[0m, in \u001b[0;36mupdate_priorities_in_replay_buffer\u001b[0;34m(replay_buffer, config, train_batch, train_results)\u001b[0m\n\u001b[1;32m    117\u001b[0m     prio_dict[policy_id] \u001b[39m=\u001b[39m (batch_indices, td_error)\n\u001b[1;32m    119\u001b[0m \u001b[39m# Make the actual buffer API call to update the priority weights on all\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39m# policies.\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m replay_buffer\u001b[39m.\u001b[39;49mupdate_priorities(prio_dict)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/replay_buffers/multi_agent_prioritized_replay_buffer.py:252\u001b[0m, in \u001b[0;36mMultiAgentPrioritizedReplayBuffer.update_priorities\u001b[0;34m(self, prio_dict)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mfor\u001b[39;00m policy_id, (batch_indexes, td_errors) \u001b[39min\u001b[39;00m prio_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    251\u001b[0m     new_priorities \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(td_errors) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprioritized_replay_eps\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffers[policy_id]\u001b[39m.\u001b[39;49mupdate_priorities(\n\u001b[1;32m    253\u001b[0m         batch_indexes, new_priorities\n\u001b[1;32m    254\u001b[0m     )\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/replay_buffers/prioritized_replay_buffer.py:187\u001b[0m, in \u001b[0;36mPrioritizedReplayBuffer.update_priorities\u001b[0;34m(self, idxes, priorities)\u001b[0m\n\u001b[1;32m    185\u001b[0m delta \u001b[39m=\u001b[39m priority\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_alpha \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_it_sum[idx]\n\u001b[1;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prio_change_stats\u001b[39m.\u001b[39mpush(delta)\n\u001b[0;32m--> 187\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_it_sum[idx] \u001b[39m=\u001b[39m priority\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_alpha\n\u001b[1;32m    188\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_it_min[idx] \u001b[39m=\u001b[39m priority\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_alpha\n\u001b[1;32m    190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_priority \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_priority, priority)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/execution/segment_tree.py:155\u001b[0m, in \u001b[0;36mSegmentTree.__setitem__\u001b[0;34m(self, idx, val)\u001b[0m\n\u001b[1;32m    153\u001b[0m update_idx \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m idx  \u001b[39m# calculate only once\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m# Update the reduction value at the correct \"first half\" idx.\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue[idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moperation(\n\u001b[1;32m    156\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue[update_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue[update_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m idx \u001b[39m=\u001b[39m idx \u001b[39m>>\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        info = algo.train()\n",
    "    print(info['info']['learner']['Machine_0']['learner_stats']['actor_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
