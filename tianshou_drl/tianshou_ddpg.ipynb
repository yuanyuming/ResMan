{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.exploration import GaussianNoise\n",
    "from tianshou.policy import DDPGPolicy, MultiAgentPolicyManager\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.continuous import Actor, Critic\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Args:\n",
    "    def __init__(\n",
    "        self,\n",
    "        task=\"Pendulum-v1\",\n",
    "        reward_threshold=None,\n",
    "        seed=0,\n",
    "        buffer_size=20000,\n",
    "        actor_lr=1e-4,\n",
    "        critic_lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        exploration_noise=0.1,\n",
    "        epoch=5,\n",
    "        step_per_epoch=20000,\n",
    "        step_per_collect=8,\n",
    "        update_per_step=0.125,\n",
    "        batch_size=128,\n",
    "        hidden_sizes=[128, 128],\n",
    "        training_num=8,\n",
    "        test_num=100,\n",
    "        logdir=\"log\",\n",
    "        render=0.0,\n",
    "        rew_norm=False,\n",
    "        n_step=3,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ):\n",
    "        self.task = task\n",
    "        self.reward_threshold = reward_threshold\n",
    "        self.seed = seed\n",
    "        self.buffer_size = buffer_size\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.exploration_noise = exploration_noise\n",
    "        self.epoch = epoch\n",
    "        self.step_per_epoch = step_per_epoch\n",
    "        self.step_per_collect = step_per_collect\n",
    "        self.update_per_step = update_per_step\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.training_num = training_num\n",
    "        self.test_num = test_num\n",
    "        self.logdir = logdir\n",
    "        self.render = render\n",
    "        self.rew_norm = rew_norm\n",
    "        self.n_step = n_step\n",
    "        self.device = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _get_agents(get_env, args=Args()):\n",
    "    env = get_env()\n",
    "    state_shape = env.observation_space.shape or env.observation_space.n\n",
    "    action_shape = env.action_space.shape or env.action_space.n\n",
    "    max_action = env.action_space.high[0]\n",
    "    agents = []\n",
    "    for i in range(env.num_agents):\n",
    "        # model\n",
    "        net = Net(state_shape, hidden_sizes=args.hidden_sizes, device=args.device)\n",
    "        actor = Actor(net, action_shape, max_action=max_action, device=args.device).to(\n",
    "            args.device\n",
    "        )\n",
    "        actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n",
    "        net = Net(\n",
    "            state_shape,\n",
    "            action_shape,\n",
    "            hidden_sizes=args.hidden_sizes,\n",
    "            concat=True,\n",
    "            device=args.device,\n",
    "        )\n",
    "        critic = Critic(net, device=args.device).to(args.device)\n",
    "        critic_optim = torch.optim.Adam(critic.parameters(), lr=args.critic_lr)\n",
    "        policy = DDPGPolicy(\n",
    "            actor,\n",
    "            actor_optim,\n",
    "            critic,\n",
    "            critic_optim,\n",
    "            tau=args.tau,\n",
    "            gamma=args.gamma,\n",
    "            exploration_noise=GaussianNoise(sigma=args.exploration_noise),\n",
    "            reward_normalization=args.rew_norm,\n",
    "            estimation_step=args.n_step,\n",
    "            action_space=env.action_space,\n",
    "        )\n",
    "        agents.append(policy)\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, env.agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_collector(get_env,args=Args()):\n",
    "    env = get_env()\n",
    "    args.reward_threshold = 1000\n",
    "    # you can also use tianshou.env.SubprocVectorEnv\n",
    "    train_envs = DummyVectorEnv([get_env for _ in range(args.training_num)])\n",
    "    test_envs = DummyVectorEnv([get_env for _ in range(args.test_num)])\n",
    "    # seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    train_envs.seed(args.seed)\n",
    "    test_envs.seed(args.seed)\n",
    "    # policy\n",
    "    policy, agents = _get_agents(get_env, args)\n",
    "    # collector\n",
    "    train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(args.buffer_size, len(train_envs)),\n",
    "        exploration_noise=True,\n",
    "    )\n",
    "    test_collector = Collector(policy, test_envs)\n",
    "    return train_collector, test_collector, policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_ddpg(get_env, args=Args()):\n",
    "    # log\n",
    "    log_path = os.path.join(args.logdir, args.task, \"ddpg\")\n",
    "    writer = SummaryWriter(log_path)\n",
    "    logger = TensorboardLogger(writer)\n",
    "    # collector\n",
    "    test_collector, train_collector, policy= construct_collector(get_env, args)\n",
    "    # callback function\n",
    "    def save_best_fn(policy):\n",
    "        torch.save(policy.state_dict(), os.path.join(log_path, \"policy.pth\"))\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= args.reward_threshold\n",
    "\n",
    "    # trainer\n",
    "    result = offpolicy_trainer(\n",
    "        policy,\n",
    "        train_collector,\n",
    "        test_collector,\n",
    "        args.epoch,\n",
    "        args.step_per_epoch,\n",
    "        args.step_per_collect,\n",
    "        args.test_num,\n",
    "        args.batch_size,\n",
    "        update_per_step=args.update_per_step,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        logger=logger,\n",
    "    )\n",
    "    assert stop_fn(result[\"best_reward\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_ddpg(get_env,policy,result, args=Args()):\n",
    "    pprint.pprint(result)\n",
    "    # Let's watch its performance!\n",
    "    env = get_env()\n",
    "    policy.eval()\n",
    "    collector = Collector(policy, env)\n",
    "    result = collector.collect(n_episode=1, render=args.render)\n",
    "    rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "    print(f\"Final reward: {rews.mean()}, length: {lens.mean()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 假设这是一个用于训练和评估DDPG算法的类\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDDPGTrainer\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, get_env, args\u001b[39m=\u001b[39mArgs()):\n\u001b[1;32m      4\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_env \u001b[39m=\u001b[39m get_env\n",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m, in \u001b[0;36mDDPGTrainer\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDDPGTrainer\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, get_env, args\u001b[39m=\u001b[39mArgs()):\n\u001b[1;32m      4\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_env \u001b[39m=\u001b[39m get_env\n\u001b[1;32m      5\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Args' is not defined"
     ]
    }
   ],
   "source": [
    "# 假设这是一个用于训练和评估DDPG算法的类\n",
    "class DDPGTrainer:\n",
    "    def __init__(self, get_env, args=Args()):\n",
    "        self.get_env = get_env\n",
    "        self.args = args\n",
    "        self.policy, self.agents = self._get_agents()\n",
    "        self.train_collector, self.test_collector = self.construct_collector()\n",
    "        # log\n",
    "        log_path = os.path.join(args.logdir, args.task, \"ddpg\")\n",
    "        writer = SummaryWriter(log_path)\n",
    "        logger = TensorboardLogger(writer)\n",
    "        # callback function\n",
    "        def save_best_fn(policy):\n",
    "            torch.save(policy.state_dict(), os.path.join(log_path, \"policy.pth\"))\n",
    "\n",
    "        def stop_fn(mean_rewards):\n",
    "            return mean_rewards >= args.reward_threshold\n",
    "\n",
    "        self.save_best_fn = save_best_fn\n",
    "        self.stop_fn = stop_fn\n",
    "        self.logger = logger\n",
    "\n",
    "    def _get_agents(self):\n",
    "        env = self.get_env()\n",
    "        state_shape = env.observation_space.shape or env.observation_space.n\n",
    "        action_shape = env.action_space.shape or env.action_space.n\n",
    "        max_action = env.action_space.high[0]\n",
    "        agents = []\n",
    "        for i in range(env.num_agents):\n",
    "            # model\n",
    "            net = Net(state_shape, hidden_sizes=self.args.hidden_sizes, device=self.args.device)\n",
    "            actor = Actor(net, action_shape, max_action=max_action, device=self.args.device).to(\n",
    "                self.args.device\n",
    "            )\n",
    "            actor_optim = torch.optim.Adam(actor.parameters(), lr=self.args.actor_lr)\n",
    "            net = Net(\n",
    "                state_shape,\n",
    "                action_shape,\n",
    "                hidden_sizes=self.args.hidden_sizes,\n",
    "                concat=True,\n",
    "                device=self.args.device,\n",
    "            )\n",
    "            critic = Critic(net, device=self.args.device).to(self.args.device)\n",
    "            critic_optim = torch.optim.Adam(critic.parameters(), lr=self.args.critic_lr)\n",
    "            policy = DDPGPolicy(\n",
    "                actor,\n",
    "                actor_optim,\n",
    "                critic,\n",
    "                critic_optim,\n",
    "                tau=self.args.tau,\n",
    "                gamma=self.args.gamma,\n",
    "                exploration_noise=GaussianNoise(sigma=self.args.exploration_noise),\n",
    "                reward_normalization=self.args.rew_norm,\n",
    "                estimation_step=self.args.n_step,\n",
    "                action_space=env.action_space,\n",
    "            )\n",
    "            agents.append(policy)\n",
    "        maddpg_policy = MultiAgentPolicyManager(agents, env)\n",
    "        return maddpg_policy, agents\n",
    "\n",
    "    def construct_collector(self):\n",
    "        env = self.get_env()\n",
    "        # you can also use tianshou.env.SubprocVectorEnv\n",
    "        train_envs = DummyVectorEnv([self.get_env for _ in range(self.args.training_num)])\n",
    "        test_envs = DummyVectorEnv([self.get_env for _ in range(self.args.test_num)])\n",
    "        # seed\n",
    "        np.random.seed(self.args.seed)\n",
    "        torch.manual_seed(self.args.seed)\n",
    "        train_envs.seed(self.args.seed)\n",
    "        test_envs.seed(self.args.seed)\n",
    "        \n",
    "        # collector\n",
    "        train_collector = Collector(\n",
    "            self.policy,\n",
    "            train_envs,\n",
    "            VectorReplayBuffer(self.args.buffer_size, len(train_envs)),\n",
    "            exploration_noise=True,\n",
    "        )\n",
    "        test_collector = Collector(self.policy, test_envs)\n",
    "        return train_collector, test_collector\n",
    "    \n",
    "    def train(self):\n",
    "        # trainer\n",
    "        result = offpolicy_trainer(\n",
    "            self.policy,\n",
    "            self.train_collector,\n",
    "            self.test_collector,\n",
    "            self.args.epoch,\n",
    "            self.args.step_per_epoch,\n",
    "            self.args.step_per_collect,\n",
    "            self.args.test_num,\n",
    "            self.args.batch_size,\n",
    "            update_per_step=self.args.update_per_step,\n",
    "            stop_fn=self.stop_fn,\n",
    "            save_best_fn=self.save_best_fn,\n",
    "            logger=self.logger,\n",
    "        )\n",
    "        assert self.stop_fn(result[\"best_reward\"])\n",
    "    \n",
    "    def eval(self):\n",
    "        pprint.pprint(result)\n",
    "        # Let's watch its performance!\n",
    "        env = self.get_env()\n",
    "        self.policy.eval()\n",
    "        collector = Collector(self.policy, env)\n",
    "        result = collector.collect(n_episode=1, render=self.args.render)\n",
    "        rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "        print(f\"Final reward: {rews.mean()}, length: {lens.mean()}\")\n",
    "\n",
    "# 使用这个类的示例代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建一个环境函数，例如gym.make(\"Pendulum-v0\")\n",
    "    get_env = lambda: gym.make(\"Pendulum-v0\")\n",
    "    # 创建一个参数对象，可以设置一些超参数，例如hidden_sizes=[128, 128], actor_lr=1e-3等\n",
    "    args = Args(hidden_sizes=[128, 128], actor_lr=1e-3)\n",
    "    # 创建一个DDPGTrainer对象，传入环境函数和参数对象\n",
    "    trainer = DDPGTrainer(get_env, args)\n",
    "    # 调用train方法进行训练\n",
    "    trainer.train()\n",
    "    # 调用eval方法进行评估\n",
    "    trainer.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
