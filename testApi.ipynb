{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import Environment\n",
    "from tianshou.env import (\n",
    "    ContinuousToDiscrete,\n",
    "    DummyVectorEnv,\n",
    "    PettingZooEnv,\n",
    "    ShmemVectorEnv,\n",
    "    SubprocVectorEnv,\n",
    ")\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from pettingzoo.utils.wrappers import BaseWrapper\n",
    "def get_env():\n",
    "    env = Environment.VehicleJobSchedulingEnvACE()\n",
    "    env = BaseWrapper(env)\n",
    "    env = ContinuousToDiscrete(env, 30)\n",
    "    env = PettingZooEnv(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PettingZooEnv' object has no attribute 'possible_agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m env \u001b[39m=\u001b[39m PettingZooEnv(env)\n\u001b[1;32m      4\u001b[0m env \u001b[39m=\u001b[39m ContinuousToDiscrete(env, \u001b[39m30\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m env \u001b[39m=\u001b[39m PettingZooEnv(env)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/tianshou/env/pettingzoo_env.py:41\u001b[0m, in \u001b[0;36mPettingZooEnv.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env\n\u001b[1;32m     40\u001b[0m \u001b[39m# agent idx list\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mpossible_agents\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_idx \u001b[39m=\u001b[39m {}\n\u001b[1;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m i, agent_id \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents):\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/gymnasium/core.py:282\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39melif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    281\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PettingZooEnv' object has no attribute 'possible_agents'"
     ]
    }
   ],
   "source": [
    "env = Environment.VehicleJobSchedulingEnvACE()\n",
    "env = BaseWrapper(env)\n",
    "env = PettingZooEnv(env)\n",
    "env = ContinuousToDiscrete(env, 30)\n",
    "env = PettingZooEnv(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.VehicleJobSchedulingEnvACE()\n",
    "env = BaseWrapper(env)\n",
    "env = PettingZooEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.33333334, 3.0, (1,), float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gymnasium.spaces import Discrete\n",
    "\n",
    "Discrete(30).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
       " 101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
       "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousToDiscrete(gym.ActionWrapper):\n",
    "    \"\"\"Gym environment wrapper to take discrete action in a continuous environment.\n",
    "\n",
    "    :param gym.Env env: gym environment with continuous action space.\n",
    "    :param int action_per_dim: number of discrete actions in each dimension\n",
    "        of the action space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, action_per_dim: Union[int, List[int]]) -> None:\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.Box)\n",
    "        low, high = env.action_space.low, env.action_space.high\n",
    "        if isinstance(action_per_dim, int):\n",
    "            action_per_dim = [action_per_dim] * env.action_space.shape[0]\n",
    "        assert len(action_per_dim) == env.action_space.shape[0]\n",
    "        self.action_space = gym.spaces.MultiDiscrete(action_per_dim)\n",
    "        self.mesh = np.array(\n",
    "            [np.linspace(lo, hi, a) for lo, hi, a in zip(low, high, action_per_dim)],\n",
    "            dtype=object\n",
    "        )\n",
    "\n",
    "    def action(self, act: np.ndarray) -> np.ndarray:  # type: ignore\n",
    "        # modify act\n",
    "        assert len(act.shape) <= 2, f\"Unknown action format with shape {act.shape}.\"\n",
    "        if len(act.shape) == 1:\n",
    "            return np.array([self.mesh[i][a] for i, a in enumerate(act)])\n",
    "        return np.array([[self.mesh[i][a] for i, a in enumerate(a_)] for a_ in act])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[39m=\u001b[39m get_env()\n\u001b[1;32m      2\u001b[0m obs, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      3\u001b[0m obs\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mget_env\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m env \u001b[39m=\u001b[39m Environment\u001b[39m.\u001b[39mVehicleJobSchedulingEnvACE()\n\u001b[1;32m     14\u001b[0m env \u001b[39m=\u001b[39m BaseWrapper(env)\n\u001b[0;32m---> 15\u001b[0m env \u001b[39m=\u001b[39m ContinuousToDiscrete(env, \u001b[39m30\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m env \u001b[39m=\u001b[39m PettingZooEnv(env)\n\u001b[1;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m env\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/tianshou/env/gym_wrappers.py:18\u001b[0m, in \u001b[0;36mContinuousToDiscrete.__init__\u001b[0;34m(self, env, action_per_dim)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, env: gym\u001b[39m.\u001b[39mEnv, action_per_dim: Union[\u001b[39mint\u001b[39m, List[\u001b[39mint\u001b[39m]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(env)\n\u001b[0;32m---> 18\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(env\u001b[39m.\u001b[39maction_space, gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mBox)\n\u001b[1;32m     19\u001b[0m     low, high \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(action_per_dim, \u001b[39mint\u001b[39m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = get_env()\n",
    "obs, *_ = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m test_envs\u001b[39m.\u001b[39mseed(seed)\n\u001b[1;32m     82\u001b[0m \u001b[39m# ======== Step 2: Agent setup =========\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m policy, optim, agents \u001b[39m=\u001b[39m _get_agents()\n\u001b[1;32m     85\u001b[0m \u001b[39m# ======== Step 3: Collector setup =========\u001b[39;00m\n\u001b[1;32m     86\u001b[0m train_collector \u001b[39m=\u001b[39m Collector(\n\u001b[1;32m     87\u001b[0m     policy,\n\u001b[1;32m     88\u001b[0m     train_envs,\n\u001b[1;32m     89\u001b[0m     VectorReplayBuffer(\u001b[39m20_000\u001b[39m, \u001b[39mlen\u001b[39m(train_envs)),\n\u001b[1;32m     90\u001b[0m     exploration_noise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m )\n",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m, in \u001b[0;36m_get_agents\u001b[0;34m(agent_learn, agent_opponent, optim)\u001b[0m\n\u001b[1;32m     34\u001b[0m observation_space \u001b[39m=\u001b[39m (\n\u001b[1;32m     35\u001b[0m     env\u001b[39m.\u001b[39mobservation_space[\u001b[39m\"\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(env\u001b[39m.\u001b[39mobservation_space, gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mDict)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m env\u001b[39m.\u001b[39mobservation_space\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m agent_learn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[39m# model\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     net \u001b[39m=\u001b[39m Net(\n\u001b[1;32m     42\u001b[0m         state_shape\u001b[39m=\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49mshape \u001b[39mor\u001b[39;49;00m observation_space\u001b[39m.\u001b[39;49mn,\n\u001b[1;32m     43\u001b[0m         action_shape\u001b[39m=\u001b[39;49menv\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49mshape \u001b[39mor\u001b[39;49;00m env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49mn,\n\u001b[1;32m     44\u001b[0m         hidden_sizes\u001b[39m=\u001b[39;49m[\u001b[39m128\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m128\u001b[39;49m],\n\u001b[1;32m     45\u001b[0m         device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available() \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     46\u001b[0m     )\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available() \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m optim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         optim \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(net\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"This is a minimal example of using Tianshou with MARL to train agents.\n",
    "\n",
    "Author: Will (https://github.com/WillDudley)\n",
    "\n",
    "Python version used: 3.8.10\n",
    "\n",
    "Requirements:\n",
    "pettingzoo == 1.22.0\n",
    "git+https://github.com/thu-ml/tianshou\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    env = _get_env()\n",
    "    observation_space = (\n",
    "        env.observation_space[\"observation\"]\n",
    "        if isinstance(env.observation_space, gym.spaces.Dict)\n",
    "        else env.observation_space\n",
    "    )\n",
    "    if agent_learn is None:\n",
    "        # model\n",
    "        net = Net(\n",
    "            state_shape=observation_space.shape or observation_space.n,\n",
    "            action_shape=env.action_space.shape or env.action_space.n,\n",
    "            hidden_sizes=[128, 128, 128, 128],\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "        agent_learn = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor=0.9,\n",
    "            estimation_step=3,\n",
    "            target_update_freq=320,\n",
    "        )\n",
    "\n",
    "    if agent_opponent is None:\n",
    "        agent_opponent = RandomPolicy()\n",
    "\n",
    "    agents = [agent_opponent, agent_learn]\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, optim, env.agents\n",
    "\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"\n",
    "    return PettingZooEnv(tictactoe_v3.env())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ======== Step 1: Environment setup =========\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(10)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(10)])\n",
    "\n",
    "    # seed\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # ======== Step 2: Agent setup =========\n",
    "    policy, optim, agents = _get_agents()\n",
    "\n",
    "    # ======== Step 3: Collector setup =========\n",
    "    train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(20_000, len(train_envs)),\n",
    "        exploration_noise=True,\n",
    "    )\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "    # policy.set_eps(1)\n",
    "    train_collector.collect(n_step=64 * 10)  # batch size * training_num\n",
    "\n",
    "    # ======== Step 4: Callback functions setup =========\n",
    "    def save_best_fn(policy):\n",
    "        model_save_path = os.path.join(\"log\", \"rps\", \"dqn\", \"policy.pth\")\n",
    "        os.makedirs(os.path.join(\"log\", \"rps\", \"dqn\"), exist_ok=True)\n",
    "        torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= 0.6\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        policy.policies[agents[1]].set_eps(0.1)\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        policy.policies[agents[1]].set_eps(0.05)\n",
    "\n",
    "    def reward_metric(rews):\n",
    "        return rews[:, 1]\n",
    "\n",
    "    # ======== Step 5: Run the trainer =========\n",
    "    result = offpolicy_trainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,\n",
    "        max_epoch=50,\n",
    "        step_per_epoch=1000,\n",
    "        step_per_collect=50,\n",
    "        episode_per_test=10,\n",
    "        batch_size=64,\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        update_per_step=0.1,\n",
    "        test_in_train=False,\n",
    "        reward_metric=reward_metric,\n",
    "    )\n",
    "\n",
    "    # return result, policy.policies[agents[1]]\n",
    "    print(f\"\\n==========Result==========\\n{result}\")\n",
    "    print(\"\\n(the trained policy can be accessed via policy.policies[agents[1]])\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d652fbd7feb28eaf2ff622dc57929f599600a55ad62e1efe6c9f098053cf5e75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
