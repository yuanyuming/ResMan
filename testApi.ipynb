{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is a minimal example of using Tianshou with MARL to train agents.\n",
    "\n",
    "Author: Will (https://github.com/WillDudley)\n",
    "\n",
    "Python version used: 3.8.10\n",
    "\n",
    "Requirements:\n",
    "pettingzoo == 1.22.0\n",
    "git+https://github.com/thu-ml/tianshou\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    env = _get_env()\n",
    "    observation_space = (\n",
    "        env.observation_space[\"observation\"]\n",
    "        if isinstance(env.observation_space, gym.spaces.Dict)\n",
    "        else env.observation_space\n",
    "    )\n",
    "    if agent_learn is None:\n",
    "        # model\n",
    "        net = Net(\n",
    "            state_shape=observation_space.shape or observation_space.n,\n",
    "            action_shape=env.action_space.shape or env.action_space.n,\n",
    "            hidden_sizes=[128, 128, 128, 128],\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "        agent_learn = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor=0.9,\n",
    "            estimation_step=3,\n",
    "            target_update_freq=320,\n",
    "        )\n",
    "\n",
    "    if agent_opponent is None:\n",
    "        agent_opponent = RandomPolicy()\n",
    "\n",
    "    agents = [agent_opponent, agent_learn]\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, optim, env.agents\n",
    "\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"\n",
    "    return PettingZooEnv(tictactoe_v3.env())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ======== Step 1: Environment setup =========\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(10)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(10)])\n",
    "\n",
    "    # seed\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # ======== Step 2: Agent setup =========\n",
    "    policy, optim, agents = _get_agents()\n",
    "\n",
    "    # ======== Step 3: Collector setup =========\n",
    "    train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(20_000, len(train_envs)),\n",
    "        exploration_noise=True,\n",
    "    )\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "    # policy.set_eps(1)\n",
    "    train_collector.collect(n_step=64 * 10)  # batch size * training_num\n",
    "\n",
    "    # ======== Step 4: Callback functions setup =========\n",
    "    def save_best_fn(policy):\n",
    "        model_save_path = os.path.join(\"log\", \"rps\", \"dqn\", \"policy.pth\")\n",
    "        os.makedirs(os.path.join(\"log\", \"rps\", \"dqn\"), exist_ok=True)\n",
    "        torch.save(policy.policies[agents[1]].state_dict(), model_save_path)\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= 0.6\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        policy.policies[agents[1]].set_eps(0.1)\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        policy.policies[agents[1]].set_eps(0.05)\n",
    "\n",
    "    def reward_metric(rews):\n",
    "        return rews[:, 1]\n",
    "\n",
    "    # ======== Step 5: Run the trainer =========\n",
    "    result = offpolicy_trainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,\n",
    "        max_epoch=50,\n",
    "        step_per_epoch=1000,\n",
    "        step_per_collect=50,\n",
    "        episode_per_test=10,\n",
    "        batch_size=64,\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        update_per_step=0.1,\n",
    "        test_in_train=False,\n",
    "        reward_metric=reward_metric,\n",
    "    )\n",
    "\n",
    "    # return result, policy.policies[agents[1]]\n",
    "    print(f\"\\n==========Result==========\\n{result}\")\n",
    "    print(\"\\n(the trained policy can be accessed via policy.policies[agents[1]])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Environment\n",
    "\n",
    "env = Environment.VehicleJobSchedulingEnvACE()\n",
    "env.reset()\n",
    "for agent in env.agent_iter(10000):\n",
    "    env.step(env.action_space(agent).sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 3000\n",
    "x = random.normal(key, (size, size), dtype=jnp.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.numpy' has no attribute 'random'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Yuming\\Documents\\ResMan\\testApi.ipynb 单元格 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Yuming/Documents/ResMan/testApi.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39menvironment\u001b[39;00m \u001b[39mimport\u001b[39;00m environment_jax\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Yuming/Documents/ResMan/testApi.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m env \u001b[39m=\u001b[39m environment_jax\u001b[39m.\u001b[39mVehicleJobSchedulingEnvACE()\n",
      "File \u001b[1;32mc:\\Users\\Yuming\\Documents\\ResMan\\environment\\environment_jax.py:473\u001b[0m\n\u001b[0;32m    469\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbacklog[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbacklog[\u001b[39m1\u001b[39m:]\n\u001b[0;32m    470\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbacklog[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m job\n\u001b[1;32m--> 473\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mJobRecord\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Yuming\\Documents\\ResMan\\environment\\environment_jax.py:477\u001b[0m, in \u001b[0;36mJobRecord\u001b[1;34m()\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 477\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_record\u001b[39m(\u001b[39mself\u001b[39m, job\u001b[39m=\u001b[39mJob()):\n\u001b[0;32m    478\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[39m    Purpose: add new record\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    481\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord[job\u001b[39m.\u001b[39mid] \u001b[39m=\u001b[39m job\n",
      "File \u001b[1;32mc:\\Users\\Yuming\\Documents\\ResMan\\environment\\environment_jax.py:145\u001b[0m, in \u001b[0;36mJob.__init__\u001b[1;34m(self, res_vec, job_len, job_id, enter_time, priority, average_cost_vec)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39m# self.job_vec = self.generate_job()  # 生成的任务向量\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maverage_cost_vec \u001b[39m=\u001b[39m average_cost_vec  \u001b[39m# 平均成本向量\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbudget \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculate_budget(average_cost_vec)  \u001b[39m# 计算的预算\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpay \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# 支付金额\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mutility \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Yuming\\Documents\\ResMan\\environment\\environment_jax.py:165\u001b[0m, in \u001b[0;36mJob.calculate_budget\u001b[1;34m(self, average_cost_vec, var)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_budget\u001b[39m(\u001b[39mself\u001b[39m, average_cost_vec, var\u001b[39m=\u001b[39m\u001b[39m0.03\u001b[39m):\n\u001b[0;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(\n\u001b[0;32m    162\u001b[0m         \u001b[39m0\u001b[39m,\n\u001b[0;32m    163\u001b[0m         np\u001b[39m.\u001b[39mdot(np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres_vec), np\u001b[39m.\u001b[39marray(average_cost_vec))\n\u001b[0;32m    164\u001b[0m         \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlen\n\u001b[1;32m--> 165\u001b[0m         \u001b[39m*\u001b[39m ((\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpriority \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m) \u001b[39m+\u001b[39m var \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39mnormal()),\n\u001b[0;32m    166\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Yuming\\Documents\\ResMan\\man\\lib\\site-packages\\jax\\_src\\deprecations.py:53\u001b[0m, in \u001b[0;36mdeprecation_getattr.<locals>.getattr\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     51\u001b[0m   warnings\u001b[39m.\u001b[39mwarn(message, \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     52\u001b[0m   \u001b[39mreturn\u001b[39;00m fn\n\u001b[1;32m---> 53\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'jax.numpy' has no attribute 'random'"
     ]
    }
   ],
   "source": [
    "from environment import environment_jax\n",
    "\n",
    "env = environment_jax.VehicleJobSchedulingEnvACE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Environment\n",
    "\n",
    "env = Environment.VehicleJobSchedulingEnvACE()\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.parameters.cluster.machines[1].observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration:  25  average_per_slot:  10\n",
      "Starting performance benchmark\n",
      "12679.175816391342 turns per second\n",
      "1056.5979846992784 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  25  average_per_slot:  30\n",
      "Starting performance benchmark\n",
      "12731.171350055083 turns per second\n",
      "1060.9309458379237 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  25  average_per_slot:  50\n",
      "Starting performance benchmark\n",
      "12692.710303681068 turns per second\n",
      "1057.725858640089 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  25  average_per_slot:  70\n",
      "Starting performance benchmark\n",
      "12447.760684349141 turns per second\n",
      "1037.3133903624284 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  25  average_per_slot:  90\n",
      "Starting performance benchmark\n",
      "12489.932789005727 turns per second\n",
      "1040.8277324171438 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  30  average_per_slot:  10\n",
      "Starting performance benchmark\n",
      "12766.805595751817 turns per second\n",
      "1063.9004663126514 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  30  average_per_slot:  30\n",
      "Starting performance benchmark\n",
      "12662.82735057095 turns per second\n",
      "1055.2356125475792 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  30  average_per_slot:  50\n",
      "Starting performance benchmark\n",
      "12595.536271335071 turns per second\n",
      "1049.628022611256 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  30  average_per_slot:  70\n",
      "Starting performance benchmark\n",
      "12495.82102308555 turns per second\n",
      "1041.3184185904624 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  30  average_per_slot:  90\n",
      "Starting performance benchmark\n",
      "12931.976991107913 turns per second\n",
      "1077.6647492589927 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  35  average_per_slot:  10\n",
      "Starting performance benchmark\n",
      "12884.796371381104 turns per second\n",
      "1073.7330309484253 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  35  average_per_slot:  30\n",
      "Starting performance benchmark\n",
      "12875.00360249619 turns per second\n",
      "1072.9169668746827 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  35  average_per_slot:  50\n",
      "Starting performance benchmark\n",
      "12687.333530403246 turns per second\n",
      "1057.2777942002706 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  35  average_per_slot:  70\n",
      "Starting performance benchmark\n",
      "12869.3117786244 turns per second\n",
      "1072.4426482187 cycles per second\n",
      "Finished performance benchmark\n",
      "duration:  35  average_per_slot:  90\n",
      "Starting performance benchmark\n",
      "12899.279734775095 turns per second\n",
      "1074.9399778979246 cycles per second\n",
      "Finished performance benchmark\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "from pettingzoo.test import performance_benchmark\n",
    "from environment import Environment\n",
    "for i in range(25,40,5):\n",
    "    for j in range(10,100,20):\n",
    "        print(\"duration: \", i, \" average_per_slot: \", j)\n",
    "        para = Environment.VehicleJobSchedulingParameters(duration=i,average_per_slot=j)\n",
    "        env = Environment.VehicleJobSchedulingEnvACE()\n",
    "        performance_benchmark(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Rate:  0.8738916256157635\n",
      "Finish Rate:  0.837245696400626\n",
      "Finish Rate:  0.7435723951285521\n",
      "Finish Rate:  0.6604767879548307\n",
      "Finish Rate:  0.5661592505854801\n",
      "Finish Rate:  0.5196389771017884\n",
      "Finish Rate:  0.4667235494880546\n",
      "Finish Rate:  0.43742098609355246\n",
      "Finish Rate:  0.3821138211382114\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "from tkinter import E\n",
    "from pettingzoo.test import performance_benchmark, api_test\n",
    "from environment import Environment\n",
    "\n",
    "for i in range(10,100,10):\n",
    "    para = Environment.VehicleJobSchedulingParameters(average_per_slot=i, duration=30)\n",
    "    env = Environment.VehicleJobSchedulingEnvACE(parameter=para)\n",
    "    env.reset()\n",
    "    for agent in env.agent_iter(100000):\n",
    "        env.step(env.action_space(agent).sample())   \n",
    "    print(\"Finish Rate: \",env.finished_job/env.total_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in env.agent_iter(10):\n",
    "    action = 1\n",
    "    env.step(action)\n",
    "    print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"env job finished rate\", env.finished_job/env.total_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Environment, AllocationMechanism, Machine\n",
    "\n",
    "env = Environment.VehicleJobSchedulingEnvACE()\n",
    "for job in env.get_job_next_step():\n",
    "    job1 = job\n",
    "    break\n",
    "cluster = env.parameters.cluster\n",
    "\n",
    "fp = AllocationMechanism.FirstPrice()\n",
    "bids = Machine.Bids(cluster,job1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids.request_bids()\n",
    "bids.get_bids()\n",
    "bids.bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.data import Collector\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.policy import MultiAgentPolicyManager, RandomPolicy\n",
    "\n",
    "import tianshou_setup\n",
    "\n",
    "env = tianshou_setup.get_env()\n",
    "policies = MultiAgentPolicyManager(\n",
    "    [\n",
    "        RandomPolicy(\n",
    "            observation_space=env.observation_space, action_space=env.action_space\n",
    "        )\n",
    "        for _ in range(len(env.agents))\n",
    "    ],\n",
    "    env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = RandomPolicy(\n",
    "            observation_space=env.observation_space, action_space=env.action_space\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from tianshou.data import Collector\n",
    "from tianshou.env import DummyVectorEnv, SubprocVectorEnv\n",
    "from tianshou.policy import MultiAgentPolicyManager\n",
    "\n",
    "import tianshou_setup\n",
    "from static_policy.TruthfulPolicy import TruthfulPolicy\n",
    "\n",
    "env = tianshou_setup.get_env_continous()\n",
    "\n",
    "\n",
    "def get_DummyVectorEnv_n(n):\n",
    "    return DummyVectorEnv([lambda: env for _ in range(n)])\n",
    "\n",
    "\n",
    "def get_SubprocVectorEnv_n(n):\n",
    "    return SubprocVectorEnv([lambda: env for _ in range(n)])\n",
    "\n",
    "\n",
    "def get_policy_manager():\n",
    "    env = tianshou_setup.get_env_continous()\n",
    "    policies = MultiAgentPolicyManager(\n",
    "        [\n",
    "            TruthfulPolicy(\n",
    "                observation_space=env.observation_space, action_space=env.action_space\n",
    "            )\n",
    "            for _ in range(len(env.agents))\n",
    "        ],\n",
    "        env,\n",
    "    )\n",
    "    return policies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "env = get_DummyVectorEnv_n(n)\n",
    "policies = get_policy_manager()\n",
    "collector = Collector(policies, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute one episode\n",
    "start = time.time()\n",
    "result = collector.collect(n_episode=10)\n",
    "end = time.time()\n",
    "print(f\"Time elapsed: {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = policies.policies['Machine_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d652fbd7feb28eaf2ff622dc57929f599600a55ad62e1efe6c9f098053cf5e75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
