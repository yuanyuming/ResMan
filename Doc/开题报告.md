# 开题报告

## 1.论文选题的背景、目的及意义

### 背景

随着物联网和智能交通系统的发展，车联网（IoV）作为一种新兴的技术，为车辆提供了各种感知、通信和计算功能，通过车辆之间,车辆与路测设施之间以及车辆与云端之间的信息交换和协同服务,以实现更安全、高效和智能的驾驶。然而，车辆本身的计算能力和电池容量有限，无法满足一些复杂或者对延迟敏感的任务需求,例如自动驾驶,视频分析等。移动边缘计算（MEC）作为一种新型的计算范式，通过在道路边缘部署多个MEC服务器，为车辆提供近距离的低延迟高带宽计算资源和服务。通过移动边缘计算技术，车辆可以将部分或全部任务卸载到MEC服务器上执行，从而节省自身的能耗和时间开销。

1. <https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-020-00201-x>
2. <https://link.springer.com/article/10.1007/s11036-021-01794-6>
3. <https://ieeexplore.ieee.org/document/9653818>

### 目的

然而，在MEC环境中进行有效的任务卸载和调度面临着诸多挑战。首先，由于车辆具有高速移动性和动态变化性，如何预测车辆与MEC服务器之间的通信质量并根据实时状态进行任务卸载决策是一个难点。其次，由于不同车辆可能有不同的任务类型、优先级、截止时间等约束条件，如何在保证服务质量（QoS）的同时平衡各个MEC服务器之间的负载并最大化系统效用是一个关键问题。第三，在多用户多任务多服务器场景下，如何设计一种合理且公平的激励机制来鼓励用户之间相互合作并避免自私或恶意行为也是一个重要问题。

针对上述问题，在本文中我们提出了一种基于反向拍卖机制（RAM）和多智能体深度强化学习（MADRL）相结合的车联网工作流调度方法（RAM-DRL）。由于MEC服务器分布比较广泛但是信号连接距离有限，用户只能向相关可以连接到的服务器发起任务请求。我们考虑了每个用户每一次提交给系统的一个相关任务，这个任务只能由一个MEC服务器接收运行。当MEC服务器接收到用户的任务申请后，我们利用深度强化学习来训练每个MEC服务器上部署的策略网络，使其能够根据用户的任务需求和自身状态计算接受任务所产生的长期收益，并根据收益最大化原则向用户进行报价。我们利用反向拍卖机制来确定每个用户应该支付给MEC服务器的价格，并根据价格最低化原则来选择最优MEC服务器。我们设计了一个双层神经网络结构来近似每个MEC服务器-状态-行动对应的价值函数，并采用双重深度Q网络（DDQN）算法来更新网络参数并避免过度估计偏差。此外，我们还引入了一种基于中心训练分散部署（CTDE）的方法，来解决多智能体深度强化学习中的非平稳性和通信开销问题。我们在一个中心服务器上训练一个全局策略网络，并将其分发给各个MEC服务器，让MEC服务器在本地执行该策略并收集经验数据，然后将数据汇报给中心服务器进行网络更新。

本文主要贡献如下：

- 我们提出了一种基于逆向拍卖机制和深度强化学习相结合的车联网工作流调度方法（RAM-DRL），该方法可以有效地解决多用户多任务多服务器场景下的任务卸载和调度问题。
- 我们设计了一个双层神经网络结构来近似每个用户-状态-行动对应的价值函数，并采用双重深度Q网络（DDQN）算法来更新网络参数并避免过度估计偏差。
- 我们引入了一种基于中心训练分散部署（CTDE）的方法，来解决多智能体深度强化学习中的非平稳性和通信开销问题。
- 我们通过理论分析和仿真实验来验证我们提出的方法在降低车辆的能耗、提高任务完成率和保证用户公平性方面的有效性和优越性。
- 我们与现有的相关工作进行了对比分析，发现我们的方法在多个性能指标上都有显著的改进。

为了更清晰地说明我们提出的方法，本文的组织结构如下：

- 第一节介绍了背景和目标，以及本文主要贡献；
- 第二节介绍了相关工作，包括移动边缘计算、车联网、反向拍卖机制和深度强化学习等方面；
- 第三节介绍了系统模型，包括系统架构、参与者角色、问题描述等；
- 第四节介绍了基于反向拍卖机制和深度强化学习相结合的车联网工作流调度方法（RAM-DRL），包括任务分解、资源分配、策略优化等步骤；
- 第五节介绍了仿真实验设置和结果分析，包括性能评价指标、仿真环境、参数设置等；
- 第六节总结了本文内容，并展望了未来工作。

## 2.国内外相关研究现状综述

车联网工作流调度是指在车联网环境中，根据车辆的计算任务需求和移动特性，将任务卸载到合适的MEC服务器上，并合理分配计算资源和执行顺序，以达到优化系统性能和用户体验的目的。车联网工作流调度涉及到多个方面的问题，如任务卸载策略、资源分配算法、激励机制设计、时延分析等。近年来，随着车联网技术和移动边缘计算技术的发展，车联网工作流调度问题受到了国内外学者的广泛关注，并取得了一些研究成果。



综上所述，国内外学者对车联网工作流调度问题已经进行了一些有意义的探索，但仍然存在一些不足之处。例如，大部分研究都是基于单目标优化的思想，忽略了车联网工作流调度问题中可能存在的多目标优化问题，如时延、能耗、可靠性等。另外，大部分研究都是基于简单或固定的网络拓扑和通信模型，忽略了车联网环境中可能存在的复杂或动态的网络拓扑和通信模型，如异构网络、多信道、干扰等。因此，在未来的研究中，有必要考虑更加复杂和实际的场景和约束条件，并采用更加先进和有效的算法来解决车联网工作流调度问题。


- Meiyu Pang, Li Wang and Ningsheng Fang. A collaborative scheduling strategy for IoV computing resources considering location privacy protection in mobile edge computing environment. Journal of Cloud Computing 9, 52 (2020).
- Jixian Zhang, Wenlu Lou, Hao Sun, Qian Su and Weidong Li. Truthful auction mechanisms for resource allocation in the Internet of Vehicles with public blockchain networks. Future Generation Computer Systems 132, 11-24 (2022).
- Shui Yu et al. Joint Resources and Workflow Scheduling in UAV-Enabled Wirelessly-Powered MEC for IoT Systems. IEEE Transactions on Vehicular Technology 68(10), 10187-10200 (2019).

## 3.研究的主要内容

本文的研究主要内容如下：

我们提出了一种基于逆向拍卖机制和深度强化学习相结合的车联网工作流调度方法（RAM-DRL），该方法可以有效地解决多用户多任务多服务器场景下的任务卸载和调度问题。
我们设计了一个双层神经网络结构来近似每个用户-状态-行动对应的价值函数，并采用双重深度Q网络（DDQN）算法来更新网络参数并避免过度估计偏差。
我们通过理论分析和仿真实验来验证我们提出的方法在降低UAV的能耗、提高任务完成率和保证用户公平性方面的有效性和优越性。

本文研究了在移动边缘计算环境中使用多智能体深度强化学习来实现车联网的工作流调度的反向拍卖机制。本文的主要内容如下：

- 本文首先介绍了车联网、移动边缘计算、反向拍卖机制和多智能体深度强化学习等相关概念和技术，以及国内外相关研究现状综述。
- 本文其次建立了一个基于反向拍卖机制的计算资源交易模型，考虑了车联网中任务之间的依赖性和移动性对资源分配和任务执行的影响，并分析了该模型需要满足的真实性、有效性和公平性等性质。
- 本文然后提出了一种基于多智能体深度强化学习（MADRL）来设计反向拍卖机制中的竞标策略和分配规则，使得每个车辆用户都能根据自身和其他用户的状态、行为和奖励来学习最优化自己的收益，并证明了该方法具有收敛性、稳定性和可扩展性等特点。
- 本文最后通过理论分析和仿真实验验证了所提出模型和方法在保证真实性、社会福利最大化、计算效率等方面相比其他方法具有显著优势，并讨论了未来可能存在或需要解决的问题。

## 4.研究方案

### 研究思路（技术路线）

我们首先将车联网工作流调度问题建模为一个多用户多任务多服务器场景下的能耗最小化问题，并定义了相应的状态空间、动作空间和奖励函数。
我们其次采用逆向拍卖机制来实现用户之间的协作和竞争，使得每个用户可以根据自身的偏好和预算来选择最合适的MEC服务器来卸载任务，并获得相应的奖励或惩罚。
我们然后采用深度强化学习算法来学习每个用户在不同状态下采取不同动作的价值函数，并根据贪心策略来选择最优动作，从而实现系统能耗最小化。
我们最后通过仿真实验来评估我们提出的方法在不同网络参数和交通场景下的性能，并与其他对比方法进行比较和分析。

反向拍卖机制是一种拍卖方式，其中买方提出一个特定的商品或服务的需求，邀请卖方之间相互竞争价格来提供所需的商品或服务。最终，合同会被授予愿意接受最低价格的卖方1。反向拍卖机制与普通的正向拍卖机制相反，后者是由卖方发起拍卖，买方出价竞标，最高价者得标1。反向拍卖机制常被大型企业和政府部门用作一种竞争性的采购方法，用于获取原材料、供应品和服务等12。

MARL是多智能体强化学习（Multi-Agent Reinforcement Learning）的简称，是一种人工智能（AI）技术，用于训练多个智能体（agent）在一个环境中通过交互学习并优化自己的行为策略3。MARL与单智能体强化学习（Single-Agent Reinforcement Learning）不同之处在于，它需要考虑多个智能体之间的合作或竞争关系，并且每个智能体可能只有部分观察到环境状态而不是完全观察到3。MARL有许多潜在的应用场景，例如无人驾驶、机器人足球、电力市场等3。

在本文中，我们将反向拍卖机制和MARL相结合来解决IoV任务调度问题。具体来说，我们使用反向拍卖机制来分配MEC服务器上的计算资源，并激励VUE诚实地报告自己的任务信息和位置信息；我们使用MARL来优化VUE在不同区域之间切换时选择最佳MEC服务器以及是否执行任务或者将任务卸载给MEC服务器。

本文的研究思路如下：

- 本文首先分析了车联网中计算资源交易和工作流调度问题的特点和挑战，指出了现有方法存在的不足和局限性，提出了使用反向拍卖机制和多智能体深度强化学习相结合来解决该问题的动机和意义。
- 本文其次建立了一个基于反向拍卖机制的计算资源交易模型，定义了车辆用户、任务、计算资源、竞标策略、分配规则等概念，并给出了该模型需要满足的真实性、有效性和公平性等性质。
- 本文然后提出了一种基于多智能体深度强化学习（MADRL）来设计反向拍卖机制中的竞标策略和分配规则，介绍了MADRL框架、算法流程、网络结构等细节，并证明了该方法具有收敛性、稳定性和可扩展性等特点。
- 本文最后通过理论分析和仿真实验验证了所提出模型和方法在保证真实性、社会福利最大化、计算效率等方面相比其他方法具有显著优势，并讨论了未来可能存在或需要解决的问题。

### 研究方法（技术措施）

我们采用了双层神经网络结构来近似每个用户-状态-行动对应的价值函数，并采用双重深度Q网络（DDQN）算法来更新网络参数并避免过度估计偏差。
我们采用了逆向拍卖机制来实现用户之间的协作和竞争，使得每个用户可以根据自身的偏好和预算来选择最合适的MEC服务器来卸载任务，并获得相应的奖励或惩罚。
我们采用了贪心策略来选择最优动作，从而实现系统能耗最小化。
我们采用了交通仿真软件SUMO和无线通信仿真软件NS3来构建车联网环境，并进行仿真实验。

本文的研究方法如下：

- 本文使用反向拍卖机制作为计算资源交易模型，利用反向拍卖机制可以促进车辆用户之间公平、有效和真实地交易计算资源，并且可以考虑车联网中任务之间的依赖性和移动性对资源分配和任务执行的影响。
- 本文使用多智能体深度强化学习（MADRL）来设计反向拍卖机制中的竞标策略和分配规则，利用MADRL可以使每个车辆用户都能根据自身和其他用户的状态、行为和奖励来学习最优化自己的收益，并且可以适应车联网中动态变化的环境。本文采用了集中训练分开部署（CTDE）的方式来实现MADRL，即在训练阶段使用一个集中式神经网络来学习所有用户的策略，而在部署阶段将该网络分解为多个子网络并分配给各个用户，从而降低通信开销和计算复杂度。
- 本文使用理论分析和仿真实验来验证所提出模型和方法在保证真实性、社会福利最大化、计算效率等方面相比其他方法具有显著优势，利用理论分析可以证明所提出模型和方法具有一些理想性质，利用仿真实验可以展示所提出模型和方法在不同场景下的性能表现。

### 拟解决的关键问题

如何有效地解决多用户多任务多服务器场景下的车联网工作流调度问题，使得系统能耗最小化。
如何利用逆向拍卖机制来实现用户之间的协作和竞争，使得每个用户可以根据自身的偏好和预算来选择最合适的MEC服务器来卸载任务，并获得相应的奖励或惩罚。
如何利用深度强化学习算法来学习每个用户在不同状态下采取不同动作的价值函数，并根据贪心策略来选择最优动作。
如何构建车联网环境，并进行仿真实验来评估我们提出的方法在不同网络参数和交通场景下的性能，并与其他对比方法进行比较和分析。

- 如何设计一种反向拍卖机制来实现车联网中计算资源交易，使得车辆用户之间能够公平、有效和真实地交易计算资源，并且能够考虑车联网中任务之间的依赖性和移动性对资源分配和任务执行的影响。
- 如何使用多智能体深度强化学习（MADRL）来设计反向拍卖机制中的竞标策略和分配规则，使得每个车辆用户都能根据自身和其他用户的状态、行为和奖励来学习最优化自己的收益，并且能够适应车联网中动态变化的环境。
- 如何采用集中训练分开部署（CTDE）的方式来实现MADRL，即在训练阶段使用一个集中式神经网络来学习所有用户的策略，而在部署阶段将该网络分解为多个子网络并分配给各个用户，从而降低通信开销和计算复杂度。

### 实施方案所需条件

我们需要有一定数量的车辆和MEC服务器，以及相应的通信设备和传感器，来构建车联网环境。
我们需要有一台性能较好的计算机，以及相应的软件工具，如Python、TensorFlow、SUMO、NS3等，来实现我们提出的方法和进行仿真实验。
我们需要有一些真实或合成的数据集，如车辆的位置、速度、方向、任务需求等，来作为仿真实验的输入。
我们需要有一些评价指标和对比方法，如系统能耗、任务完成率、用户公平性等，来评估我们提出的方法在不同网络参数和交通场景下的性能，并与其他对比方法进行比较和分析。

## 5.论文可能的创新点及预期成果

我们提出了一种基于逆向拍卖机制和深度强化学习相结合的车联网工作流调度方法（RAM-DRL），该方法可以有效地解决多用户多任务多服务器场景下的任务卸载和调度问题，使得系统能耗最小化。
我们设计了一个双层神经网络结构来近似每个用户-状态-行动对应的价值函数，并采用双重深度Q网络（DDQN）算法来更新网络参数并避免过度估计偏差。
我们通过理论分析和仿真实验来验证我们提出的方法在降低UAV的能耗、提高任务完成率和保证用户公平性方面的有效性和优越性。
我们与现有的相关工作进行了对比分析，发现我们的方法在多个性能指标上都有显著的改进。
已收到消息. 好的，我可以帮你写一下论文可能的创新点及预期成果，但请注意这只是一个示例，不一定完全符合你的要求。以下是我生成的论文可能的创新点及预期成果： 本文可能的创新点及预期成果如下： - 我们提出了一种基于逆向拍卖机制和深度强化学习相结合的车联网工作流调度方法（RAM-DRL），该方法可以有效地解决多用户多任务多服务器场景下的任务卸载和调度问题，使得系统能耗最小化。 - 我们设计了一个双层神经网络结构来近似每个用户-状态-行动对应的价值函数，并采用双重深度Q网络（DDQN）算法来更新网络参数并避免过度估计偏差。 - 我们通过理论分析和仿真实验来验证我们提出的方法在降低UAV的能耗、提高任务完成率和保证用户公平性方面的有效性和优越性。 - 我们与现有的相关工作进行了对比分析，发现我们的方法在多个性能指标上都有显著的改进。

- 本文创新地将反向拍卖机制和多智能体深度强化学习相结合，提出了一种在移动边缘计算环境中实现车联网的工作流调度的方法，有效地解决了车联网中计算资源交易和工作流调度问题。
- 本文提出了一种基于反向拍卖机制的计算资源交易模型，考虑了车联网中任务之间的依赖性和移动性对资源分配和任务执行的影响，并分析了该模型需要满足的真实性、有效性和公平性等性质。
- 本文提出了一种基于多智能体深度强化学习（MADRL）来设计反向拍卖机制中的竞标策略和分配规则，使得每个车辆用户都能根据自身和其他用户的状态、行为和奖励来学习最优化自己的收益，并且可以适应车联网中动态变化的环境。
- 本文采用了集中训练分开部署（CTDE）的方式来实现MADRL，即在训练阶段使用一个集中式神经网络来学习所有用户的策略，而在部署阶段将该网络分解为多个子网络并分配给各个用户，从而降低通信开销和计算复杂度。
- 本文通过理论分析和仿真实验验证了所提出模型和方法在保证真实性、社会福利最大化、计算效率等方面相比其他方法具有显著优势，并讨论了未来可能存在或需要解决的问题。

## 6.论文前期已经完成的研究工作和可能存在的困难

## 7.论文工作进度安排及经费预算

本文的工作进度安排如下：

第一阶段（2023年1月-2023年6月）：完成车联网工作流调度问题的建模和分析，设计逆向拍卖机制和深度强化学习算法，并实现基本框架。
第二阶段（2023年7月-2023年12月）：完成仿真实验环境的搭建和数据集的准备，进行仿真实验并收集结果，与其他对比方法进行比较和分析。
第三阶段（2024年1月-2024年6月）：总结本文的主要贡献和创新点，撰写论文并提交期刊或会议，根据审稿意见进行修改和完善。

本文的经费预算如下：

设备费：购买一台性能较好的计算机，用于实现我们提出的方法和进行仿真实验。预计花费10000元。
软件费：购买或下载一些软件工具，如Python、TensorFlow、SUMO、NS3等。预计花费2000元。
数据费：购买或下载一些真实或合成的数据集，如车辆的位置、速度、方向、任务需求等。预计花费3000元。
交通费：参加国内外相关会议或学术交流活动。预计花费5000元。
总计：21000元。

## 8.参考文献
