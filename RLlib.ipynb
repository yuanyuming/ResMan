{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import torch\n",
    "from rllib_setup import get_env_continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env_continuous()\n",
    "env.action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"VJS\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.rllib.utils.check_env(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import register_env\n",
    "register_env(env_name,lambda config: get_env())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import PolicySpec\n",
    "test_env = get_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "def policies(agent_ids):\n",
    "    return {\n",
    "        str(i): PolicySpec(\n",
    "            # observation_space=obs_space,\n",
    "            # action_space=act_space,\n",
    "            config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = test_env.action_space_sample()\n",
    "test_env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env.action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import get_trainable_cls\n",
    "import os\n",
    "config = (\n",
    "    get_trainable_cls(\"MADDPG\")\n",
    "    .get_default_config()\n",
    "    .environment(env=env_name)\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    ")\n",
    "(\n",
    "    config.framework(\"tf\")\n",
    "    # .environment(env_config={\"actions_are_logits\": True})\n",
    "    .training(num_steps_sampled_before_learning_starts=100)\n",
    "    .multi_agent(\n",
    "        policies=policies(test_env._agent_ids),\n",
    "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: str(\n",
    "            agent_id\n",
    "        ),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import air, tune\n",
    "stop = {\n",
    "    \"training_iteration\": 10000,\n",
    "}\n",
    "results = tune.Tuner(\n",
    "    \"MADDPG\",\n",
    "    run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "    param_space=config,\n",
    ").fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ddpg.ddpg import DDPGConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune import register_env\n",
    "import rich  \n",
    "import rllib_setup\n",
    "\n",
    "env_name = \"VJS\"\n",
    "register_env(\n",
    "    env_name,\n",
    "    lambda config: rllib_setup.get_env_continuous(),\n",
    ")\n",
    "test_env = rllib_setup.get_env_continuous()\n",
    "\n",
    "\n",
    "def policies(agent_ids):\n",
    "    obs_space = test_env.observation_space\n",
    "    act_space = test_env.action_space\n",
    "    return {\n",
    "        str(i): PolicySpec(\n",
    "            observation_space=obs_space,\n",
    "            action_space=act_space,\n",
    "            # config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    DDPGConfig()\n",
    "    .training(lr=0.01)\n",
    "    .resources(num_gpus=1)\n",
    "    .multi_agent(\n",
    "        policies=policies(test_env._agent_ids),\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: str(agent_id),\n",
    "    )\n",
    ")\n",
    "# config.batch_mode = \"complete_episodes\"\n",
    "rich.print(config.to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build(env=env_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray import air\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.Tuner(  \n",
    "    \"DDPG\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.sac import SACConfig\n",
    "from ray.tune import register_env\n",
    "import rllib_setup\n",
    "\n",
    "env_name = \"VJS\"\n",
    "register_env(\n",
    "    env_name,\n",
    "    lambda config: rllib_setup.get_env_continuous(),\n",
    ")\n",
    "test_env = rllib_setup.get_env_continuous()\n",
    "\n",
    "\n",
    "def policies(agent_ids):\n",
    "    obs_space = test_env.observation_space\n",
    "    act_space = test_env.action_space\n",
    "    return {\n",
    "        str(i): (\n",
    "            None,\n",
    "            obs_space,\n",
    "            act_space,\n",
    "            {}\n",
    "            # config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }\n",
    "    \n",
    "    \n",
    "config = SACConfig().training(gamma=0.9, lr=0.01)\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=4).multi_agent(\n",
    "        policies=policies(test_env._agent_ids),\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: str(agent_id),\n",
    "    )\n",
    "config.batch_mode = \"complete_episodes\"\n",
    "print(config.to_dict())  \n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=env_name)  \n",
    "algo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.maddpg.maddpg import MADDPGConfig\n",
    "config = MADDPGConfig()\n",
    "print(config.replay_buffer_config)  \n",
    "replay_config = config.replay_buffer_config.update(  \n",
    "    {\n",
    "        \"capacity\": 100000,\n",
    "        \"prioritized_replay_alpha\": 0.8,\n",
    "        \"prioritized_replay_beta\": 0.45,\n",
    "        \"prioritized_replay_eps\": 2e-6,\n",
    "    }\n",
    ")\n",
    "config.training(replay_buffer_config=replay_config)   \n",
    "config = config.resources(num_gpus=0)   \n",
    "config = config.rollouts(num_rollout_workers=4)   \n",
    "config = config.environment(env=env_name)  \n",
    "algo = config.build()  \n",
    "algo.train()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 13:47:45,193\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:442: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-09-26 13:47:46,516\tINFO algorithm.py:536 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': None, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'is_atari': None, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'complete_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.01, 'lr_schedule': None, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 256, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'OrnsteinUhlenbeckNoise', 'random_timesteps': 1000, 'ou_base_scale': 0.1, 'ou_theta': 0.15, 'ou_sigma': 0.2, 'initial_scale': 1.0, 'final_scale': 0.02, 'scale_timesteps': 10000}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 1000, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'target_network_update_freq': 0, 'replay_buffer_config': {'type': 'MultiAgentPrioritizedReplayBuffer', 'capacity': 50000, 'prioritized_replay': -1, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'worker_side_prioritization': False}, 'num_steps_sampled_before_learning_starts': 1500, 'store_buffer_in_checkpoints': False, 'adam_epsilon': 1e-08, 'tau': 0.002, 'twin_q': False, 'policy_delay': 1, 'smooth_target_policy': False, 'target_noise': 0.2, 'target_noise_clip': 0.5, 'use_state_preprocessor': False, 'actor_hiddens': [400, 300], 'actor_hidden_activation': 'relu', 'critic_hiddens': [400, 300], 'critic_hidden_activation': 'relu', 'n_step': 1, 'training_intensity': None, 'critic_lr': 0.001, 'actor_lr': 0.001, 'use_huber': False, 'huber_threshold': 1.0, 'l2_reg': 1e-06, 'worker_side_prioritization': -1, 'input': 'sampler', 'multiagent': {'policies': {'Machine_0': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_1': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_2': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_3': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_4': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_5': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_6': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_7': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_8': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_9': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_10': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {}), 'Machine_11': (None, Box(0.0, [4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03\n",
      " 4.097e+03 4.097e+03 4.097e+03 4.097e+03 4.097e+03 1.000e+02 1.000e+02\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00], (73,), float64), Box(0.5, 2.0, (1,), float32), {})}, 'policy_mapping_fn': <function <lambda> at 0x7f636c444790>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-26 13:47:57,703\tINFO trainable.py:173 -- Trainable.setup took 11.189 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2925449)\u001b[0m 2023-09-26 13:48:41,962\tWARNING env_runner_v2.py:154 -- More than 22256 observations in 22256 env steps for episode 692983022856277297 are buffered in the sampler. If this is more than you expected, check that that you set a horizon on your environment correctly and that it terminates at some point. Note: In multi-agent environments, `rollout_fragment_length` sets the batch size based on (across-agents) environment steps, not the steps of individual agents, which can result in unexpectedly large batches.Also, you may be waiting for your Env to terminate (batch_mode=`complete_episodes`). Make sure it does at some point.\n",
      "2023-09-26 13:48:42,895\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate w/o an evaluation worker set in the Trainer or w/o an env on the local worker!\nTry one of the following:\n1) Set `evaluation_interval` >= 0 to force creating a separate evaluation worker set.\n2) Set `create_env_on_driver=True` to force the local (non-eval) worker to have an environment to evaluate on.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/yuan/ResMan/RLlib.ipynb 单元格 22\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m algo \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mbuild(env\u001b[39m=\u001b[39menv_name)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m algo\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgtx6180/home/yuan/ResMan/RLlib.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m algo\u001b[39m.\u001b[39;49mevaluate()\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:911\u001b[0m, in \u001b[0;36mAlgorithm.evaluate\u001b[0;34m(self, duration_fn)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    907\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    908\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_workers \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    909\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mlocal_worker()\u001b[39m.\u001b[39minput_reader \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     ):\n\u001b[0;32m--> 911\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    912\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot evaluate w/o an evaluation worker set in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe Trainer or w/o an env on the local worker!\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTry one of the following:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m1) Set \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    915\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`evaluation_interval` >= 0 to force creating a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mseparate evaluation worker set.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m2) Set \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`create_env_on_driver=True` to force the local \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    918\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(non-eval) worker to have an environment to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    919\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mevaluate on.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    920\u001b[0m         )\n\u001b[1;32m    922\u001b[0m     \u001b[39m# How many episodes/timesteps do we need to run?\u001b[39;00m\n\u001b[1;32m    923\u001b[0m     \u001b[39m# In \"auto\" mode (only for parallel eval + training): Run as long\u001b[39;00m\n\u001b[1;32m    924\u001b[0m     \u001b[39m# as training lasts.\u001b[39;00m\n\u001b[1;32m    925\u001b[0m     unit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mevaluation_duration_unit\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot evaluate w/o an evaluation worker set in the Trainer or w/o an env on the local worker!\nTry one of the following:\n1) Set `evaluation_interval` >= 0 to force creating a separate evaluation worker set.\n2) Set `create_env_on_driver=True` to force the local (non-eval) worker to have an environment to evaluate on."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-26 16:28:45,617 E 2922067 2922067] (raylet) node_manager.cc:3069: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 1f6ea0053076213805ba22f91ea908c4d3938e814201bed0f037e38d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-26 16:29:45,619 E 2922067 2922067] (raylet) node_manager.cc:3069: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 1f6ea0053076213805ba22f91ea908c4d3938e814201bed0f037e38d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-26 16:30:45,621 E 2922067 2922067] (raylet) node_manager.cc:3069: 26 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 1f6ea0053076213805ba22f91ea908c4d3938e814201bed0f037e38d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-26 16:31:46,474 E 2922067 2922067] (raylet) node_manager.cc:3069: 47 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 1f6ea0053076213805ba22f91ea908c4d3938e814201bed0f037e38d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-26 16:32:46,769 E 2922067 2922067] (raylet) node_manager.cc:3069: 49 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 1f6ea0053076213805ba22f91ea908c4d3938e814201bed0f037e38d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-26 16:33:47,442 E 2922067 2922067] (raylet) node_manager.cc:3069: 55 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 1f6ea0053076213805ba22f91ea908c4d3938e814201bed0f037e38d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-26 16:34:47,904 E 2922067 2922067] (raylet) node_manager.cc:3069: 56 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 1f6ea0053076213805ba22f91ea908c4d3938e814201bed0f037e38d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2023-09-26 16:34:57,723\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffcb2bc4defb8d901d0f8ff65d01000000 Worker ID: e75bc3ee4a70fcd1d5948fc6bc4c9a6458d600625a51715278ce6322 Node ID: 1f6ea0053076213805ba22f91ea908c4d3938e814201bed0f037e38d Worker IP address: 192.168.3.6 Worker port: 37755 Worker PID: 2925463 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-26 16:34:57,843\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffaa0432450dc98bc4c64e231001000000 Worker ID: 49aa84373235fcef6d65aaeabbc7f58f4dea1dd4ee22b8954993ae09 Node ID: 1f6ea0053076213805ba22f91ea908c4d3938e814201bed0f037e38d Worker IP address: 192.168.3.6 Worker port: 45557 Worker PID: 2925452 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2023-09-26 16:34:57,845\tWARNING worker.py:2019 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff112f6a24d0c844dbe14f206501000000 Worker ID: 0ec8f533058c7c4f21c7870f151a63e00e31f63441e5e549f1479083 Node ID: 1f6ea0053076213805ba22f91ea908c4d3938e814201bed0f037e38d Worker IP address: 192.168.3.6 Worker port: 33411 Worker PID: 2925456 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-26 16:35:49,619 E 2922067 2922067] (raylet) node_manager.cc:3069: 29 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 1f6ea0053076213805ba22f91ea908c4d3938e814201bed0f037e38d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-09-26 16:36:56,545 E 2922067 2922067] (raylet) node_manager.cc:3069: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 1f6ea0053076213805ba22f91ea908c4d3938e814201bed0f037e38d, IP: 192.168.3.6) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.3.6`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ddpg.ddpg import DDPGConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune import register_env\n",
    "\n",
    "import rllib_setup\n",
    "\n",
    "ray.init()\n",
    "env_name = \"VJS\"\n",
    "register_env(\n",
    "    env_name,\n",
    "    lambda config: rllib_setup.get_env_continuous(60),\n",
    ")\n",
    "test_env = rllib_setup.get_env_continuous()\n",
    "\n",
    "\n",
    "def policies(agent_ids):\n",
    "    obs_space = test_env.observation_space\n",
    "    act_space = test_env.action_space\n",
    "    return {\n",
    "        str(i): (\n",
    "            None,\n",
    "            obs_space,\n",
    "            act_space,\n",
    "            {}\n",
    "            # config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }\n",
    "\n",
    "\n",
    "config = (\n",
    "    DDPGConfig().rollouts(num_rollout_workers=20)\n",
    "    .training(lr=0.01)\n",
    "    .resources(num_gpus=1)\n",
    "    .multi_agent(\n",
    "        policies=policies(test_env._agent_ids),\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: str(agent_id),\n",
    "    )\n",
    "    .environment(disable_env_checking=True)\n",
    ")\n",
    "config.batch_mode = \"complete_episodes\"\n",
    "print(config.to_dict())\n",
    "# Build a Algorithm object from the config and run one training iteration.\n",
    "algo = config.build(env=env_name)\n",
    "algo.train()\n",
    "algo.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(algo.training_step()['Machine_0']['learner_stats']['actor_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
