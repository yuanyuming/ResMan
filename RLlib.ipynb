{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import torch\n",
    "from rllib_setup import get_env_continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 18:04:13,994\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.9.13</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.5.1</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.13', ray_version='2.5.1', ray_commit='a03efd9931128d387649dd48b0e4864b43d3bfb4', address_info={'node_ip_address': '192.168.3.6', 'raylet_ip_address': '192.168.3.6', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-07-18_18-04-11_902018_1852758/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-07-18_18-04-11_902018_1852758/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2023-07-18_18-04-11_902018_1852758', 'metrics_export_port': 55362, 'gcs_address': '192.168.3.6:48962', 'address': '192.168.3.6:48962', 'dashboard_agent_listen_port': 52365, 'node_id': 'f5886bd755e5f6d67c89ded5d46264979fbdd2f442d63885865fc3d6'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env_continuous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"VJS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.rllib.utils.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import register_env\n",
    "register_env(env_name,lambda config: get_env_continuous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import PolicySpec\n",
    "test_env = get_env_continuous()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "def policies(agent_ids):\n",
    "    return {\n",
    "        str(i): PolicySpec(\n",
    "            observation_space=obs_space,\n",
    "            action_space=act_space,\n",
    "            config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'MultiAgentReplayBuffer', 'prioritized_replay': -1, 'capacity': 1000000, 'replay_mode': 'lockstep'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858872)\u001b[0m /home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py:357: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858872)\u001b[0m   out = tf1.layers.dense(out, units=hidden, activation=activation)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858872)\u001b[0m /home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py:359: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858872)\u001b[0m   out = tf1.layers.dense(feature, units=1, activation=None)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858048)\u001b[0m 2023-07-18 18:06:37,907\tERROR worker.py:861 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1858048, ip=192.168.3.6, actor_id=b29ab9965315be97787dfc7901000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f4b89de4790>)\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858048)\u001b[0m   File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 54, in __init__\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858048)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858048)\u001b[0m   File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1985, in _update_policy_map\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858048)\u001b[0m     self._build_policy_map(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858048)\u001b[0m   File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 2097, in _build_policy_map\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858048)\u001b[0m     new_policy = create_policy_for_framework(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858048)\u001b[0m   File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 142, in create_policy_for_framework\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858048)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858048)\u001b[0m     raise ValueError(\"Must set `agent_id` in the policy config.\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858048)\u001b[0m ValueError: Must set `agent_id` in the policy config.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858872)\u001b[0m   feature = tf1.layers.dense(out, units=act_space.shape[0], activation=None)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858871)\u001b[0m 2023-07-18 18:12:19,357\tWARNING algorithm_config.py:643 -- Cannot create MADDPGConfig from given `config_dict`! Property before_learn_on_batch not supported.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858872)\u001b[0m     assert sess\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1858872)\u001b[0m AssertionError\n",
      "2023-07-18 18:12:19,977\tERROR actor_manager.py:507 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1858871, ip=192.168.3.6, actor_id=c435f8f15763817a536e48a901000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fc02629c790>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 738, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1985, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 2097, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 142, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 238, in __init__\n",
      "    assert sess\n",
      "AssertionError\n",
      "2023-07-18 18:12:19,979\tERROR actor_manager.py:507 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1858872, ip=192.168.3.6, actor_id=1495b55126035cb16c9158b001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f9195c5e7f0>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 738, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1985, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 2097, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 142, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 238, in __init__\n",
      "    assert sess\n",
      "AssertionError\n",
      "2023-07-18 18:12:19,982\tERROR actor_manager.py:507 -- Ray error, taking actor 3 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1858873, ip=192.168.3.6, actor_id=280decf465fe1df81555241001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f2144cd5730>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 738, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1985, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 2097, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 142, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 238, in __init__\n",
      "    assert sess\n",
      "AssertionError\n",
      "2023-07-18 18:12:19,985\tERROR actor_manager.py:507 -- Ray error, taking actor 4 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1858874, ip=192.168.3.6, actor_id=29432530c4c613768886228801000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f8c59be5730>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 738, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1985, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 2097, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 142, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 238, in __init__\n",
      "    assert sess\n",
      "AssertionError\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39menvironment(env_name)\n\u001b[1;32m     16\u001b[0m config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mmulti_agent(policies\u001b[39m=\u001b[39mpolicies(test_env\u001b[39m.\u001b[39m_agent_ids),policy_mapping_fn\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m agent_id, episode, worker, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: \u001b[39mstr\u001b[39m(agent_id))\n\u001b[0;32m---> 17\u001b[0m algo \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39;49mbuild()  \n\u001b[1;32m     18\u001b[0m algo\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm_config.py:1071\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1069\u001b[0m     algo_class \u001b[39m=\u001b[39m get_trainable_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class)\n\u001b[0;32m-> 1071\u001b[0m \u001b[39mreturn\u001b[39;00m algo_class(\n\u001b[1;32m   1072\u001b[0m     config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m use_copy \u001b[39melse\u001b[39;49;00m copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m),\n\u001b[1;32m   1073\u001b[0m     logger_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger_creator,\n\u001b[1;32m   1074\u001b[0m )\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:475\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    462\u001b[0m     \u001b[39m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m     },\n\u001b[1;32m    473\u001b[0m }\n\u001b[0;32m--> 475\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    476\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    477\u001b[0m     logger_creator\u001b[39m=\u001b[39;49mlogger_creator,\n\u001b[1;32m    478\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    479\u001b[0m )\n\u001b[1;32m    481\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:170\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout, sync_config)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mget_node_ip_address()\n\u001b[0;32m--> 170\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    171\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:601\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[39mif\u001b[39;00m _init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    596\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[39m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[39m#   in each training iteration.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    600\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    602\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    603\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    604\u001b[0m         default_policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    605\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    606\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rollout_workers,\n\u001b[1;32m    607\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    608\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    611\u001b[0m     \u001b[39m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[1;32m    612\u001b[0m     \u001b[39m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    613\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m    614\u001b[0m         \u001b[39m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:194\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mexcept\u001b[39;00m RayActorError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    182\u001b[0m     \u001b[39m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[39m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[39m# errors.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mactor_init_failed:\n\u001b[1;32m    187\u001b[0m         \u001b[39m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[1;32m    188\u001b[0m         \u001b[39m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[39m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         \u001b[39m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m         \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m2\u001b[39m]\n\u001b[1;32m    195\u001b[0m     \u001b[39m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1431\u001b[0m, in \u001b[0;36mray._raylet.execute_task\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1510\u001b[0m, in \u001b[0;36mray._raylet.execute_task\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1434\u001b[0m, in \u001b[0;36mray._raylet.execute_task\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1438\u001b[0m, in \u001b[0;36mray._raylet.execute_task\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1378\u001b[0m, in \u001b[0;36mray._raylet.execute_task.function_executor\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/_private/function_manager.py:724\u001b[0m, in \u001b[0;36mactor_method_executor\u001b[0;34m()\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    723\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 724\u001b[0m     \u001b[39mreturn\u001b[39;00m method(__ray_actor, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py:464\u001b[0m, in \u001b[0;36m_resume_span\u001b[0;34m()\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39m# If tracing feature flag is not on, perform a no-op\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_tracing_enabled() \u001b[39mor\u001b[39;00m _ray_trace_ctx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[1;32m    466\u001b[0m tracer: _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mTracer \u001b[39m=\u001b[39m _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mget_tracer(\n\u001b[1;32m    467\u001b[0m     \u001b[39m__name__\u001b[39m\n\u001b[1;32m    468\u001b[0m )\n\u001b[1;32m    470\u001b[0m \u001b[39m# Retrieves the context from the _ray_trace_ctx dictionary we\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# injected.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:738\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m# if RLModule API is enabled, marl_module_spec holds the specs of the RLModules\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmarl_module_spec \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 738\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_policy_map(policy_dict\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_dict)\n\u001b[1;32m    740\u001b[0m \u001b[39m# Update Policy's view requirements from Model, only if Policy directly\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39m# inherited from base `Policy` class. At this point here, the Policy\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[39m# must have it's Model (if any) defined and ready to output an initial\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39m# state.\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[39mfor\u001b[39;00m pol \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_map\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py:464\u001b[0m, in \u001b[0;36m_resume_span\u001b[0;34m()\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39m# If tracing feature flag is not on, perform a no-op\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_tracing_enabled() \u001b[39mor\u001b[39;00m _ray_trace_ctx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[1;32m    466\u001b[0m tracer: _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mTracer \u001b[39m=\u001b[39m _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mget_tracer(\n\u001b[1;32m    467\u001b[0m     \u001b[39m__name__\u001b[39m\n\u001b[1;32m    468\u001b[0m )\n\u001b[1;32m    470\u001b[0m \u001b[39m# Retrieves the context from the _ray_trace_ctx dictionary we\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# injected.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:1985\u001b[0m, in \u001b[0;36m_update_policy_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1980\u001b[0m     updated_policy_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_policy_dict_with_marl_module(\n\u001b[1;32m   1981\u001b[0m         updated_policy_dict\n\u001b[1;32m   1982\u001b[0m     )\n\u001b[1;32m   1984\u001b[0m \u001b[39m# Builds the self.policy_map dict\u001b[39;00m\n\u001b[0;32m-> 1985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_policy_map(\n\u001b[1;32m   1986\u001b[0m     policy_dict\u001b[39m=\u001b[39mupdated_policy_dict,\n\u001b[1;32m   1987\u001b[0m     policy\u001b[39m=\u001b[39mpolicy,\n\u001b[1;32m   1988\u001b[0m     policy_states\u001b[39m=\u001b[39mpolicy_states,\n\u001b[1;32m   1989\u001b[0m )\n\u001b[1;32m   1991\u001b[0m \u001b[39m# Initialize the filter dict\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_filter_dict(updated_policy_dict)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py:464\u001b[0m, in \u001b[0;36m_resume_span\u001b[0;34m()\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39m# If tracing feature flag is not on, perform a no-op\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_tracing_enabled() \u001b[39mor\u001b[39;00m _ray_trace_ctx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[1;32m    466\u001b[0m tracer: _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mTracer \u001b[39m=\u001b[39m _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mget_tracer(\n\u001b[1;32m    467\u001b[0m     \u001b[39m__name__\u001b[39m\n\u001b[1;32m    468\u001b[0m )\n\u001b[1;32m    470\u001b[0m \u001b[39m# Retrieves the context from the _ray_trace_ctx dictionary we\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# injected.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:2097\u001b[0m, in \u001b[0;36m_build_policy_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[39mfor\u001b[39;00m name, policy_spec \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(policy_dict\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m   2094\u001b[0m \n\u001b[1;32m   2095\u001b[0m     \u001b[39m# Create the actual policy object.\u001b[39;00m\n\u001b[1;32m   2096\u001b[0m     \u001b[39mif\u001b[39;00m policy \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2097\u001b[0m         new_policy \u001b[39m=\u001b[39m create_policy_for_framework(\n\u001b[1;32m   2098\u001b[0m             policy_id\u001b[39m=\u001b[39mname,\n\u001b[1;32m   2099\u001b[0m             policy_class\u001b[39m=\u001b[39mget_tf_eager_cls_if_necessary(\n\u001b[1;32m   2100\u001b[0m                 policy_spec\u001b[39m.\u001b[39mpolicy_class, policy_spec\u001b[39m.\u001b[39mconfig\n\u001b[1;32m   2101\u001b[0m             ),\n\u001b[1;32m   2102\u001b[0m             merged_config\u001b[39m=\u001b[39mpolicy_spec\u001b[39m.\u001b[39mconfig,\n\u001b[1;32m   2103\u001b[0m             observation_space\u001b[39m=\u001b[39mpolicy_spec\u001b[39m.\u001b[39mobservation_space,\n\u001b[1;32m   2104\u001b[0m             action_space\u001b[39m=\u001b[39mpolicy_spec\u001b[39m.\u001b[39maction_space,\n\u001b[1;32m   2105\u001b[0m             worker_index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworker_index,\n\u001b[1;32m   2106\u001b[0m             seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed,\n\u001b[1;32m   2107\u001b[0m         )\n\u001b[1;32m   2108\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2109\u001b[0m         new_policy \u001b[39m=\u001b[39m policy\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/policy.py:142\u001b[0m, in \u001b[0;36mcreate_policy_for_framework\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[39mreturn\u001b[39;00m policy_class(observation_space, action_space, merged_config)\n\u001b[1;32m    140\u001b[0m \u001b[39m# Non-tf: No graph, no session.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m policy_class(observation_space, action_space, merged_config)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py:238\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39m# _____ TensorFlow Initialization\u001b[39;00m\n\u001b[1;32m    237\u001b[0m sess \u001b[39m=\u001b[39m tf1\u001b[39m.\u001b[39mget_default_session()\n\u001b[0;32m--> 238\u001b[0m \u001b[39massert\u001b[39;00m sess\n\u001b[1;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_loss_inputs\u001b[39m(placeholders):\n\u001b[1;32m    241\u001b[0m     \u001b[39mreturn\u001b[39;00m [(ph\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m], ph) \u001b[39mfor\u001b[39;00m ph \u001b[39min\u001b[39;00m placeholders]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.maddpg.maddpg import MADDPGConfig\n",
    "config = MADDPGConfig()\n",
    "print(config.replay_buffer_config)  \n",
    "replay_config = config.replay_buffer_config.update(  \n",
    "    {\n",
    "        \"capacity\": 100000,\n",
    "        \"prioritized_replay_alpha\": 0.8,\n",
    "        \"prioritized_replay_beta\": 0.45,\n",
    "        \"prioritized_replay_eps\": 2e-6,\n",
    "    }\n",
    ")\n",
    "config.training(replay_buffer_config=replay_config)   \n",
    "config = config.resources(num_gpus=0)   \n",
    "config = config.rollouts(num_rollout_workers=4)   \n",
    "config = config.environment(env_name)\n",
    "config = config.multi_agent(policies=policies(test_env._agent_ids),policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: str(agent_id))\n",
    "algo = config.build()  \n",
    "algo.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
