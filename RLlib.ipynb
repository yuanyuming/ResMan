{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import torch\n",
    "from rllib_setup import get_env_continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env_continuous()\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"VJS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.rllib.utils.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import register_env\n",
    "register_env(env_name,lambda config: get_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import PolicySpec\n",
    "test_env = get_env()\n",
    "obs_space = test_env.observation_space\n",
    "act_space = test_env.action_space\n",
    "def policies(agent_ids):\n",
    "    return {\n",
    "        str(i): PolicySpec(\n",
    "            # observation_space=obs_space,\n",
    "            # action_space=act_space,\n",
    "            config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = test_env.action_space_sample()\n",
    "test_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import get_trainable_cls\n",
    "import os\n",
    "config = (\n",
    "    get_trainable_cls(\"MADDPG\")\n",
    "    .get_default_config()\n",
    "    .environment(env=env_name)\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
    ")\n",
    "(\n",
    "    config.framework(\"tf\")\n",
    "    # .environment(env_config={\"actions_are_logits\": True})\n",
    "    .training(num_steps_sampled_before_learning_starts=100)\n",
    "    .multi_agent(\n",
    "        policies=policies(test_env._agent_ids),\n",
    "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: str(\n",
    "            agent_id\n",
    "        ),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import air, tune\n",
    "stop = {\n",
    "    \"training_iteration\": 10000,\n",
    "}\n",
    "results = tune.Tuner(\n",
    "    \"MADDPG\",\n",
    "    run_config=air.RunConfig(stop=stop, verbose=2),\n",
    "    param_space=config,\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ddpg.ddpg import DDPGConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune import register_env\n",
    "import rich  \n",
    "import rllib_setup\n",
    "\n",
    "env_name = \"VJS\"\n",
    "register_env(\n",
    "    env_name,\n",
    "    lambda config: rllib_setup.get_env_continuous(),\n",
    ")\n",
    "test_env = rllib_setup.get_env_continuous()\n",
    "\n",
    "\n",
    "def policies(agent_ids):\n",
    "    obs_space = test_env.observation_space\n",
    "    act_space = test_env.action_space\n",
    "    return {\n",
    "        str(i): PolicySpec(\n",
    "            observation_space=obs_space,\n",
    "            action_space=act_space,\n",
    "            # config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    DDPGConfig()\n",
    "    .training(lr=0.01)\n",
    "    .resources(num_gpus=1)\n",
    "    .multi_agent(\n",
    "        policies=policies(test_env._agent_ids),\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: str(agent_id),\n",
    "    )\n",
    ")\n",
    "# config.batch_mode = \"complete_episodes\"\n",
    "rich.print(config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build(env=env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray import air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.Tuner(  \n",
    "    \"DDPG\",\n",
    "    run_config=air.RunConfig(stop={\"episode_reward_mean\": 200}),\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': None, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': None, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'complete_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.9, 'lr': 0.01, 'lr_schedule': None, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 256, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 1, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 100, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': -1, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'twin_q': True, 'q_model_config': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': None, 'custom_model': None, 'custom_model_config': {}}, 'policy_model_config': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': None, 'custom_model': None, 'custom_model_config': {}}, 'tau': 0.005, 'initial_alpha': 1.0, 'target_entropy': 'auto', 'n_step': 1, 'replay_buffer_config': {'_enable_replay_buffer_api': True, 'type': 'MultiAgentPrioritizedReplayBuffer', 'capacity': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'worker_side_prioritization': False}, 'store_buffer_in_checkpoints': False, 'training_intensity': None, 'optimization': {'actor_learning_rate': 0.0003, 'critic_learning_rate': 0.0003, 'entropy_learning_rate': 0.0003}, 'target_network_update_freq': 0, 'num_steps_sampled_before_learning_starts': 1500, '_deterministic_loss': False, '_use_beta_distribution': False, 'use_state_preprocessor': -1, 'worker_side_prioritization': -1, 'input': 'sampler', 'multiagent': {'policies': {'Machine_0': (None, Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
      " 101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64), Box(0.33333334, 3.0, (1,), float32), {}), 'Machine_1': (None, Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
      " 101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64), Box(0.33333334, 3.0, (1,), float32), {}), 'Machine_2': (None, Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
      " 101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64), Box(0.33333334, 3.0, (1,), float32), {}), 'Machine_3': (None, Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
      " 101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64), Box(0.33333334, 3.0, (1,), float32), {}), 'Machine_4': (None, Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
      " 101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64), Box(0.33333334, 3.0, (1,), float32), {}), 'Machine_5': (None, Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
      " 101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64), Box(0.33333334, 3.0, (1,), float32), {}), 'Machine_6': (None, Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
      " 101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64), Box(0.33333334, 3.0, (1,), float32), {}), 'Machine_7': (None, Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
      " 101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64), Box(0.33333334, 3.0, (1,), float32), {}), 'Machine_8': (None, Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
      " 101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64), Box(0.33333334, 3.0, (1,), float32), {}), 'Machine_9': (None, Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
      " 101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64), Box(0.33333334, 3.0, (1,), float32), {})}, 'policy_mapping_fn': <function <lambda> at 0x7efea0f03790>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:442: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-07-21 16:33:38,193\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'info': {'learner': {'Machine_0': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 1.6580369472503662,\n",
       "     'actor_loss': -0.6601142287254333,\n",
       "     'critic_loss': 0.18166503310203552,\n",
       "     'alpha_loss': 0.0,\n",
       "     'alpha_value': 1.0,\n",
       "     'log_alpha_value': 0.0,\n",
       "     'target_entropy': -1.0,\n",
       "     'policy_t': 0.15022191405296326,\n",
       "     'mean_q': 0.008084948174655437,\n",
       "     'max_q': 0.023381337523460388,\n",
       "     'min_q': -0.013059914112091064},\n",
       "    'td_error': array([0.1546048 , 0.6731891 , 0.63759154, 0.36402893, 0.7949961 ,\n",
       "           0.7985662 , 0.52084607, 0.04691811, 0.7126701 , 0.6651226 ,\n",
       "           0.7388468 , 0.36920813, 0.4251228 , 0.1735494 , 0.66400844,\n",
       "           0.80126774, 0.81387985, 0.2162496 , 0.52262676, 0.7517286 ,\n",
       "           0.85137606, 0.7869222 , 0.74207294, 0.17382504, 0.6364951 ,\n",
       "           0.2867324 ], dtype=float32),\n",
       "    'mean_td_error': 0.5508633255958557,\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 26.0,\n",
       "    'num_grad_updates_lifetime': 1.0,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 0.0},\n",
       "   'Machine_1': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 1.5991016626358032,\n",
       "     'actor_loss': -0.4983326196670532,\n",
       "     'critic_loss': 0.2412201315164566,\n",
       "     'alpha_loss': 0.0,\n",
       "     'alpha_value': 1.0,\n",
       "     'log_alpha_value': 0.0,\n",
       "     'target_entropy': -1.0,\n",
       "     'policy_t': 0.15588559210300446,\n",
       "     'mean_q': -0.10071632266044617,\n",
       "     'max_q': 0.0015088170766830444,\n",
       "     'min_q': -0.14973969757556915},\n",
       "    'td_error': array([0.8628882 , 0.85180396, 0.6137356 , 0.7554315 , 0.87260646,\n",
       "           0.7571953 , 0.73328197, 0.6660402 , 0.5320345 , 0.46595812,\n",
       "           0.8412286 , 0.81775004, 0.71766233, 0.42282164, 0.5704464 ,\n",
       "           0.64264715, 0.83889174, 0.6983702 , 0.40633416, 0.8673872 ,\n",
       "           0.38992605, 0.60283446, 0.61481166, 0.8751246 , 0.6587727 ,\n",
       "           0.28920513], dtype=float32),\n",
       "    'mean_td_error': 0.6678919196128845,\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 26.0,\n",
       "    'num_grad_updates_lifetime': 1.0,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 0.0},\n",
       "   'Machine_2': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 1.69425630569458,\n",
       "     'actor_loss': -0.7395418882369995,\n",
       "     'critic_loss': 0.19523286819458008,\n",
       "     'alpha_loss': 0.0,\n",
       "     'alpha_value': 1.0,\n",
       "     'log_alpha_value': 0.0,\n",
       "     'target_entropy': -1.0,\n",
       "     'policy_t': -0.010072022676467896,\n",
       "     'mean_q': 0.12717321515083313,\n",
       "     'max_q': 0.148405522108078,\n",
       "     'min_q': 0.10026466101408005},\n",
       "    'td_error': array([0.1641473 , 0.82556856, 0.31896508, 0.44554126, 0.8873986 ,\n",
       "           0.5715494 , 0.05117954, 0.1513879 , 0.8980577 , 0.04042154,\n",
       "           0.79290295, 0.1375808 , 0.7200526 , 0.37566966, 0.87439597,\n",
       "           0.7627245 , 0.59937775, 0.556092  , 0.8856603 , 0.73386455,\n",
       "           0.77841455, 0.36145014, 0.864449  , 0.61459345, 0.12562646,\n",
       "           0.79326546], dtype=float32),\n",
       "    'mean_td_error': 0.5511667728424072,\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 26.0,\n",
       "    'num_grad_updates_lifetime': 1.0,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 0.0},\n",
       "   'Machine_3': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 1.7531440258026123,\n",
       "     'actor_loss': -0.6678857207298279,\n",
       "     'critic_loss': 0.14677196741104126,\n",
       "     'alpha_loss': 0.0,\n",
       "     'alpha_value': 1.0,\n",
       "     'log_alpha_value': 0.0,\n",
       "     'target_entropy': -1.0,\n",
       "     'policy_t': -0.06550523638725281,\n",
       "     'mean_q': -0.08523748815059662,\n",
       "     'max_q': -0.03920998424291611,\n",
       "     'min_q': -0.1269194483757019},\n",
       "    'td_error': array([0.360331  , 0.6229667 , 0.7360317 , 0.5823229 , 0.2845496 ,\n",
       "           0.5552991 , 0.60658807, 0.5145129 , 0.3019837 , 0.5489106 ,\n",
       "           0.46204045, 0.68435276, 0.43509686, 0.23079175, 0.29571837,\n",
       "           0.6584846 , 0.80875593, 0.39995152, 0.57166064, 0.6475868 ,\n",
       "           0.32742494, 0.53659433, 0.48330167, 0.6230371 , 0.6870532 ,\n",
       "           0.5106689 ], dtype=float32),\n",
       "    'mean_td_error': 0.5183082818984985,\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 26.0,\n",
       "    'num_grad_updates_lifetime': 1.0,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 0.0},\n",
       "   'Machine_4': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 1.684824824333191,\n",
       "     'actor_loss': -0.6918838024139404,\n",
       "     'critic_loss': 0.18665489554405212,\n",
       "     'alpha_loss': 0.0,\n",
       "     'alpha_value': 1.0,\n",
       "     'log_alpha_value': 0.0,\n",
       "     'target_entropy': -1.0,\n",
       "     'policy_t': 0.049918390810489655,\n",
       "     'mean_q': 0.14385183155536652,\n",
       "     'max_q': 0.21834391355514526,\n",
       "     'min_q': 0.07425250113010406},\n",
       "    'td_error': array([0.68771863, 0.07797354, 0.66957045, 0.5567875 , 0.68844104,\n",
       "           0.76226187, 0.76382786, 0.1954458 , 0.754163  , 0.575446  ,\n",
       "           0.37487423, 0.77253234, 0.38582015, 0.7399987 , 0.52030665,\n",
       "           0.6407942 , 0.68173707, 0.4782278 , 0.07522728, 0.6551696 ,\n",
       "           0.46055746, 0.8684386 , 0.74531853, 0.5924485 , 0.5487909 ,\n",
       "           0.59635234], dtype=float32),\n",
       "    'mean_td_error': 0.5718549489974976,\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 26.0,\n",
       "    'num_grad_updates_lifetime': 1.0,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 0.0},\n",
       "   'Machine_5': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 1.5973846912384033,\n",
       "     'actor_loss': -0.5841504335403442,\n",
       "     'critic_loss': 0.12348538637161255,\n",
       "     'alpha_loss': 0.0,\n",
       "     'alpha_value': 1.0,\n",
       "     'log_alpha_value': 0.0,\n",
       "     'target_entropy': -1.0,\n",
       "     'policy_t': 0.22964389622211456,\n",
       "     'mean_q': -0.0019568991847336292,\n",
       "     'max_q': 0.018409468233585358,\n",
       "     'min_q': -0.052427008748054504},\n",
       "    'td_error': array([0.18614234, 0.5541804 , 0.6646888 , 0.7108631 , 0.42795283,\n",
       "           0.496311  , 0.47098112, 0.16642156, 0.20158479, 0.08957845,\n",
       "           0.7180595 , 0.78257346, 0.04756667, 0.38519144, 0.34683943,\n",
       "           0.7116872 , 0.68194056, 0.03962255, 0.18835096, 0.4802031 ,\n",
       "           0.67843544, 0.43927455, 0.63822556, 0.5645315 , 0.4957074 ,\n",
       "           0.37205532], dtype=float32),\n",
       "    'mean_td_error': 0.4438064694404602,\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 26.0,\n",
       "    'num_grad_updates_lifetime': 1.0,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 0.0},\n",
       "   'Machine_6': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 1.6515860557556152,\n",
       "     'actor_loss': -0.6434053182601929,\n",
       "     'critic_loss': 0.1720167100429535,\n",
       "     'alpha_loss': 0.0,\n",
       "     'alpha_value': 1.0,\n",
       "     'log_alpha_value': 0.0,\n",
       "     'target_entropy': -1.0,\n",
       "     'policy_t': -0.11505173146724701,\n",
       "     'mean_q': 0.08138807117938995,\n",
       "     'max_q': 0.1321607381105423,\n",
       "     'min_q': 0.007479935884475708},\n",
       "    'td_error': array([0.45732224, 0.7701329 , 0.1705327 , 0.2621357 , 0.49556023,\n",
       "           0.6739466 , 0.7915436 , 0.6725367 , 0.5629113 , 0.7737222 ,\n",
       "           0.49479973, 0.19152147, 0.80194724, 0.7881701 , 0.5391453 ,\n",
       "           0.35188165, 0.39084598, 0.79656297, 0.64900076, 0.71742797,\n",
       "           0.7948904 , 0.5129976 , 0.21052219, 0.10599816, 0.6053047 ,\n",
       "           0.5383897 ], dtype=float32),\n",
       "    'mean_td_error': 0.5430673360824585,\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 26.0,\n",
       "    'num_grad_updates_lifetime': 1.0,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 0.0},\n",
       "   'Machine_7': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 1.5695878267288208,\n",
       "     'actor_loss': -0.4760107696056366,\n",
       "     'critic_loss': 0.2995120584964752,\n",
       "     'alpha_loss': 0.0,\n",
       "     'alpha_value': 1.0,\n",
       "     'log_alpha_value': 0.0,\n",
       "     'target_entropy': -1.0,\n",
       "     'policy_t': -0.16975539922714233,\n",
       "     'mean_q': 0.11052794009447098,\n",
       "     'max_q': 0.13438838720321655,\n",
       "     'min_q': 0.07766437530517578},\n",
       "    'td_error': array([0.82366145, 0.94294715, 0.893976  , 0.7196832 , 0.7922938 ,\n",
       "           0.15583502, 1.0917673 , 0.9218596 , 0.48614398, 1.1041839 ,\n",
       "           0.77639484, 0.9044107 , 1.137847  , 0.12031889, 0.38266724,\n",
       "           0.8299718 , 0.7198244 , 0.19029966, 0.7777188 , 0.845037  ,\n",
       "           1.0385057 , 0.7855265 , 0.8129716 , 0.22724374, 0.2425594 ,\n",
       "           0.6738744 ], dtype=float32),\n",
       "    'mean_td_error': 0.7075970768928528,\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 26.0,\n",
       "    'num_grad_updates_lifetime': 1.0,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 0.0},\n",
       "   'Machine_8': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 1.6553055047988892,\n",
       "     'actor_loss': -0.7161173224449158,\n",
       "     'critic_loss': 0.10445830225944519,\n",
       "     'alpha_loss': 0.0,\n",
       "     'alpha_value': 1.0,\n",
       "     'log_alpha_value': 0.0,\n",
       "     'target_entropy': -1.0,\n",
       "     'policy_t': -0.08642598241567612,\n",
       "     'mean_q': 0.06542318314313889,\n",
       "     'max_q': 0.09553211182355881,\n",
       "     'min_q': 0.005773816257715225},\n",
       "    'td_error': array([0.428194  , 0.54079807, 0.16054808, 0.18776146, 0.40918314,\n",
       "           0.21983302, 0.40662706, 0.6674923 , 0.45210567, 0.16036281,\n",
       "           0.59028953, 0.492486  , 0.60504436, 0.5830357 , 0.6001671 ,\n",
       "           0.37822777, 0.16892856, 0.5114946 , 0.34374273, 0.32133526,\n",
       "           0.5471931 , 0.6180882 , 0.07180309, 0.537994  , 0.6214222 ,\n",
       "           0.4028196 ], dtype=float32),\n",
       "    'mean_td_error': 0.42411452531814575,\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 26.0,\n",
       "    'num_grad_updates_lifetime': 1.0,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 0.0},\n",
       "   'Machine_9': {'learner_stats': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': 1.5794070959091187,\n",
       "     'actor_loss': -0.5100755095481873,\n",
       "     'critic_loss': 0.2397105097770691,\n",
       "     'alpha_loss': 0.0,\n",
       "     'alpha_value': 1.0,\n",
       "     'log_alpha_value': 0.0,\n",
       "     'target_entropy': -1.0,\n",
       "     'policy_t': 0.2576535940170288,\n",
       "     'mean_q': -0.03990357369184494,\n",
       "     'max_q': -0.01947777532041073,\n",
       "     'min_q': -0.056703682988882065},\n",
       "    'td_error': array([0.725368  , 0.7084014 , 0.74999607, 0.7326813 , 0.59145314,\n",
       "           0.71911013, 0.6490814 , 0.68771124, 0.6993098 , 0.708057  ,\n",
       "           0.66030264, 0.6962742 , 0.24768963, 0.7236441 , 0.20168242,\n",
       "           0.6893992 , 0.14741632, 0.72845453, 0.1843244 , 1.9720788 ,\n",
       "           0.7231802 , 0.38774318, 0.6897437 , 0.51534975, 0.7071996 ,\n",
       "           0.33529025], dtype=float32),\n",
       "    'mean_td_error': 0.6377285718917847,\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 26.0,\n",
       "    'num_grad_updates_lifetime': 1.0,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 0.0}},\n",
       "  'num_env_steps_sampled': 18277,\n",
       "  'num_env_steps_trained': 260,\n",
       "  'num_agent_steps_sampled': 18277,\n",
       "  'num_agent_steps_trained': 260,\n",
       "  'last_target_update_ts': 18277,\n",
       "  'num_target_updates': 1},\n",
       " 'sampler_results': {'episode_reward_max': 0.0,\n",
       "  'episode_reward_min': 0.0,\n",
       "  'episode_reward_mean': 0.0,\n",
       "  'episode_len_mean': 4569.25,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 4,\n",
       "  'policy_reward_min': {'Machine_2': 0.0,\n",
       "   'Machine_5': 0.0,\n",
       "   'Machine_0': 0.0,\n",
       "   'Machine_4': 0.0,\n",
       "   'Machine_7': 0.0,\n",
       "   'Machine_9': 0.0,\n",
       "   'Machine_6': 0.0,\n",
       "   'Machine_3': 0.0,\n",
       "   'Machine_8': 0.0,\n",
       "   'Machine_1': 0.0},\n",
       "  'policy_reward_max': {'Machine_2': 0.0,\n",
       "   'Machine_5': 0.0,\n",
       "   'Machine_0': 0.0,\n",
       "   'Machine_4': 0.0,\n",
       "   'Machine_7': 0.0,\n",
       "   'Machine_9': 0.0,\n",
       "   'Machine_6': 0.0,\n",
       "   'Machine_3': 0.0,\n",
       "   'Machine_8': 0.0,\n",
       "   'Machine_1': 0.0},\n",
       "  'policy_reward_mean': {'Machine_2': 0.0,\n",
       "   'Machine_5': 0.0,\n",
       "   'Machine_0': 0.0,\n",
       "   'Machine_4': 0.0,\n",
       "   'Machine_7': 0.0,\n",
       "   'Machine_9': 0.0,\n",
       "   'Machine_6': 0.0,\n",
       "   'Machine_3': 0.0,\n",
       "   'Machine_8': 0.0,\n",
       "   'Machine_1': 0.0},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "   'episode_lengths': [4475, 4601, 4579, 4622],\n",
       "   'policy_Machine_2_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "   'policy_Machine_5_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "   'policy_Machine_0_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "   'policy_Machine_4_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "   'policy_Machine_7_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "   'policy_Machine_9_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "   'policy_Machine_6_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "   'policy_Machine_3_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "   'policy_Machine_8_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "   'policy_Machine_1_reward': [0.0, 0.0, 0.0, 0.0]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.31043406235085563,\n",
       "   'mean_inference_ms': 1.2853396704073878,\n",
       "   'mean_action_processing_ms': 0.2078184765261193,\n",
       "   'mean_env_wait_ms': 0.12377390612060332,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0055730342864990234,\n",
       "   'StateBufferConnector_ms': 0.00449061393737793,\n",
       "   'ViewRequirementAgentConnector_ms': 0.12553870677947998}},\n",
       " 'episode_reward_max': 0.0,\n",
       " 'episode_reward_min': 0.0,\n",
       " 'episode_reward_mean': 0.0,\n",
       " 'episode_len_mean': 4569.25,\n",
       " 'episodes_this_iter': 4,\n",
       " 'policy_reward_min': {'Machine_2': 0.0,\n",
       "  'Machine_5': 0.0,\n",
       "  'Machine_0': 0.0,\n",
       "  'Machine_4': 0.0,\n",
       "  'Machine_7': 0.0,\n",
       "  'Machine_9': 0.0,\n",
       "  'Machine_6': 0.0,\n",
       "  'Machine_3': 0.0,\n",
       "  'Machine_8': 0.0,\n",
       "  'Machine_1': 0.0},\n",
       " 'policy_reward_max': {'Machine_2': 0.0,\n",
       "  'Machine_5': 0.0,\n",
       "  'Machine_0': 0.0,\n",
       "  'Machine_4': 0.0,\n",
       "  'Machine_7': 0.0,\n",
       "  'Machine_9': 0.0,\n",
       "  'Machine_6': 0.0,\n",
       "  'Machine_3': 0.0,\n",
       "  'Machine_8': 0.0,\n",
       "  'Machine_1': 0.0},\n",
       " 'policy_reward_mean': {'Machine_2': 0.0,\n",
       "  'Machine_5': 0.0,\n",
       "  'Machine_0': 0.0,\n",
       "  'Machine_4': 0.0,\n",
       "  'Machine_7': 0.0,\n",
       "  'Machine_9': 0.0,\n",
       "  'Machine_6': 0.0,\n",
       "  'Machine_3': 0.0,\n",
       "  'Machine_8': 0.0,\n",
       "  'Machine_1': 0.0},\n",
       " 'hist_stats': {'episode_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "  'episode_lengths': [4475, 4601, 4579, 4622],\n",
       "  'policy_Machine_2_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "  'policy_Machine_5_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "  'policy_Machine_0_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "  'policy_Machine_4_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "  'policy_Machine_7_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "  'policy_Machine_9_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "  'policy_Machine_6_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "  'policy_Machine_3_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "  'policy_Machine_8_reward': [0.0, 0.0, 0.0, 0.0],\n",
       "  'policy_Machine_1_reward': [0.0, 0.0, 0.0, 0.0]},\n",
       " 'sampler_perf': {'mean_raw_obs_processing_ms': 0.31043406235085563,\n",
       "  'mean_inference_ms': 1.2853396704073878,\n",
       "  'mean_action_processing_ms': 0.2078184765261193,\n",
       "  'mean_env_wait_ms': 0.12377390612060332,\n",
       "  'mean_env_render_ms': 0.0},\n",
       " 'num_faulty_episodes': 0,\n",
       " 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0055730342864990234,\n",
       "  'StateBufferConnector_ms': 0.00449061393737793,\n",
       "  'ViewRequirementAgentConnector_ms': 0.12553870677947998},\n",
       " 'num_healthy_workers': 4,\n",
       " 'num_in_flight_async_reqs': 0,\n",
       " 'num_remote_worker_restarts': 0,\n",
       " 'num_agent_steps_sampled': 18277,\n",
       " 'num_agent_steps_trained': 260,\n",
       " 'num_env_steps_sampled': 18277,\n",
       " 'num_env_steps_trained': 260,\n",
       " 'num_env_steps_sampled_this_iter': 18277,\n",
       " 'num_env_steps_trained_this_iter': 260,\n",
       " 'num_env_steps_sampled_throughput_per_sec': 1097.6801450314294,\n",
       " 'num_env_steps_trained_throughput_per_sec': 15.61508112426392,\n",
       " 'timesteps_total': 18277,\n",
       " 'num_steps_trained_this_iter': 260,\n",
       " 'agent_timesteps_total': 18277,\n",
       " 'timers': {'training_iteration_time_ms': 16650.442,\n",
       "  'sample_time_ms': 9893.389,\n",
       "  'load_time_ms': 1.672,\n",
       "  'load_throughput': 155499.649,\n",
       "  'learn_time_ms': 347.836,\n",
       "  'learn_throughput': 747.478,\n",
       "  'synch_weights_time_ms': 26.208},\n",
       " 'counters': {'num_env_steps_sampled': 18277,\n",
       "  'num_env_steps_trained': 260,\n",
       "  'num_agent_steps_sampled': 18277,\n",
       "  'num_agent_steps_trained': 260,\n",
       "  'last_target_update_ts': 18277,\n",
       "  'num_target_updates': 1},\n",
       " 'done': False,\n",
       " 'episodes_total': 4,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2023-07-21_16-33-44',\n",
       " 'timestamp': 1689928424,\n",
       " 'time_this_iter_s': 16.70966386795044,\n",
       " 'time_total_s': 16.70966386795044,\n",
       " 'pid': 89046,\n",
       " 'hostname': 'lwh-Super-Server',\n",
       " 'node_ip': '192.168.3.6',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_learner_workers': 0,\n",
       "  'num_gpus_per_learner_worker': 0,\n",
       "  'num_cpus_per_learner_worker': 1,\n",
       "  'local_gpu_idx': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'env': 'VJS',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'disable_env_checking': False,\n",
       "  'is_atari': False,\n",
       "  'auto_wrap_old_gym_envs': True,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'sample_async': False,\n",
       "  'enable_connectors': True,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'complete_episodes',\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'validate_workers_after_construction': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'compress_observations': False,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'gamma': 0.9,\n",
       "  'lr': 0.01,\n",
       "  'lr_schedule': None,\n",
       "  'grad_clip': None,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  'train_batch_size': 256,\n",
       "  'model': {'_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1},\n",
       "  'optimizer': {},\n",
       "  'max_requests_in_flight_per_sampler_worker': 2,\n",
       "  '_learner_class': None,\n",
       "  '_enable_learner_api': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'policy_states_are_swappable': False,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 180.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_config': None,\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_workers': 0,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'enable_async_evaluation': False,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': 1,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 100,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  'worker_cls': None,\n",
       "  'ignore_worker_failures': False,\n",
       "  'recreate_failed_workers': False,\n",
       "  'max_num_worker_restarts': 1000,\n",
       "  'delay_between_worker_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_worker_failures_tolerance': 100,\n",
       "  'worker_health_probe_timeout_s': 60,\n",
       "  'worker_restore_timeout_s': 1800,\n",
       "  'rl_module_spec': None,\n",
       "  '_enable_rl_module_api': False,\n",
       "  '_AlgorithmConfig__prior_exploration_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': True,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  'simple_optimizer': False,\n",
       "  'replay_sequence_length': None,\n",
       "  'horizon': -1,\n",
       "  'soft_horizon': -1,\n",
       "  'no_done_at_end': -1,\n",
       "  'twin_q': True,\n",
       "  'q_model_config': {'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': None,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {}},\n",
       "  'policy_model_config': {'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': None,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {}},\n",
       "  'tau': 0.005,\n",
       "  'initial_alpha': 1.0,\n",
       "  'target_entropy': 'auto',\n",
       "  'n_step': 1,\n",
       "  'replay_buffer_config': {'_enable_replay_buffer_api': True,\n",
       "   'type': 'MultiAgentPrioritizedReplayBuffer',\n",
       "   'capacity': 1000000,\n",
       "   'prioritized_replay': False,\n",
       "   'prioritized_replay_alpha': 0.6,\n",
       "   'prioritized_replay_beta': 0.4,\n",
       "   'prioritized_replay_eps': 1e-06,\n",
       "   'worker_side_prioritization': False},\n",
       "  'store_buffer_in_checkpoints': False,\n",
       "  'training_intensity': None,\n",
       "  'optimization': {'actor_learning_rate': 0.0003,\n",
       "   'critic_learning_rate': 0.0003,\n",
       "   'entropy_learning_rate': 0.0003},\n",
       "  'target_network_update_freq': 0,\n",
       "  'num_steps_sampled_before_learning_starts': 1500,\n",
       "  '_deterministic_loss': False,\n",
       "  '_use_beta_distribution': False,\n",
       "  'use_state_preprocessor': -1,\n",
       "  'worker_side_prioritization': -1,\n",
       "  'input': 'sampler',\n",
       "  'multiagent': {'policies': {'Machine_0': (None,\n",
       "     Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
       "      101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
       "        1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64),\n",
       "     Box(0.33333334, 3.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_1': (None,\n",
       "     Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
       "      101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
       "        1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64),\n",
       "     Box(0.33333334, 3.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_2': (None,\n",
       "     Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
       "      101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
       "        1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64),\n",
       "     Box(0.33333334, 3.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_3': (None,\n",
       "     Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
       "      101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
       "        1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64),\n",
       "     Box(0.33333334, 3.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_4': (None,\n",
       "     Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
       "      101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
       "        1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64),\n",
       "     Box(0.33333334, 3.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_5': (None,\n",
       "     Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
       "      101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
       "        1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64),\n",
       "     Box(0.33333334, 3.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_6': (None,\n",
       "     Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
       "      101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
       "        1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64),\n",
       "     Box(0.33333334, 3.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_7': (None,\n",
       "     Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
       "      101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
       "        1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64),\n",
       "     Box(0.33333334, 3.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_8': (None,\n",
       "     Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
       "      101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
       "        1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64),\n",
       "     Box(0.33333334, 3.0, (1,), float32),\n",
       "     {}),\n",
       "    'Machine_9': (None,\n",
       "     Box(0.0, [101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101. 101.\n",
       "      101. 101. 101. 101. 101. 101.  30.  30.   1.   1.   1.   1.   1.   1.\n",
       "        1.   1.   1.   1.   1.   1.   1.   1.   1.   1.], (38,), float64),\n",
       "     Box(0.33333334, 3.0, (1,), float32),\n",
       "     {})},\n",
       "   'policy_mapping_fn': <function __main__.<lambda>(agent_id, episode, **kwargs)>,\n",
       "   'policies_to_train': None,\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': -1,\n",
       "   'count_steps_by': 'env_steps',\n",
       "   'observation_fn': None},\n",
       "  'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch',\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'num_workers': 4},\n",
       " 'time_since_restore': 16.70966386795044,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': 12.142857142857142,\n",
       "  'ram_util_percent': 25.73333333333333,\n",
       "  'gpu_util_percent0': 0.030000000000000006,\n",
       "  'vram_util_percent0': 0.5079752604166665,\n",
       "  'gpu_util_percent1': 0.13571428571428573,\n",
       "  'vram_util_percent1': 0.8719075520833334}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.sac import SACConfig\n",
    "from ray.tune import register_env\n",
    "import rllib_setup\n",
    "\n",
    "env_name = \"VJS\"\n",
    "register_env(\n",
    "    env_name,\n",
    "    lambda config: rllib_setup.get_env_continuous(),\n",
    ")\n",
    "test_env = rllib_setup.get_env_continuous()\n",
    "\n",
    "\n",
    "def policies(agent_ids):\n",
    "    obs_space = test_env.observation_space\n",
    "    act_space = test_env.action_space\n",
    "    return {\n",
    "        str(i): (\n",
    "            None,\n",
    "            obs_space,\n",
    "            act_space,\n",
    "            {}\n",
    "            # config=config.overrides(agent_id=int(i[8:])),\n",
    "        )\n",
    "        for i in agent_ids\n",
    "    }\n",
    "    \n",
    "    \n",
    "config = SACConfig().training(gamma=0.9, lr=0.01)\n",
    "config = config.resources(num_gpus=0)  \n",
    "config = config.rollouts(num_rollout_workers=4).multi_agent(\n",
    "        policies=policies(test_env._agent_ids),\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: str(agent_id),\n",
    "    )\n",
    "config.batch_mode = \"complete_episodes\"\n",
    "print(config.to_dict())  \n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build(env=env_name)  \n",
    "algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'MultiAgentReplayBuffer', 'prioritized_replay': -1, 'capacity': 1000000, 'replay_mode': 'lockstep'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:442: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=129913)\u001b[0m     self._update_policy_map(policy_dict=self.policy_dict)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=129913)\u001b[0m   File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1985, in _update_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=129913)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=129913)\u001b[0m   File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 2097, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=129913)\u001b[0m     new_policy = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=129913)\u001b[0m   File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 142, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=129913)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=129913)\u001b[0m     raise ValueError(\"Must set `agent_id` in the policy config.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=129913)\u001b[0m ValueError: Must set `agent_id` in the policy config.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=129913)\u001b[0m ValueError: Must set `agent_id` in the policy config.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=129913)\u001b[0m   File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 54, in __init__\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=129912)\u001b[0m 2023-07-21 16:35:35,077\tERROR worker.py:861 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=129912, ip=192.168.3.6, actor_id=5feb5bfedc6984d4caecd7c101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0956a1d820>)\n",
      "2023-07-21 16:35:35,320\tERROR actor_manager.py:507 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=129911, ip=192.168.3.6, actor_id=42197ab003253a5e8a1a0a3001000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7efd0445e7f0>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 738, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1985, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 2097, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 142, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 54, in __init__\n",
      "    raise ValueError(\"Must set `agent_id` in the policy config.\")\n",
      "ValueError: Must set `agent_id` in the policy config.\n",
      "2023-07-21 16:35:35,323\tERROR actor_manager.py:507 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=129912, ip=192.168.3.6, actor_id=5feb5bfedc6984d4caecd7c101000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f0956a1d820>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 738, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1985, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 2097, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 142, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 54, in __init__\n",
      "    raise ValueError(\"Must set `agent_id` in the policy config.\")\n",
      "ValueError: Must set `agent_id` in the policy config.\n",
      "2023-07-21 16:35:35,325\tERROR actor_manager.py:507 -- Ray error, taking actor 3 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=129913, ip=192.168.3.6, actor_id=d5a013683cbc9ca81d36a82501000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fdffa0657f0>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 738, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1985, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 2097, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 142, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 54, in __init__\n",
      "    raise ValueError(\"Must set `agent_id` in the policy config.\")\n",
      "ValueError: Must set `agent_id` in the policy config.\n",
      "2023-07-21 16:35:35,327\tERROR actor_manager.py:507 -- Ray error, taking actor 4 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=129914, ip=192.168.3.6, actor_id=a1fe656ab8f403f53281575f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7ef8e17d67f0>)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 738, in __init__\n",
      "    self._update_policy_map(policy_dict=self.policy_dict)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1985, in _update_policy_map\n",
      "    self._build_policy_map(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 2097, in _build_policy_map\n",
      "    new_policy = create_policy_for_framework(\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/policy.py\", line 142, in create_policy_for_framework\n",
      "    return policy_class(observation_space, action_space, merged_config)\n",
      "  File \"/home/yuan/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py\", line 54, in __init__\n",
      "    raise ValueError(\"Must set `agent_id` in the policy config.\")\n",
      "ValueError: Must set `agent_id` in the policy config.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must set `agent_id` in the policy config.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mrollouts(num_rollout_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)   \n\u001b[1;32m     15\u001b[0m config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39menvironment(env\u001b[39m=\u001b[39menv_name)  \n\u001b[0;32m---> 16\u001b[0m algo \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39;49mbuild()  \n\u001b[1;32m     17\u001b[0m algo\u001b[39m.\u001b[39mtrain()  \n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm_config.py:1071\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1069\u001b[0m     algo_class \u001b[39m=\u001b[39m get_trainable_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class)\n\u001b[0;32m-> 1071\u001b[0m \u001b[39mreturn\u001b[39;00m algo_class(\n\u001b[1;32m   1072\u001b[0m     config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m use_copy \u001b[39melse\u001b[39;49;00m copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m),\n\u001b[1;32m   1073\u001b[0m     logger_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger_creator,\n\u001b[1;32m   1074\u001b[0m )\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:475\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    462\u001b[0m     \u001b[39m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m     },\n\u001b[1;32m    473\u001b[0m }\n\u001b[0;32m--> 475\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    476\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    477\u001b[0m     logger_creator\u001b[39m=\u001b[39;49mlogger_creator,\n\u001b[1;32m    478\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    479\u001b[0m )\n\u001b[1;32m    481\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/tune/trainable/trainable.py:170\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout, sync_config)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mget_node_ip_address()\n\u001b[0;32m--> 170\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    171\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:601\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[39mif\u001b[39;00m _init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    596\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[39m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[39m#   in each training iteration.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    600\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    602\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    603\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    604\u001b[0m         default_policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    605\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    606\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rollout_workers,\n\u001b[1;32m    607\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    608\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    611\u001b[0m     \u001b[39m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[1;32m    612\u001b[0m     \u001b[39m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    613\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m    614\u001b[0m         \u001b[39m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:194\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mexcept\u001b[39;00m RayActorError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    182\u001b[0m     \u001b[39m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[39m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[39m# errors.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mactor_init_failed:\n\u001b[1;32m    187\u001b[0m         \u001b[39m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[1;32m    188\u001b[0m         \u001b[39m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[39m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         \u001b[39m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m         \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m2\u001b[39m]\n\u001b[1;32m    195\u001b[0m     \u001b[39m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1431\u001b[0m, in \u001b[0;36mray._raylet.execute_task\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1510\u001b[0m, in \u001b[0;36mray._raylet.execute_task\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1434\u001b[0m, in \u001b[0;36mray._raylet.execute_task\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1438\u001b[0m, in \u001b[0;36mray._raylet.execute_task\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1378\u001b[0m, in \u001b[0;36mray._raylet.execute_task.function_executor\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/_private/function_manager.py:724\u001b[0m, in \u001b[0;36mactor_method_executor\u001b[0;34m()\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    723\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 724\u001b[0m     \u001b[39mreturn\u001b[39;00m method(__ray_actor, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py:464\u001b[0m, in \u001b[0;36m_resume_span\u001b[0;34m()\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39m# If tracing feature flag is not on, perform a no-op\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_tracing_enabled() \u001b[39mor\u001b[39;00m _ray_trace_ctx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[1;32m    466\u001b[0m tracer: _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mTracer \u001b[39m=\u001b[39m _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mget_tracer(\n\u001b[1;32m    467\u001b[0m     \u001b[39m__name__\u001b[39m\n\u001b[1;32m    468\u001b[0m )\n\u001b[1;32m    470\u001b[0m \u001b[39m# Retrieves the context from the _ray_trace_ctx dictionary we\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# injected.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:738\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m# if RLModule API is enabled, marl_module_spec holds the specs of the RLModules\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmarl_module_spec \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 738\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_policy_map(policy_dict\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_dict)\n\u001b[1;32m    740\u001b[0m \u001b[39m# Update Policy's view requirements from Model, only if Policy directly\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39m# inherited from base `Policy` class. At this point here, the Policy\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[39m# must have it's Model (if any) defined and ready to output an initial\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39m# state.\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[39mfor\u001b[39;00m pol \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_map\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py:464\u001b[0m, in \u001b[0;36m_resume_span\u001b[0;34m()\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39m# If tracing feature flag is not on, perform a no-op\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_tracing_enabled() \u001b[39mor\u001b[39;00m _ray_trace_ctx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[1;32m    466\u001b[0m tracer: _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mTracer \u001b[39m=\u001b[39m _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mget_tracer(\n\u001b[1;32m    467\u001b[0m     \u001b[39m__name__\u001b[39m\n\u001b[1;32m    468\u001b[0m )\n\u001b[1;32m    470\u001b[0m \u001b[39m# Retrieves the context from the _ray_trace_ctx dictionary we\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# injected.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:1985\u001b[0m, in \u001b[0;36m_update_policy_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1980\u001b[0m     updated_policy_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_policy_dict_with_marl_module(\n\u001b[1;32m   1981\u001b[0m         updated_policy_dict\n\u001b[1;32m   1982\u001b[0m     )\n\u001b[1;32m   1984\u001b[0m \u001b[39m# Builds the self.policy_map dict\u001b[39;00m\n\u001b[0;32m-> 1985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_policy_map(\n\u001b[1;32m   1986\u001b[0m     policy_dict\u001b[39m=\u001b[39mupdated_policy_dict,\n\u001b[1;32m   1987\u001b[0m     policy\u001b[39m=\u001b[39mpolicy,\n\u001b[1;32m   1988\u001b[0m     policy_states\u001b[39m=\u001b[39mpolicy_states,\n\u001b[1;32m   1989\u001b[0m )\n\u001b[1;32m   1991\u001b[0m \u001b[39m# Initialize the filter dict\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_filter_dict(updated_policy_dict)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py:464\u001b[0m, in \u001b[0;36m_resume_span\u001b[0;34m()\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39m# If tracing feature flag is not on, perform a no-op\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_tracing_enabled() \u001b[39mor\u001b[39;00m _ray_trace_ctx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[1;32m    466\u001b[0m tracer: _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mTracer \u001b[39m=\u001b[39m _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mget_tracer(\n\u001b[1;32m    467\u001b[0m     \u001b[39m__name__\u001b[39m\n\u001b[1;32m    468\u001b[0m )\n\u001b[1;32m    470\u001b[0m \u001b[39m# Retrieves the context from the _ray_trace_ctx dictionary we\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# injected.\u001b[39;00m\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py:2097\u001b[0m, in \u001b[0;36m_build_policy_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[39mfor\u001b[39;00m name, policy_spec \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(policy_dict\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m   2094\u001b[0m \n\u001b[1;32m   2095\u001b[0m     \u001b[39m# Create the actual policy object.\u001b[39;00m\n\u001b[1;32m   2096\u001b[0m     \u001b[39mif\u001b[39;00m policy \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2097\u001b[0m         new_policy \u001b[39m=\u001b[39m create_policy_for_framework(\n\u001b[1;32m   2098\u001b[0m             policy_id\u001b[39m=\u001b[39mname,\n\u001b[1;32m   2099\u001b[0m             policy_class\u001b[39m=\u001b[39mget_tf_eager_cls_if_necessary(\n\u001b[1;32m   2100\u001b[0m                 policy_spec\u001b[39m.\u001b[39mpolicy_class, policy_spec\u001b[39m.\u001b[39mconfig\n\u001b[1;32m   2101\u001b[0m             ),\n\u001b[1;32m   2102\u001b[0m             merged_config\u001b[39m=\u001b[39mpolicy_spec\u001b[39m.\u001b[39mconfig,\n\u001b[1;32m   2103\u001b[0m             observation_space\u001b[39m=\u001b[39mpolicy_spec\u001b[39m.\u001b[39mobservation_space,\n\u001b[1;32m   2104\u001b[0m             action_space\u001b[39m=\u001b[39mpolicy_spec\u001b[39m.\u001b[39maction_space,\n\u001b[1;32m   2105\u001b[0m             worker_index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworker_index,\n\u001b[1;32m   2106\u001b[0m             seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed,\n\u001b[1;32m   2107\u001b[0m         )\n\u001b[1;32m   2108\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2109\u001b[0m         new_policy \u001b[39m=\u001b[39m policy\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/utils/policy.py:142\u001b[0m, in \u001b[0;36mcreate_policy_for_framework\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[39mreturn\u001b[39;00m policy_class(observation_space, action_space, merged_config)\n\u001b[1;32m    140\u001b[0m \u001b[39m# Non-tf: No graph, no session.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m policy_class(observation_space, action_space, merged_config)\n",
      "File \u001b[0;32m~/ResMan/man/lib/python3.9/site-packages/ray/rllib/algorithms/maddpg/maddpg_tf_policy.py:54\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m agent_id \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39magent_id\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m agent_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMust set `agent_id` in the policy config.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(agent_id) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mint\u001b[39m:\n\u001b[1;32m     56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAgent ids must be integers for MADDPG.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Must set `agent_id` in the policy config."
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.maddpg.maddpg import MADDPGConfig\n",
    "config = MADDPGConfig()\n",
    "print(config.replay_buffer_config)  \n",
    "replay_config = config.replay_buffer_config.update(  \n",
    "    {\n",
    "        \"capacity\": 100000,\n",
    "        \"prioritized_replay_alpha\": 0.8,\n",
    "        \"prioritized_replay_beta\": 0.45,\n",
    "        \"prioritized_replay_eps\": 2e-6,\n",
    "    }\n",
    ")\n",
    "config.training(replay_buffer_config=replay_config)   \n",
    "config = config.resources(num_gpus=0)   \n",
    "config = config.rollouts(num_rollout_workers=4)   \n",
    "config = config.environment(env=env_name)  \n",
    "algo = config.build()  \n",
    "algo.train()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
